PaperID,Abstract
2,"In this paper, we propose a level-set based topology optimization method for designing a reactor, which is used as a part of the DC-DC converter in electric and hybrid vehicles. Since it realizes a high-power driving motor and its performance relies on its component, i.e., reactor core, it is valuable to establish a reasonable design method for the reactor core. Boundary tracking type level-set topology optimization is suitable for this purpose, because the shape and topology of the target structure is clearly represented by the zero boundary of the level-set function, and the state variables are accurately computed using the zero boundary tracking mesh. We formulate the design problem on the basis of electromagnetics, and derive the design sensitivities. The derived sensitivities are linked with boundary tracking type level-set topology optimization, and as a result, a useful structural optimization method for the reactor core design problem is developed."
3,"Level-set methods are domain classification techniques that are gaining popularity in the recent years for structural topology optimization. Level sets classify a domain into two or more categories (such as material and void) by examining the value of a scalar level-set function (LSF) defined in the entire design domain. In most level-set formulations, a large number of design variables, or degrees of freedom is used to define the LSF, which implicitly defines the structure. The large number of design variables makes non-gradient optimization techniques all but ineffective. Kriging-interpolated level sets (KLS) on the other hand are formulated with an objective to enable non-gradient optimization by defining the design variables as the LSF values at few select locations (knot points) and using a Kriging model to interpolate the LSF in the rest of the design domain. A downside of concern when adopting KLS, is that using too few knot points may limit the capability to represent complex shapes, while using too many knot points may cause difficulty for non-gradient optimization. This paper presents a study of the effect of number and layout of the knot points in KLS on the capability to represent complex topologies in single and multi-component structures. Image matching error metrics are employed to assess the degree of mismatch between target topologies and those best-attainable via KLS. Results are presented in a catalogue-style in order to facilitate appropriate selection of knot-points by designers wishing to apply KLS for topology optimization."
4,"Optimum selection of cutting conditions in high-speed and ultra-precision machining processes often poses a challenging task due to several reasons; such as the need for costly experimental setup and the limitation on the number of experiments that can be performed before tool degradation starts becoming a source of noise in the readings. Moreover, oftentimes there are several objectives to consider, some of which may be conflicting, while others may be somewhat correlated. Pareto-optimality analysis is needed for conflicting objectives; however the existence of several objectives (high-dimension Pareto space) makes the generation and interpretation of Pareto solutions difficult. The approach adopted in this paper is a modified multi-objective efficient global optimization (m-EGO). In m-EGO, sample data points from experiments are used to construct Kriging meta-models, which act as predictors for the performance objectives. Evolutionary multi-objective optimization is then conducted to spread a population of new candidate experiments towards the zones of search space that are predicted by the Kriging models to have favorable performance, as well as zones that are under-explored. New experiments are then used to update the Kriging models, and the process is repeated until termination criteria are met. Handling a large number of objectives is improved via a special selection operator based on principle component analysis (PCA) within the evolutionary optimization. PCA is used to automatically detect correlations among objectives and perform the selection within a reduced space in order to achieve a better distribution of experimental sample points on the Pareto frontier. Case studies show favorable results in ultra-precision diamond turning of Aluminum alloy as well as high-speed drilling of woven composites."
5,"When developing a first-generation product, an iterative approach often yields the shortest time-to-market. In order to optimize its performance, however, a fundamental understanding of the theory governing its operation becomes necessary. This paper details the optimization of the Tata Swach, a consumer water purifier produced for India. The primary objective of the work was to increase flow rate while considering other factors such as cost, manufacturability, and efficacy. A mathematical model of the flow characteristics through the filter was developed. Based on this model, a design tool was created to allow designers to predict flow behavior without prototyping, significantly reducing the necessity of iteration. Sensitivity analysis was used to identify simple ways to increase flow rate as well as potential weak points in the design. Finally, it was demonstrated that maximum flow rate can be increased by 50% by increasing the diameter of a flow-restricting feature while simultaneously increasing the length of the active purification zone. This can be accomplished without significantly affecting cost, manufacturability, and efficacy."
6,"We describe a trainable, hand drawn, single stroke 3D sketch–based classification system, using a motion detecting depth sense camera. Our system captures data from a user, who is free to sketch any desired shape in a 3D environment. The overall system is based on a set of previously defined and well developed classifiers, which are, the Rubine Classifier, $1 recognizer and the Image based classifier. The novelty of this paper comes from 1) the classification of sketches drawn in a 3D environment; 2) extending the pixel based image representation to a voxel–based scheme; and 3) combining the results from individual classifiers using a sensitivity matrix. To evaluate the performance of the system, user studies were performed. To validate the significance of results obtained from the user studies, we performed a t–test. Our system outperforms the individual classifiers and is able to achieve an average overall accuracy of 93+%."
7,"A novel parameterization concept for structural truss topology optimization is presented in this article that enables the use of evolutionary algorithms in design of large-scale structures. The representational power of Boolean networks is used here to parameterize truss topology. A genetic algorithm then operates on parameters that govern the generation of truss topologies using this random network instead of operating directly on design variables. A genetic algorithm implementation is also presented that is congruent with the local rule application of the random network. The primary advantage of using a Boolean random network representation is that a relatively large number of ground structure nodes can be used, enabling successful exploration of a large-scale design space. In the classical binary representation of ground structures, the number of optimization variables increases quadratically with the number of nodes, restricting the maximum number of nodes that can be considered using a ground structure approach. The Boolean random network representation proposed here allows for the exploration of the entire topology space in a systematic way using only a linear number of variables. The number of nodes in the design domain, therefore, can be increased significantly. Truss member geometry and size optimization is performed here in a nested manner where an inner loop size optimization problem is solved for every candidate topology using sequential linear programming with move-limits. The Boolean random network and nested inner-loop optimization allows for the concurrent optimization of truss topology, geometry, and size. The effectiveness of this method is demonstrated using a planar truss design optimization benchmark problem."
8,"Computational Design Synthesis (CDS) is used to enable the computer to generate valid and even creative solutions for an engineering task. Graph grammars are a CDS approach in which engineering knowledge is formalized using graphs to represent designs and rules that describe possible graph transformations, i.e. changes of designs. For most engineering tasks two different kinds of rules are required: rules that change the topology and rules that change parameters of a design. One of the main challenges in CDS using both topologic and parametric rules is to decide "
9,"Product alternatives suggested by a generative design system often need to be evaluated on qualitative criteria. This evaluation necessitates that several feasible solutions which fulfill all technical constraints can be proposed to the user of the system. Also, as concept development is an iterative process, it is important that these solutions are generated quickly; i.e., the system must have a low convergence time. A problem, however, is that stochastic constraint-handling techniques can have highly unpredictable convergence times, spanning several orders of magnitude, and might sometimes not converge at all. A possible solution to avoid the lengthy runs is to restart the search after a certain time, with the hope that a new starting point will lead to a lower overall convergence time, but selecting an optimal restart-time is not trivial. In this paper, two strategies are investigated for such selection, and their performance is evaluated on two constraint-handling techniques for a product design problem. The results show that both restart strategies can greatly reduce the overall convergence time. Moreover, it is shown that one of the restart strategies can be applied to a wide range of constraint-handling techniques and problems, without requiring any fine-tuning of problem-specific parameters."
10,"The amount of user-generated content related to consumer products continues to grow as users increasingly take advantage of forums, product review sites, and social media platforms. The content is a promising source of insight into users’ needs and experiences. However, the challenge remains as to how concise and useful insights can be extracted from large quantities of unstructured data. We propose a visualization tool which allows designers to quickly and intuitively sift through large amounts of user-generated content and derive useful insights regarding users’ perceptions of product features. The tool leverages machine learning algorithms to automate labor-intensive portions of the process, and no manual labeling is required by the designer. Language processing techniques are arranged in a novel way to guide the designer in selecting the appropriate inputs, and multidimensional scaling enables presentation of the results in concise 2D plots. To demonstrate the efficacy of the tool, a case study is performed on action cameras. Product reviews from Amazon.com are analyzed as the user-generated content. Results from the case study show that the tool is helpful in condensing large amounts of user-generated content into useful insights, such as the key differentiations that users perceive among similar products."
11,"Design-by-analogy is an effective approach to innovative concept generation, but can be elusive at times due to the fact that few methods and tools exist to assist designers in systematically seeking and identifying analogies from general data sources, databases, or repositories, such as patent databases. A new method for extracting analogies from data sources has been developed to provide this capability. Building on past research, we utilize a functional vector space model to quantify analogous similarity between a design problem and the data source of potential analogies. We quantitatively evaluate the functional similarity between represented design problems and, in this case, patent descriptions of products. We develop a complete functional vocabulary to map the patent database to applicable functionally critical terms, using document parsing algorithms to reduce text descriptions of the data sources down to the key functions, and applying Zipf’s law on word count order reduction to reduce the words within the documents. The reduction of a document (in this case a patent) into functional analogous words enables the matching to novel ideas that are functionally similar, which can be customized in various ways. This approach thereby provides relevant sources of design-by-analogy inspiration. Although our implementation of the technique focuses on functional descriptions of patents and the mapping of these functions to those of the design problem, resulting in a set of analogies, we believe that this technique is applicable to other analogy data sources as well. As a verification of the approach, an original design problem for an automated window washer illustrates the distance range of analogical solutions that can be extracted, extending from very near-field, literal solutions to far-field cross-domain analogies. Finally, a comparison with a current patent search tool is performed to draw a contrast to the status quo and evaluate the effectiveness of this work."
12,"Predictive design analytics is a new paradigm to enable design engineers to extract knowledge from large-scale, multi-dimensional, unstructured, volatile data, and transform the knowledge and its trend into design decision making. Predictive, data-driven family design (PDFD) is proposed as one of the predictive design analytics methods to tackle some issues in family design. First, a number and specifications of product architectures are determined by data (not by pre-defined market segments) in order to maximize expected profit. A trade-off between price and cost in terms of the quantity and specifications of architectures helps to set the target in the enterprise level. k-means clustering is used to find architectures that minimize within architecture sum of squared errors. Second, a price prediction method as a function of product performance and deviations between performance and customer requirements is suggested with exponential smoothing based on innovations state space models. Regression coefficients are treated as customer preferences over product performance, and analyzed as a time series. Prediction intervals are proposed to show market uncertainties. Third, multiple values for common parameters in family design can be identified using the expectation maximization clustering so that multiple-platform design can be explored. Last, large-scale data can be handled by the PDFD algorithm. A data set which contains a total of 14 million instances is used in the case study. The design of a family of universal electronic motors demonstrates the proposed approach and highlights its benefits and limitations."
13,"Motivated by continued interest within the design community to model design preferences, this paper investigates the question of predicting preferences with particular application to consumer purchase behavior: How can we obtain high prediction accuracy in a consumer preference model using market purchase data? To this end, we employ sparse coding and sparse restricted Boltzmann machines, recent methods from machine learning, to transform the original market data into a sparse and high-dimensional representation. We show that these ‘feature learning’ techniques, which are independent from the preference model itself (e.g., logit model), can complement existing efforts towards high-accuracy preference prediction. Using actual passenger car market data, we achieve significant improvement in prediction accuracy on a binary preference task by properly transforming the original consumer variables and passenger car variables to a sparse and high-dimensional representation."
14,"The Change Prediction Method is an approach that has been proposed in the literature as a way to assess the risk of change propagation. This approach requires experts to define the elements of the design structure matrix and provide both impact and likelihood values for each subsystem interaction. The combined risk values produced by the Change Prediction Method indicate where high probabilities of propagation may exist, but the results rely heavily on the supplied expert data. This study explores how potential variability in expert data impacts the rank order of returned risk values from the Change Prediction Method. Results are presented that indicate significant changes in rank order, highlighting both the importance of expert data accuracy and the insights that can be gained from the Change Prediction Method as a design tool."
15,"When most designers set out to develop a new product they solicit feedback from potential consumers. These data are incorporated into the design process in an effort to more effectively meet customer requirements. Often these data are used to construct a model of consumer preference capable of evaluating candidate designs. Although the mechanics of these models have been extensively studied there are still some open questions, particularly with respect to models of aesthetic preference. When constructing preference models, simplistic product representations are often favored over high fidelity product models in order to save time and expense. This work investigates how choice of product representation can affect model performance in visual conjoint analysis. Preference models for a single product, a table knife, are derived using three different representation schemes; simple sketches, solid models, and 3D printed models. Each of these representations is used in a separate conjoint analysis survey. The results from this study showed that consumer responses were inconsistent and potentially contradictory between different representations. Consequently, when using conjoint analysis for product innovation, obtaining a true understanding of consumer preference requires selecting representations based on how accurately they convey the product details in question."
16,"When discussing Arrow’s Impossibility Theorem (AIT) in engineering design, we find that one condition, Independence of Irrelevant Alternatives (IIA), has been misunderstood generally. In this paper, two types of IIA are distinguished. One is based on Kenneth Arrow (IIA-A) that concerns the rationality condition of a collective choice rule (CCR). Another one is based on Amartya Sen (IIA-S) that is a condition for a choice function (CF). Through the analysis of IIA-A, this paper revisits three decision methods (i.e., Pugh matrix, Borda count and Quality Function Deployment) that have been criticized for their failures in some situations. It is argued that the violation of IIA-A does not immediately imply irrationality in engineering design, and more detailed analysis should be applied to examine the meaning of “irrelevant information”. Alternatively, IIA-S is concerned with the transitivity of CF, and it is associated with contraction consistency (Property α) and expansion consistency (Property β). It is shown that IIA-A and IIA-S are technically distinct and should not be confused in the rationality arguments. Other versions of IIA-A are also introduced to emphasize the significance of mathematical clarity in the discussion of AIT-related issues."
17,"This article discusses a design methodology for a Decision Support System (DSS) in the area of Data-Driven Management (DDM). We partition the DSS into an offline and an online system. Through rigorous testing, the offline system finds the best combination of Data Mining (DM) and Artificial Intelligence (AI) algorithms. Only the best algorithms are used in the online system to extract information from data and to make sense of this information by providing an objective second opinion on a decision result. To support the proposed design methodology, we construct a DSS that uses DM methods for market segmentation and AI methods for product positioning. As part of the offline system construction, we evaluate four intrinsic dimension estimation, three dimension reduction and four clustering algorithms. The performance is evaluated with statistical methods, silhouette mean and 10-fold stratified cross validated classification accuracy. We find that every DSS problem requires us to search a suitable algorithm structure, because different algorithms, for the same task, have different merits and shortcomings and it is impossible to know a priory which combination of algorithms gives the best results. Therefore, to select the best algorithms is empirical science where the possible combinations are tested. With this study, we deliver a blueprint on how to construct a DSS for product positioning. The proposed design methodology can be easily adopted to serve in a wide range of DDM problems."
18,"Complex system design problems tend to be high dimensional and nonlinear, and also often involve multiple objectives and mixed-integer variables. Heuristic optimization algorithms have the potential to address the typical (if not most) characteristics of such complex problems. Among them, the Particle Swarm Optimization (PSO) algorithm has gained significant popularity due to its maturity and fast convergence abilities. This paper seeks to translate the unique benefits of PSO from solving typical continuous single-objective optimization problems to solving multi-objective mixed-discrete problems, which is a relatively new ground for PSO application. The previously developed Mixed-Discrete Particle Swarm Optimization (MDPSO) algorithm, which includes an exclusive diversity preservation technique to prevent premature particle clustering, has been shown to be a powerful single-objective solver for highly constrained MINLP problems. In this paper, we make fundamental advancements to the MDPSO algorithm, enabling it to solve challenging multi-objective problems with mixed-discrete design variables. In the velocity update equation, the explorative term is modified to point towards the non-dominated solution that is the closest to the corresponding particle (at any iteration). The fractional domain in the diversity preservation technique, which was previously defined in terms of a single global leader, is now applied to multiple global leaders in the intermediate Pareto front. The multi-objective MDPSO (MO-MDPSO) algorithm is tested using a suite of diverse benchmark problems and a disc-brake design problem. To illustrate the advantages of the new MO-MDPSO algorithm, the results are compared with those given by the popular Elitist Non-dominated Sorting Genetic Algorithm-II (NSGA-II)."
19,"Computer models are very important to planning, operation, and control of power system. Although elements such as generators and transmission lines have been relatively well understood, developing a comprehensive power system model is a daunting task because challenges associated with loads modeling (they change all the time and utilities have very little control on). Unfortunately, inaccurate load models have serious implications such as unsafe operating conditions, power outages, under-utilization of system capacity, or inappropriate capital investment. This paper presents the use of state-of-the art Bayesian calibration framework for simultaneous load model selection and calibration. The approach aims at identifying configuration and reducing parameters uncertainty of the Western Electricity Coordinating Council’s (WECC) composite load model in the presence of measured field data. The success of the approach is illustrated with synthetic field data and a simplified model."
20,"The global quest for energy sustainability has motivated the development of technology for efficiently transforming various natural resources into energy. Combining these alternative energy sources with existing power systems requires systematic assessments and planning. The present study investigates the conversion of an existing power system into one with a wind-integrated microgrid. The standard approach applies wind resource assessment to determine suitable wind farm locations with high potential energy and then develops specific dispatch strategies to meet the power demand for the wind-integrated system with low cost, high reliability, and low impact on the environment. However, the uncertainty in wind resource results in fluctuating power generation. The installation of additional energy storage devices is thus needed in the dispatch strategy to ensure a stable power supply. The present work proposes a design procedure for obtaining the optimal sizing of wind turbines and storage devices considering wind resource assessment and dispatch strategy under uncertainty. Two wind models are developed from real-world wind data and apply in the proposed optimization framework. Based on comparisons of system reliability between the optimal results and real operating states, an appropriate wind model can be chosen to represent the wind characteristics of a particular region. Results show that the trend model of wind data is insufficient for wind-integrated microgrid planning because it does not consider the large variation of wind data. The wind model should include the uncertainties of wind resource in the design of a wind-integrated microgrid system to ensure high reliability of optimal results."
21,"Modeling and unit-cost optimization of a water-heated humidification-dehumidification (HDH) desalination system were presented in previous work of the authors. The system controlled the saline water flow rate to prevent salts from precipitating at higher water temperatures. It was then realized that this scheme had a negative impact on condensation performance when the controlled flow rate was not sufficiently high. This work builds on the previous system by disconnecting the condenser from the saline water cycle and by introducing a solar air heater to further augment the humidification performance. In addition, improved models for the condenser and the humidifier were used to obtain more accurate productivity estimations. The Heuristic Gradient Projection (HGP) optimization procedure was also refactored to result in reduced number of function evaluations to reach the global optimum compared to Genetic Algorithms (GA’s). A case study which assumes a desalination plant on the Red Sea near the city of Hurghada is presented. The unit-cost of produced fresh water for the new optimum system is $0.5/m"
22,"Reverse osmosis (RO) is one of the main commercial technologies for desalination of water with salinity content too high for human consumption in order to produce fresh water. RO may hold promise for remote areas with scarce fresh water resources, however, RO energy requirements being in the form of electric power have few options in such areas. Fortunately, scarce rainfall is often associated with abundant sunshine, which makes solar photovoltaic (PV) power an attractive option. Equipping a photovoltaic powered reverse osmosis (PV-RO) desalination plant with battery storage has an advantage of steadier and longer hours of operation, thereby making better use of the investments in RO system components, but the additional cost from including batteries may end up increasing the overall cost of fresh water. It is therefore of paramount importance to consider the overall cost-effectiveness of the PV-RO system when designing the desalination plant. Recent work by the authors has generalized the steady operation model of RO systems to hourly adjusted power-dispatch via a proportional-derivative (PD) controller that depends on the state of charge (SOC) of the battery, yet the operating conditions; namely pressure and flow for a given power dispatch were only empirically selected. This paper considers a multi-level optimization model for PV-RO systems with battery storage by considering a “sub-loop” optimization of the feed pressure and flow given power dispatch for a fixed RO system configuration, as well as a “top-level” optimization where the system configuration itself is adjusted by the design variables. Effect of the sub-loop optimization is assessed by comparing the obtained cost of fresh water with the previous empirically adjusted system for locations and weather conditions near the city of Hurgada on the Red Sea."
23,"The variable and uncertain nature of wind generation presents a new concern to power system operators. One of the biggest concerns associated with integrating a large amount of wind power into the grid is the ability to handle large ramps in wind power output. Large ramps can significantly influence system economics and reliability, on which power system operators place primary emphasis. The Wind Forecasting Improvement Project (WFIP) was performed to improve wind power forecasts and determine the value of these improvements to grid operators. This paper evaluates the performance of improved short-term wind power ramp forecasting. The study is performed for the Electric Reliability Council of Texas (ERCOT) by comparing the experimental WFIP forecast to the current short-term wind power forecast (STWPF). Four types of significant wind power ramps are employed in the study; these are based on the power change magnitude, direction, and duration. The swinging door algorithm is adopted to extract ramp events from actual and forecasted wind power time series. The results show that the experimental short-term wind power forecasts improve the accuracy of the wind power ramp forecasting, especially during the summer."
24,"Traditionally viewed as mere energy consumers, buildings have in recent years adapted, capitalizing on smart grid technologies and distributed energy resources to not only efficiently use energy, but to also output energy. This has led to the development of net-zero energy buildings, a concept which encapsulates the synergy of energy efficient buildings, smart grids, and renewable energy utilization to reach a balanced energy budget over an annual cycle. This work looks to further expand on this idea, moving beyond just individual buildings and considering net-zero at a community scale. We hypothesize that applying net-zero concepts to building communities, also known as building clusters, instead of individual buildings will result in cost effective building systems which in turn will be resilient to power disruption. To this end, this paper develops an intelligent energy optimization algorithm for demand side energy management, taking into account a multitude of factors affecting cost including comfort, energy price, Heating, Ventilation, and Air Conditioning (HVAC) system, energy storage, weather, and on-site renewable resources. A bi-level operation decision framework is presented to study the energy tradeoffs within the building cluster, with individual building energy optimization on one level and an overall net-zero energy optimization handled on the next level. The experimental results demonstrate that the proposed approach is capable of significantly shifting demand, and when viable, reducing the total energy demand within net-zero building clusters. Furthermore, the optimization framework is capable of deriving Pareto solutions for the cluster which provide valuable insight for determining suitable energy strategies."
25,"Large-scale desalination plants are complex systems with many inter-disciplinary interactions and different levels of sub-system hierarchy. Advanced complex systems design tools have been shown to have a positive impact on design in aerospace and automotive, but have generally not been used in the design of water systems. This work presents a multi-disciplinary design optimization approach to desalination system design to minimize the total water production cost of a 30,000m"
26,"Photovoltaic reverse osmosis (PVRO) systems can provide a viable clean water source for many remote communities. To be cost-effective, PVRO systems need to be custom-tailored for the local water demand, solar insolation, and water characteristics. Designing a custom system composed of modular components is not simple due to the large number of design choices and the variations in the sunlight and demand. This paper presents a modular design architecture, which when implemented on a low-cost PC, would enable users to configure systems from inventories of modular components. The method uses a hierarchy of filters or design rules, which can be provided in the form of an expert system, to limit the design space. The architecture then configures a system from the reduced design space using a genetic algorithm to minimize the system lifetime cost subject to system constraints. The genetic algorithm uses a detailed cost model and physics-based PVRO system model which determines the ability of the system to meet demand. Determining the ability to meet demand is challenging due to variations in water demand and solar radiation. Here, the community’s historical water demand, solar radiation history, and PVRO system physics are used in a Markov model to quantify the ability of a system to meet demand or the loss-of-water probability (LOWP). Case studies demonstrate the approach and the cost-reliability trade-off for community-scale PVRO systems. In addition, long-duration simulations are used to demonstrate the Markov model appropriately captures the uncertainty."
27,"This paper provides justification for solar-powered electrodialysis desalination systems for rural Indian villages. It is estimated that 11% of India’s 800 million people living in rural areas do not have access to an improved water source. If the source’s quality in regards to biological, chemical, or physical contaminants is also considered, this percentage is even higher. User interviews conducted by the authors and in literature reveal that users judge the quality of their water source based on its aesthetic quality (taste, odor, and temperature). Seventy-three percent of Indian villages rely on groundwater as their primary drinking supply. However, saline groundwater underlies approximately 60% of the land area in India. Desalination is necessary in order to improve the aesthetics of this water (by reducing salinity below the taste threshold) and remove contaminants that cause health risks."
28,"Wind turbine tower design looks primarily at the structural integrity and durability of the tower. Optimization techniques are sometimes employed to maximize the loading capability while reducing material use and cost. Still, the tower is a dynamic part of a complex wind energy conversion system. During system operation, the tower is excited and sways back and forth. This undesirable movement increases cyclical loading on the tower and drivetrain components. To minimize this motion the tower frequency must be offset from the natural frequency of other components. Hence, it is necessary to look at the relationships that exist between the tower and other wind turbine components, such as the rotor, nacelle, and foundation. In addition, tradeoffs between cost, structural performance, and environmental impact can be examined to guide the designer toward a truly sustainable alternative to fossil fuels. Ultimately, an optimal design technique can be implemented and used to automate tower design. This work will introduce the analytical model and decision-making architecture that can be used to incorporate greater considerations in future studies. In this paper, nine wind turbine tower designs with different materials and geometries are analyzed using Finite Element Analysis (FEA). The optimal tower design is selected using a multi-level variation of the Hypothetical Equivalents and Inequivalents Method (HEIM). Using this analysis, a steel tower with variable thickness has been chosen. The findings reaffirm that steel is a favorable choice for turbine tower construction as it performs well on environmental, performance, and cost objectives. The method proposed in this work can be expanded to examine additional design goals and present a higher fidelity model of the wind turbine tower system in future work."
29,"The market is a complex system with many different stakeholders and interactions. A number of decisions within this system affect the design of new products, not only from design teams but also from consumers, producers, and policy-makers. Market systems studies have shown how profit-optimal producer decisions regarding product design and pricing can influence a number of different factors including the quality, environmental impact, production costs, and ultimately consumer demand for the product. This study models the ways that policies and consumer demand combine in a market systems framework to influence optimal product design and, in particular, product quality and environmental sustainability. Implementing this model for the design of a mobile phone case shows how different environmental impact assessment methods, levels of taxation, and factors introduced to the consumer decision-making process will influence producer profits and overall environmental impacts. This demonstrates how different types of policies might be evaluated for their effectiveness in achieving economic success for the producer and reduced environmental impacts for society, and a “win-win” scenario was uncovered in the case of the mobile phone."
30,"Consideration set formation using non-compensatory screening rules is a vital component of real purchasing decisions with decades of experimental validation. Marketers have recently developed statistical methods that can estimate quantitative choice models that include consideration set formation via non-compensatory screening rules. But is capturing consideration within models of choice important for design? This paper reports on a simulation study of a vehicle portfolio design when households screen over vehicle body style built to explore the importance of capturing consideration rules for optimal designers. We generate synthetic market share data, fit a variety of discrete choice models to this data, and then optimize design decisions using the estimated models. Model predictive power, design “error”, and profitability relative to ideal profits are compared as the amount of market data available increases. We find that even when estimated compensatory models provide relatively good predictive accuracy, they can lead to sub-optimal design decisions when the population uses consideration behavior; convergence of compensatory models to non-compensatory behavior is likely to require unrealistic amounts of data; and modeling heterogeneity in non-compensatory screening is more valuable than heterogeneity in compensatory trade-offs. This supports the claim that designers should carefully identify consideration behaviors before optimizing product portfolios. We also find that higher model predictive power does not necessarily imply better design decisions; that is, different model forms can provide “descriptive” rather than “predictive” information that is useful for design."
31,"Mapua Institute of Technology has been constantly engaged in providing free, renewable energy to rural and under privileged communities. Guided by the mission and vision of the School of Mechanical and Manufacturing Engineering Department and of the Office of Social Orientation and Community Involvement, the school had implemented several renewable energy activities. This paper showcases 8 different projects — 6 hydropower plant projects, 1 human kinetic harvesting demonstration facility, and 1 wind turbine project. In this paper, implemented projects are presented briefly with emphasis on the different locations, local cultural settings and different experiences encountered. It will also share how students have changed from being participants to autonomous implementers of renewable energy projects for communities."
32,"Product design decision makers are frequently challenged by the difficulty of ensuring compatibility of parts sourced from various suppliers, including the services that the product is designed to integrate. The crux of the difficulty is in analyzing the ability of sourced parts (and services) to interoperate under uncertainty, and the impact of such compatibility on the overall marketing objectives. Therefore, the decisions in a design for market system problem can be closely related to the considerations along both the upstream (e.g., suppliers) and downstream (e.g., service providers and customers) market systems. This paper fills a gap in the existing research by exploring a design decision method that integrates upstream and downstream market systems with interoperability considerations. The proposed method is based on a mathematical model and metric for interoperability presented for the first time in the literature and particularly in the context of engineering design. The design decision framework is demonstrated using three examples: a mechanical design tolerance problem, a power tool design problem, and a tablet computer design problem. The mechanical design problem demonstrates how the interoperability metric can be used as a new way of analyzing tolerances in mechanical systems. The power tool design example involves an integration of upstream and downstream market systems for design selection. The tablet computer design selection problem considers not only the upstream suppliers but also customers and digital service providers along the downstream market system."
33,"This paper explores opportunities for reductions in lifecycle greenhouse gas (GHG) emissions through adoption of electric drive vehicles (EDV), including hybrid, plug-in hybrid and battery electric vehicles. EDVs have generally lower GHG emission rates during operation than similar-class conventional vehicles (CV). However, a key observation is that GHG reductions per mile are much larger during city driving conditions than on the highway. An examination of the estimated GHG emissions is conducted for city and highway driving conditions for several CV and EDV models based on testing results from the US Environmental Protection Agency (EPA), then compared with key findings from the 2009 National Household Travel Survey (NHTS 2009). Through an empirical analysis of actual driving patterns in the U.S., this study highlights potential missed opportunities to reduce transportation GHG emissions through the allocation of incentives and/or regulations. Key findings include the significant potential to reduce GHG emissions of taxis and delivery vehicles, as well as driving pattern-based incentives for individual vehicle owners."
34,"Conjoint analysis from marketing has been successfully integrated with engineering analysis in design for market systems. The long questionnaires needed for conjoint analysis in relatively complex design decisions can become cumbersome to the human respondents. This paper presents an adaptive questionnaire generation strategy that uses active learning and allows incorporation of engineering knowledge in order to identify efficiently designs with high probability to be optimal. The strategy is based on viewing optimal design as a group identification problem. A running example demonstrates that a good estimation of consumer preference is not always necessary for finding the optimal design and that conjoint analysis could be configured more effectively for the specific purpose of design optimization. Extending the proposed method beyond a homogeneous preference model and noiseless user responses is also discussed."
35,"About 80% of farms in India are less than five acres in size and are cultivated by farmers who use bullocks for farming operations. Even the smallest tractors available in the Indian market are too expensive and large, and not designed to meet the unique requirements of these farmers. To address these needs, we have developed a proof-of-concept lightweight (350 kg) tractor in collaboration with Mahindra and Mahindra Limited, an Indian tractor manufacturer. Given the challenges of accurately predicting traction in Indian soils by applying existing terramechanics models, an alternative design approach based on Mohr-Coulomb soil-failure criterion is presented. Analysis of weight, power and drawbar of existing tractors on the market, a single wheel traction test, and a drawbar test of a proof-of-concept small tractor prototype suggest that ∼200kg is the maximum drawbar force that could be achieved by a 350kg tractor of conventional design. In order to attain higher drawbar performance of 70% of the tractor weight needed for specific agricultural operations, additional design changes are required. An approach for increasing traction by adding tires is investigated and discussed. Additional research on weight distribution, dynamic drawbar testing and tread design is suggested as future work."
36,"A major barrier in consumer adoption of electric vehicles (EVs) is ‘range anxiety,’ the concern that the vehicle will run out of power at an inopportune time. Range anxiety is caused by the current relatively low electric-only operational range and sparse public charging station infrastructure. Range anxiety may be significantly mitigated if EV manufacturers and charging station operators work in partnership using a cooperative business model to balance EV performance and charging station coverage. This model is in contrast to a sequential decision making model where manufacturers bring new EVs to the market first and charging station operators decide on charging station deployment given EV specifications and market demand. This paper proposes an integrated decision making framework to assess profitability of a cooperative business models based on a multi-disciplinary optimization model that combines marketing, engineering, and operations. This model is demonstrated in a case study involving battery electric vehicle design and direct-current fast charging station location network in the State of Michigan. The expected benefits can motive both government and private enterprise actions."
37,"To be competitive in today’s market, firms need to offer a variety of products that appeal to a diverse set of customer needs. Product line optimization provides a simple method to design for this challenge. Using a heterogeneous customer preference model allows the optimization to better explore the diversity in the market. The optimization should also consider aesthetic, engineering, manufacturing, and marketing constraints to ensure the feasibility of the final solution. However, as more constraints are added the difficulty of the optimization increases. There is an opportunity to reduce the difficulty of the optimization by allowing the heterogeneous customer preference model to handle a subset of these constraints termed design prohibitions. Design prohibitions include component incompatibility and dependency. This paper investigates whether design prohibitions should be handled solely in the heterogeneous customer preference model, solely in the optimization formulation, or in both. The effects of including design prohibitions in the creation of a hierarchical Bayes mixed logit model and a genetic algorithm based product line optimization are explored using a bicycle case study."
38,"A new metamodeling approach is proposed to characterize the output (response) random process of a dynamic system with random variables, excited by input random processes. The metamodel is then used to efficiently estimate the time-dependent reliability. The input random processes are decomposed using principal components or wavelets and a few simulations are used to estimate the distributions of the decomposition coefficients. A similar decomposition is performed on the output random process. A Kriging model is then built between the input and output decomposition coefficients and is used subsequently to quantify the output random process corresponding to a realization of the input random variables and random processes. In our approach, the system input is not deterministic but random. We establish therefore, a surrogate model between the input and output random processes. The quantified output random process is finally used to estimate the time-dependent reliability or probability of failure using the total probability theorem. The proposed method is illustrated with a corroding beam example."
39,"This paper presents a robust design framework to develop piezoelectric materials based structural sensing systems for failure diagnostics and prognostics. At first, a detectability measure is proposed to evaluate the performance of any given sensing system given various uncertainties. Thus, the censoring system design problem can be formulated to maximize the detectability of the censoring system through optimally allocating piezoelectric materials into a target structural system. Secondly, the formulated problem can be conveniently solved using reliability-based robust design framework to ensure design robustness while considering the uncertainties. Two engineering case studies are employed to demonstrate the effectiveness of the design framework in developing multifunctional material sensing systems."
40,"Lifecycle health management plays an increasingly important role in realizing resilience of aging complex engineered systems since it detects, diagnoses, and predicts system-wide effects of adverse events, therefore enables a proactive approach to deal with system failures. To address an increasing demand to develop high-reliability low-cost systems, this paper presents a new platform for operational stage system health management, referred to as Evolving Design Model Synchronization (EDMS), which enables health management of aging engineered systems by efficiently synchronizing system design models with degrading health conditions of actual physical system in operation. A Laplace approximation approach is employed for the design model updating, which can incorporate heterogeneous operating stage information from multiple sources to update the system design model based on the information theory, thereby increases the updating accuracy compared with traditionally used Bayesian updating methodology. The design models synchronized over time using sensory data acquired from the system in operation can thus reflect system health degradation with evolvingly updated design model parameters, which enables the application of failure prognosis for system health management. One case study is used to demonstrate the efficacy of the proposed approach for system health management."
41,"The concept of engineering resilience has received prevalent attention from academia as well as industry because it contributes a new means of thinking about how to withstand against disruptions and recover properly from them. Although the concept of resilience was scholarly explored in diverse disciplines, there are only few which focus on how to quantitatively measure the engineering resilience. This paper is dedicated to explore the gap between quantitative and qualitative assessment of engineering resilience in the domain of design of complex engineered systems. A conceptual framework is first proposed for the modeling of engineering resilience, and then Bayesian network is employed as a quantitative tool for the assessment and analysis of engineering resilience for complex systems. A case study related to electric motor supply chain is employed to demonstrate the proposed approach. The proposed resilience quantification and analysis approach using Bayesian networks would empower system designers to have a better grasp of the weakness and strength of their own systems against system disruptions induced by adverse failure events."
42,"Safe and reliable operation of lithium-ion batteries as major energy storage devices is of vital importance, as unexpected battery failures could result in enormous economic and societal losses. Accurate estimation of the state-of-charge (SoC) and state-of-health (SoH) for an operating battery system, as a critical task for battery health management, greatly depends on the validity and generalizability of battery models. Due to the variability and uncertainties involved in battery design, manufacturing, and operation, developing a generally applicable battery physical model is a big challenge. To eliminate the dependency of SoC and SoH estimation on battery physical models, this paper presents a generic self-cognizant dynamic system approach for lithium-ion battery health management, which integrates an artificial neural network (ANN) with a dual extended Kalman filter (DEKF) algorithm. The ANN is trained offline to model the battery terminal voltages to be used by the DEKF. With the trained ANN, the DEKF algorithm is then employed online for SoC and SoH estimation, where voltage outputs from the trained ANN model are used in DEKF state-space equations to replace the battery physical model. Experimental results are used to demonstrate the effectiveness of the developed self-cognizant dynamic system approach for battery health management."
43,"Real-time monitoring systems have been developed for tooling machine for the purpose of investigating the time-dependent cutting conditions, to detect instantaneous events, and to estimate life of cutting tools and the machine itself. An Energy-based Reliability Model (ERM) has been developed for real-time monitoring of cutting conditions. A standardized inspection process was defined and the two most sensible signals, vibrational signals and temperature increments, are collected to monitor the accumulation of dissipated energy during the tooling processes. The ERM then computes the normalized accumulative dissipated energy in replace of evaluating surface quality of workpiece at the end of each tooling process. This paper focuses on the implementation of ERM in the turning process on a lathe. The experimental results showed the dissipated energy linearly grows with respect to the amount of volume removal from the workpiece. The ERM built from the experimental results under the same condition were then utilized to estimate the turning performance under different experimental conditions. As a result, similar trends of dissipated energy versus volume removal were found. Therefore, ERM can be utilized to estimate a reliable replacement time of cutting tool in tooling machines."
44,"One of most important components in power generator is a stator winding since an unexpected failure of the water absorbed-winding leads to plant shut-down and substantial loss. Typically the stator winding is maintained with a time- or usage-based strategy, which could result in substantial waste of remaining life, high maintenance cost and low plant availability. Recently, the field of prognostics and health management offers general diagnostic and prognostic techniques to precisely assess the health condition and robustly predict the remaining useful life of an engineered system, with an aim to address the aforementioned deficiencies. This research aims at developing health reasoning system of power generator stator winding with physical and statistical analysis against water absorption. And it is based upon the capacitance measurements on winding insulations. In particular, a new health measure, Directional Mahalanobis Distance (DMD), is proposed to quantify the health condition. In addition, the empirical health grade system based upon the proposed technique, DMD, is carried out with the maintenance history. The smart health reasoning system is validated using eight years’ field data from eight generators, each of which contains forty two windings."
45,"Design of engineering resilient systems is an emerging research field. The contribution of this paper is to i) define engineering resilience on the basis of various resilience concepts in different fields; ii) propose the engineering recoverability as a new component in the framework of designing engineering resilient systems; and iii) introduce a general mathematical formulation to quantify the engineering resilience. One case study of a CNC machining system is used to demonstrate the value of designing engineering resilient systems."
46,"The question of how to effectively design products for consumers in the developing world has been widely debated. Several methodologies have been developed to address this issue focusing on human centered and community centered methods, but few methods are rooted in market-centered approaches. Recent advances in market-centered design from lean startup methodologies hold promise for the development of new methods that allow effective product design for consumers in the developing world. This paper contributes a method from which consumer level products can be designed to effectively supply the under-served markets of the developing world with innovative and sustainable solutions. Utilizing an iterative method based on three fundamental hypotheses, the Lean Design for Developing World Method (LDW) seeks to provide products that are economically viable, have strong market growth potential, and have a net positive impact on the customers and their communities."
47,"The development of energy services for the 40% of the world’s population currently living in energy poverty is a challenging design problem. There are competing and often conflicting objectives between stakeholders from global to user viewpoints, and the confounding effects of real-world performance, rebound, and stacking of technologies makes the determination of optimal strategies for off-grid village energy complicated. Yet there are holistic and lasting solutions that can adequately address the technical, social, economic, and environmental constraints and satisfy the goals of all stakeholders. These solutions can be better identified by systematically considering five major qualitative and quantitative outcomes including 1) energy access and efficiency, 2) climate benefits, 3) health impacts, 4) upfront and recurring economic and opportunity costs, and 5) quality of life for the user in terms of several metrics. Beginning with a comprehensive survey of energy uses in a village and current and potential technological options to meet those needs, this article proposes a methodology to identify and quantify these five outcomes for various intervention scenarios. These evaluations can provide a better understanding of the constraints, trade-offs, sensitivity to various factors, and conditions under which certain designs are appropriate for the village energy system. Ultimately a balance of all five objectives is most likely to result in equitable, user-driven, and sustainable solutions."
48,"There are currently 1.4 billion people in the world living on less than $1.25 a day. Many engineers have designed products intended to alleviate the poverty faced by these individuals but most of these products have failed to have the desired impact. This is largely because we as engineers do not clearly understand the needs of people in poverty, which is understandable as it is particularly hard to determine needs in this context. This lack of understanding is usually because the engineer and the resource-poor individuals are from different cultures, because the engineer has no personal experience with life in poverty, and because the engineer has limited access to suitable market surrogates for testing and validation. This paper presents a method for determining the needs of resource-poor individuals in the developing world. The method presented here is organized into four steps to be completed within three different stages of need finding. Engineers and designers can follow these steps to more accurately determine the needs of resource-poor individuals as they design a product. The paper also includes examples of this method being used to determine customer needs for products in India and Peru."
49,"Many household electronic devices — flashlights, stereos, radios — require AA, AAA, C, and D size batteries. These batteries are often disposable in remote areas of the world that lack access to grid electricity. In parts of the globe, disposable batteries can account for over 50% of household energy expenditures and amount to 25 or more batteries disposed of per person per year. This amounts to more than 25,000 batteries annually for a village of 1000 people. Solutions to this problem can address economic and environmental concerns. Replacing disposable batteries with rechargeable batteries maintained by a local entrepreneur is one business-driven method to reduce environmental waste and household energy expenditures. This study evaluates technical options for providing rechargeable batteries to a decentralized population, and introduces a prototype portable charging kit that addresses the techno-economic requirements of charging batteries, delivering batteries at a reasonable cost to consumers, providing a profit margin for local entrepreneurs, and allowing for portability during travel between villages or refugee camps. The unit includes a solar PV power source, a lead-acid battery for intermediate energy storage, a battery charger equipped with single cell batteries, a charge controller to manage power flow, and a protective suitcase to house the equipment."
50,"Water-lifting technologies in rural areas of the developing world have enormous potential to stimulate agricultural and economic growth. The treadle pump, a human-powered low-cost pump designed for irrigation in developing countries, can help farmers maximize financial return on small plots of land by ending their dependency on rain-fed irrigation systems. The treadle pump uses a suction piston to draw groundwater to the surface by way of a foot-powered treadle attached to each suction piston. Current treadle pump designs lift water from depths up to 7 meters at a flow-rate of 1–5 liters per second. This work seeks to optimize the design of the Dekhi style treadle pump, which has gained significant popularity due to its simplicity. A mathematical model of the working fluid and treadle pump structure has been developed in this study. Deterministic optimization methods are then employed to maximize the flow rate of the groundwater pumped, maximize the lift height, and minimize the volume of material used for manufacturing. Design variables for the optimization included the dimensions of the pump, well depth, and speed of various parts of the system. The solutions are subject to constraints on the geometry of the system, the bending stress in the treadles, and ergonomic factors. Findings indicate that significant technical improvements can be made on the standard Dekhi design, such as increasing the size of the pump cylinders and hose, while maintaining a standard total treadle length. These improvements could allow the Dekhi pump to be implemented in new regions and benefit additional rural farmers in the developing world."
51,"Conceptual design of multidisciplinary systems begins with a description of requirements and proceeds with a solution at a high abstraction level. A systematic and rigorous approach is required to evaluate complex systems and can be achieved by mapping the interactions between disciplines. Research has shown that the use of geometry in the early stages act as enablers for high fidelity analyses as required information can be extracted from the model. In the paper, Knowledge Based Engineering is used with the aim of managing the added complexity as it supports design automation and reuse. This article describes a configuration tool, which allows for quick generation of train geometry using High Level CAD Templates. The tool was created as part of a research project, with the primary objective of the development of a robust framework for a Multidisciplinary Design Optimization process which can support design of high-speed trains."
52,"This paper presents a new methodology for modeling complex engineered systems using complex networks for failure analysis. Many existing network-based modeling approaches for complex engineered systems “abstract away” the functional details to focus on the topological configuration of the system and thus do not provide adequate insight into system behavior. To model failures more adequately, we present two types of network representations of a complex engineered system: a uni-partite architectural network and a weighted bi-partite behavioral network. Whereas the architectural network describes physical inter-connectivity, the behavioral network represents the interaction between functions and variables in mathematical models of the system and its constituent components. The levels of abstraction for nodes in both network types affords the evaluation of failures involving morphology or behavior, respectively. The approach is shown with respect to a drivetrain model. Architectural and behavioral networks are compared with respect to the types of faults that can be described. We conclude with considerations that should be employed when modeling complex engineered systems as networks for the purpose of failure analysis."
53,"Design decision-making involves trade-offs between many design variables and attributes, which can be difficult to model and capture in complex engineered systems. To choose the best design, the decision-maker is often required to analyze many different combinations of these variables and attributes and process the information internally. Trade Space Exploration (TSE) tools, including interactive and multi-dimensional data visualization, can be used to aid in this process and provide designers with a means to make better decisions, particularly during the design of complex engineered systems. In this paper, we investigate the use of TSE tools to support decision-makers using a Value-Driven Design (VDD) approach for complex engineered systems. A VDD approach necessitates a rethinking of trade space exploration. In this paper, we investigate the different uses of trade space exploration in a VDD context. We map a traditional TSE process into a value-based trade environment to provide greater decision support to a design team during complex systems design. The research leverages existing TSE paradigms and multi-dimensional data visualization tools to identify optimal designs using a value function for a system. The feasibility of using these TSE tools to help formulate value functions is also explored. A satellite design example is used to demonstrate the differences between a VDD approach to design complex engineered systems and a multi-objective approach to capture the Pareto frontier. Ongoing and future work is also discussed."
54,"In engineering design, the volume and weight of a number of systems consisting of valves and plumbing lines often need to be minimized. In current practice, this is facilitated under empirical experience with trial and error, which is time-consuming and may not yield the optimal result. This problem is intrinsically difficult due to the challenge in the formulation of optimization problem that has to be computationally tractable. In this research, we choose a sequential approach towards the design optimization, i.e., first optimizing the placement of valves under prescribed constraints to minimize the volume occupied, and then identifying the shortest paths of plumbing lines to connect the valves. In the first part, the constraints are described by analytical expressions, and two approaches of valve placement optimization are reported, i.e., a two-phase method and a simulated annealing-based method. In the second part, a three-dimensional routing algorithm is explored to connect the valves. Our case study indicates that the design can indeed be automated and design optimization can be achieved under reasonable computational cost. The outcome of this research can benefit both existing manufacturing practice and future additive manufacturing."
55,"A transmission gear is generally produced by a sequence of several processes from steelmaking to final machining and surface treatment. The intermediate processes such as hot rolling induce microstructure evolution and phase transformation which play a significant role in determining the mechanical properties and fatigue strength of gears. Therefore, these intermediate processes should be carefully considered in determining the performance and properties of the end product."
56,"The evolution of meso-structures in the development of the shear band of Michelin’s non-pneumatic tire, the Tweel, is presented in this paper. Designers and researchers at Clemson University worked on a research projects with Michelin to support NIST efforts in fuel efficiency improvement and NASA efforts in manned exploration systems. The goal of each was to replace the elastomeric material of shear band with materials which can tolerate harsh temperatures and shear loads or to replace the materials with linear elastic low-hysteretic loss materials. The concepts initially proposed by ideation method were prototyped for physical testing. A case study examining the documentation reports for each project is conducted to provide a reflective understanding of how the evolution in the projects occurred. The goal of developing this retrospective is to try to identify guidelines and approaches that could be integrated into a designer driven systematic approach for custom design of meso-structures."
57,"In this paper, we explore the design space associated with the realization of automobile steel gear blanks manufactured using hot forging. ABAQUS, a commercially available software package, is used to simulate the hot forging process. Surrogate models are developed and used to model the solution space. Then the compromise Decision Support Problem (cDSP) is used to explore the feasible space. Results are presented as ternary plots that allow a designer to visualize the trade-offs between competing goals such as reducing the size of the underfill, reducing the flash (excess material), waste of forged materials, energy required for deformation and crack susceptibility, on various possible solutions. The efficacy of the exploration of the design space and trade-offs between competing solutions is illustrated."
58,"Finding the input specifications to obtain the specified performance of a component being designed is an essential activity of a designer. However, obtaining solutions for this inverse problem is a complex task; especially when there are multiple steps with many-to-one mappings at each step in the forward problem. This complexity is further augmented in the presence of uncertainty of the parameters and models used."
59,"Continuous casting is a crucial step in the production of a variety of steel products. Its performance is measured in terms of conflicting objectives including productivity, yield, quality and production costs. These are conflicting in the sense that, if the productivity is increased, there is a reduction in other performance parameters. These performance parameters are greatly influenced by operating conditions such as casting speed, superheat, mold oscillation frequency, and secondary cooling conditions. An optimized solution for continuous casting process can be obtained. However uncertainty in operating parameters which affects the performance of caster is rarely considered. Moreover, the solution obtained is optimal with respect to a particular performance measure and does not provide a balance between all. In this paper an integrated design framework has been developed based on metamodels and the compromise Decision Support Problem (cDSP). The framework developed deals with uncertainty and yields robust solutions for performance measures. Further, the design space for continuous casting has been explored for different scenarios to determine satisficing solutions. The utility of the framework has been illustrated for providing decision support when an existing configuration for continuous casting is unable to meet the requirements. This approach can be instantiated for other unit operations involved in steel manufacturing and then may be integrated to simulate the entire production cycle of steel manufacturing. This in turn will enable development of materials with specific properties and reduce the time and cost incurred in the development of new materials and their manufacturing."
60,"Due to the stringent requirements of industry, it has become extremely important to have a careful control over the required performance and properties of steels. Performance and properties of advanced high strength steel depend significantly on its cleanliness. Cleanliness is achieved by restricting the inclusion count to a permissible limit. Over the past few years, there has been increased use of tundish, a device that acts as a buffer between ladle and mold, for controlling inclusions. Apart from facilitating inclusion removal, tundish also maintains low dead volume and thermal and chemical homogeneity, which is required for smooth casting operation. Thus, performance of the tundish operation greatly influences the properties and quality of the cast slab. Tundish performance is generally assessed using parameters such as inclusion removal efficiency, dead volume within tundish and effectiveness in maintaining the desired amount of superheat. But, the aforesaid parameters are conflicting in nature. Managing the conflict and providing a satisficing solution based on the customer requirements become essential."
61,"In multi-scale materials modeling, it is desirable that different levels of details can be specified in different regions of interest without the separation of scales so that the geometric and physical properties of materials can be designed and characterized. Existing materials modeling approaches focus on the representation of the distributions of material compositions captured from images. In this paper, a multi-scale materials modeling method is proposed to support interactive specification and visualization of material microstructures at multiple levels of details, where designer’s intent at multiple scales is captured. This method provides a feature-based modeling approach based on a recently developed surfacelet basis. It has the capability to support seamless zoom-in and zoom-out. The modeling, operation, and elucidation of materials are realized in both the surfacelet space and the image space."
62,"A set-based approach is presented for solving multi-scale or multi-level design problems. The approach incorporates Bayesian network classifiers (BNC) for mapping design spaces at each level and flexibility metrics for intelligently narrowing the design space as the design process progresses. The approach is applied to a hierarchical composite materials design problem, specifically, the design of composite materials with macroscopic mechanical stiffness and loss properties surpassing those of conventional composites. This macroscopic performance is achieved by embedding small volume fractions of negative stiffness (NS) inclusions in a host material. To design these materials, the set-based, multilevel design approach is coupled with a hierarchical modeling strategy that spans several scales, from the behavior of microscale NS inclusions to the effective properties of a composite material containing those inclusions and finally to the macroscopic performance of components. The approach is shown to increase the efficiency of multi-level design space exploration, and it is particularly appropriate for top-down, performance-driven design, as opposed to bottom-up, trial-and-error modeling. The design space mappings also build intuitive knowledge of the problem and promising regions of the design space, such that it is almost trivial to identify designs that yield preferred system-level performance."
63,"In designing microstructural materials systems, one of the key research questions is how to represent the microstructural design space quantitatively using a descriptor set that is sufficient yet small enough to be tractable. Existing approaches describe complex microstructures either using a small set of descriptors that lack sufficient level of details, or using generic high order microstructure functions of infinite dimensionality without explicit physical meanings. We propose a new machine learning-based method for identifying the key microstructure descriptors from vast candidates as potential microstructural design variables. With a large number of candidate microstructure descriptors collected from literature covering a wide range of microstructural material systems, a 4-step machine learning-based method is developed to eliminate redundant microstructure descriptors via image analyses, to identify key microstructure descriptors based on structure-property data, and to determine the microstructure design variables. The training criteria of the supervised learning process include both microstructure correlation functions and material properties. The proposed methodology effectively reduces the infinite dimension of the microstructure design space to a small set of descriptors without a significant information loss. The benefits are demonstrated by an example of polymer nanocomposites optimization. We compare designs using key microstructure descriptors versus using empirically-chosen microstructure descriptors to validate the proposed method."
64,"The development of new materials must start with an understanding of their phase stability. Researchers have used the CALPHAD method to develop self-consistent databases encoding the thermodynamics of phases. In this "
65,"This paper presents a B-spline based approach for topology optimization of three-dimensional (3D) problems where the density representation is based on B-splines. Compared with the usual density filter in topology optimization, the new B-spline based density representation approach is advantageous in both memory usage and CPU time. This is achieved through the use of tensor-product form of B-splines. As such, the storage of the filtered density variables is linear with respect to the effective filter size instead of the cubic order as in the usual density filter. Numerical examples of 3D topology optimization of minimal compliance and heat conduction problems are demonstrated. We further reveal that our B-spline based density representation resolves the bottleneck challenge in multiple density per element optimization scheme where the storage of filtering weights had been prohibitively expensive."
66,"Conventionally, design domain of topology optimization is predefined and is not adjusted in the design optimization process since designers are required to specify the design domain in advance. However, it is difficult for a fixed design domain to satisfy design requirements such as domain sizing adjustment or boundaries change. In this paper, Domain Composition Method (DCM) for structural optimization is presented and it deals with the design domain adjustment and the material distribution optimization in one framework. Instead of treating design domain as a whole, DCM divides domain into several subdomains. Additional scaling factors and subdomain transformations are applied to describe changes between different designs. It then composites subdomains and solve it as a whole in the updated domain. Based on the domain composition, static analysis with DCM and sensitivity analysis are derived. Consequently, the design domain and the topology of the structure are optimized simultaneously. Finally, the effectiveness of the proposed DCM for structural optimization is demonstrated through different numerical examples."
67,"We look to expand the reach of continuum topology optimization to include the design of ‘structures’ that gain functionality or are specifically manufactured from discrete, non-overlapping objects. While significant advancements have been made in restricting the geometric properties of topology-optimized structures, including restricting the minimum and maximum length scale of features, continuum topology optimization is still largely limited to monolithic structures. A wide variety of structures and materials, however, gain their stiffness or functionality from discrete objects, such as fiber-reinforced composites. This work examines a recently developed method for optimizing the distribution of discrete objects (2d inclusions) across a design domain and extends the approach to variable shape and variable sized objects that must be selected from a designer-defined set. This essentially enables simultaneous optimization of object sizes, shapes, and/or locations within the projection framework, without need for additional constraints. As in traditional topology optimization, gradient-based optimizers are used with sensitivity information estimated via the adjoint method, solved using finite element analysis. The algorithm is demonstrated on benchmark problems in structural design for the case where the objects are stiff inclusions embedded in a compliant matrix material."
68,"In this paper we present a new, efficient algorithm for computing the “raw offset” curves of 2D polygons with holes. Prior approaches focus on (a) complete computation of the Voronoi Diagram, or (b) pair-wise techniques for generating a raw offset followed by removal of “invalid loops” using a sweepline algorithm. Both have drawbacks in practice. Robust implementation of Voronoi Diagram algorithms has proven complex. Sweeplines take "
69,"Physical prototyping is an important stage of product design where designers have a chance to physically evaluate and alter digitally created surfaces. In these scenarios, designers generate a digital model, manufacture and alter the prototype as needed, and redigitize the prototype through scanning. Despite the variety of reverse engineering tools, redigitizing the prototypes into forms amenable to further digital editing remains a challenge. This is because current digitization methods cannot take advantage of the key elements of the original digital model such as the wireframe topology and surface flows. This paper presents a new reverse engineering method that augments conventional digitization with the knowledge of the original digital model’s curve topology to enhance iterative shape design activities. Our algorithm takes as input a curve network topology forming a subdivision control cage and a 3D scan of the physically modified prototype. To facilitate the digital capture of the physical modifications, our algorithm performs a series of registration, correspondence and deformation calculations to compute the new configuration of the initial control cage. The key advantage of the proposed technique is the preservation of the edge flows and initial topology while transferring surface modifications from prototypes. Our studies show that the proposed technique can be particularly useful for bridging the gap between physical and digital modeling in the early stages of product design."
70,"Functionally Gradient Materials (FGM) smoothly transition from one material to another within a single object, allowing engineers to customize the physical response of different regions of the object by modifying the material composition at each region. New FGM research makes design, manufacturing, and use of FGM objects a promising alternative to homogeneous objects or composites with one direction of gradation. Heterogeneous anisotropic artifacts can be manufactured with specific 3D printing processes and potentially bring significant increases in functionalities. However, many challenges exist while designing and manufacturing these objects. This paper explores these challenges and suggests needed research. In particular the ability to model FGM objects, setup and run optimization algorithms, create manufacturing process plans, and control the manufacturing process all need more research and better software tools. In addition, researchers must rigorously test optimally designed FGM objects in order to validate the FGM object properties and the FGM design process before adoption of FGM objects by industry is likely to occur."
71,"Increasing the strength of the gear tooth is a recurrent demand from industry. The authors report a novel approach to the design of tooth-root profile of spur gears using cubic splines, with the aim of investigating the effect of tooth-root geometry on stress concentration in order to increase the gear tooth strength by optimizing the root profile. An iterative co-simulation procedure, consisting of tooth-root profile shape synthesis via nonlinear programming and finite element analysis software tools is conducted, for the purpose of forming the tooth-root geometry design with the minimum stress concentration. The proposed design was verified to be capable of reducing the stress concentration by 21% over its conventional circular-filleted counterpart. Hence, the results showcase an innovative and sound methodology for the design of the tooth-root profile to increase gear load-carrying capacity."
72,"Tolerances are specified by a designer to allow reasonable freedom by a manufacturer for imperfections and inherent variability without compromising performance. It takes knowledge and experience to create a good tolerance scheme. It is a tedious process driven by the type of parts, their features and controls needed for each one of them. In this paper, we investigate the extent to which GD&T schema development can be automated. Automated tolerance schema generation, requires identifying critical tolerance loops. The tolerance loop is a loop of dimensions between faces of features governing assembly conditions. For this purpose, the first major step is to identify mating features called assembly features. Also, in order to create the tolerance chains we need Local Constraints (assembly feature relationships), Global constraints (part feature relationships) and directions of control. In addition, we have to identify feature patterns since they have associated tolerances according to Dimensioning and Tolerancing Standard ASME Y14.5M. Directions in which these loops lie are also needed; we call them Direction of Control (DoC). Then we can create the GD&T schema, allocate tolerance values, and prepare it for tolerance evaluation. In this paper, we present an approach to automatically identify the dimensional loops based on assembly requirements. Assignment of tolerance values will be covered in future works as it is based on design function."
73,"In this digital era, the natures of services are becoming increasingly complex and diverse due to the convergences between the existing human-centered services and other supportive device services, or interactions between heterogeneous services. Expecting this trend is to be accelerated more, new scientific and engineered approaches to the service design are needed more than ever. In this context, a service designed conceptually and abstractly has significant limitations that keep the customers’ satisfaction from advancing above a certain level. Hence, in an initial service design phase, a service delivery process can be expressed quantitatively through a systematic analysis of its natures and the goals. In this paper, a human-centered complex service system is newly defined as service ecosystem. This study proposes a method for designing service processes as a Discrete Event System (DES) in formal ways by utilizing the concepts of affordance, preference, and a Affordance-based Finite State Automata (FSA) modeling methodology. The proposed design method suggests a model framework that focuses on service actions that reflects related properties of customers, employees, and environment entities and their interactions quantitatively. The formally expressed relations between properties of service entities such as customer, employee, and service environment provide guidelines for service designers in a more scientific way than traditional ones. In addition, it is expected that it will enable to add computational to the human-centered service system design and control and develop effective reusable controllers for complex service deliveries as well."
74,"Total Shoulder Arthroplasty is performed on patients to restore range of motion of the shoulder and decrease pain caused by osteoarthritis at the glenohumeral joint. The glenohumeral joint is a slightly unstable ball and socket joint, where muscles hold the humerus in contact with the glenoid, located on the scapula. Improper sizing or alignment of the implant can cause the surgery to fail to restore mobility to the shoulder or only restore mobility for a limited time. Additionally, placement of the glenoid implant on the scapula is complicated by the limited view available during surgery and the deformation of the glenoid caused by osteoarthritis. Implant designs must take into account the large amount of variability present in both intact and osteoarthritic joints. The purpose of this research is to provide a morphable glenoid representation for the scapula to assist with preoperative planning and implant design. CT scans of healthy and osteoarthritic glenoids were provided by Hershey Medical Center for this study. Principal component analysis and radial basis functions are used to represent a range of potential glenoid geometries, both with and without osteoarthritis. This parametric model can be used to guide the design and sizing of implants. This approach should be extensible to the modeling of other bony surfaces, which can improve both implant design and surgical procedure."
75,"Visualization of the U.S. civilian population provides perspective on the variability of body size and shape, enabling designers and engineers to better understand the needs of their target users. This paper presents a virtual population of digital human models, representative of the U.S. civilian population, and the methods used to create it. It is based on the observed variability in stature, BMI, and waist circumference in the data from the 2007–2010 CDC NHANES survey. The NHANES data were used in conjunction with regression models to develop the required model parameters. The models were then presented such that the variability and distribution of variability in anthropometry can be easily seen."
76,"At the aim of reducing the computational time of engineering design optimization problems using metamodeling technologies, we developed a flexible distributed framework independent of any third-part parallel computing software to implement simultaneous sampling during metamodel-based design optimization procedures. In this paper, the idea and implementation of hardware configuration, software structure, the main functional modular and interfaces of this framework are represented in detail. The proposed framework is capable of integrating black-box functions and legacy software for analyzing and common MBDO methods for space exploring. In addition, a message-based communication infrastructure based on TCP/IP protocol is developed for distributed data exchange. The Client/Server architecture and computing budget allocation algorithm considering software dependency enable samples to be effectively allocated to the distributed computing nodes for simultaneous execution, which gives rise to decreasing the elapsed time and improving MBDO’s efficiency. Through testing on several numerical benchmark problems, the favorable results demonstrate that the proposal framework can evidently save the computational time, and is practical for engineering MBDO problems."
77,"Typical challenges of simulation-based design optimization include unavailable gradients and unreliable approximations thereof, expensive function evaluations, numerical noise, multiple local optima and the failure of the analysis to return a value to the optimizer. The remedy for all these issues is to use surrogate models in lieu of the computational models or simulations and derivative-free optimization algorithms. In this work, we use the "
78,"In this paper, an approach to generate surrogate models constructed by radial basis function networks (RBFN) with a priori bias is presented. RBFN as a weighted combination of radial basis functions only, might become singular and no interpolation is found. The standard approach to avoid this is to add a polynomial bias, where the bias is defined by imposing orthogonality conditions between the weights of the radial basis functions and the polynomial basis functions. Here, in the proposed a priori approach, the regression coefficients of the polynomial bias are simply calculated by using the normal equation without any need of the extra orthogonality prerequisite. In addition to the simplicity of this approach, the method has also proven to predict the actual functions more accurately compared to the RBFN with a posteriori bias. Several test functions, including Rosenbrock, Branin-Hoo, Goldstein-Price functions and two mathematical functions (one large scale), are used to evaluate the performance of the proposed method by conducting a comparison study and error analysis between the RBFN with a priori and a posteriori known biases. Furthermore, the aforementioned approaches are applied to an engineering design problem, that is modeling of the material properties of a three phase spherical graphite iron (SGI). The corresponding surrogate models are presented and compared."
79,"In engineering design, spending excessive amount of time on physical experiments or expensive simulations makes the design costly and lengthy. This issue exacerbates when the design problem has a large number of inputs, or of high dimension. High Dimensional Model Representation (HDMR) is one powerful method in approximating high dimensional, expensive, black-box (HEB) problems. One existing HDMR implementation, Random Sampling HDMR (RS-HDMR), can build a HDMR model from random sample points with a linear combination of basis functions. The most critical issue in RS-HDMR is that calculating the coefficients for the basis functions includes integrals that are approximated by Monte Carlo summations, which are error prone with limited samples and especially with non-uniform sampling. In this paper, a new approach based on Principal Component Analysis (PCA), called PCA-HDMR, is proposed for finding the coefficients that provide the best linear combination of the bases with minimum error and without using any integral. Benchmark problems are modeled using the method and the results are compared with RS-HDMR results. With both uniform and non-uniform sampling, PCA-HDMR built more accurate models than RS-HDMR for a given set of sample points."
80,"One of the primary drawbacks plaguing wider acceptance of surrogate models is their low fidelity in general. This issue can be in a large part attributed to the lack of automated model selection techniques, particularly ones that do not make limiting assumptions regarding the choice of model types and kernel types. A novel model selection technique was recently developed to perform optimal model search concurrently at three levels: (i) optimal model type (e.g., RBF), (ii) optimal kernel type (e.g., multiquadric), and (iii) optimal values of hyper-parameters (e.g., shape parameter) that are conventionally kept constant. The error measures to be minimized in this optimal model selection process are determined by the Predictive Estimation of Model Fidelity (PEMF) method, which has been shown to be significantly more accurate than typical cross-validation-based error metrics. In this paper, we make the following important advancements to the PEMF-based model selection framework, now called the Concurrent Surrogate Model Selection or COSMOS framework: (i) The optimization formulation is modified through binary coding to allow surrogates with differing numbers of candidate kernels and kernels with differing numbers of hyper-parameters (which was previously not allowed). (ii) A robustness criterion, based on the variance of errors, is added to the existing criteria for model selection. (iii) A larger candidate pool of 16 surrogate-kernel combinations is considered for selection — possibly making COSMOS one of the most comprehensive surrogate model selection framework (in theory and implementation) currently available. The effectiveness of the COSMOS framework is demonstrated by successfully applying it to four benchmark problems (with 2–30 variables) and an airfoil design problem. The optimal model selection results illustrate how diverse models provide important tradeoffs for different problems."
81,"One of the significant breakthroughs in quantum computation is Grover’s algorithm for unsorted database search. Recently, the applications of Grover’s algorithm to solve global optimization problems have been demonstrated, where unknown optimum solutions are found by iteratively improving the threshold value for the selective phase shift operator in Grover rotation. In this paper, a hybrid approach that combines continuous-time quantum walks with Grover search is proposed. By taking advantage of quantum tunneling effect, local barriers are overcome and better threshold values can be found at the early stage of search process. The new algorithm based on the formalism is demonstrated with benchmark examples of global optimization. The results between the new algorithm and the Grover search method are also compared."
82,"Divide-and-conquer strategies have been utilized to perform evaluation calculations of complex network systems, such as reliability analysis of a Markov chain. This paper focuses on partitioning of Markov chain for a multi-modular redundant system and the fast calculation using parallel processing. The complexity of Markov chain is first reduced by eliminating the connections with low transition probabilities associated with a threshold parameter. The transition probability matrix is then reordered and partitioned such that a worse-case reliability is evaluated through the calculations in only the diagonal sub-matrices of the transition probability matrix. Since the calculations of the sub-matrices are independent to each other, the numerical efficiency can be greatly improved using parallel computing. The numerical results showed the selection of threshold parameter is a key factor to numerical efficiency. In this paper, the sensitivity of the numerical performance of Partitioning and Parallel-processing of Markov Chain (PPMC) to the threshold parameter has been investigated and discussed."
83,"The performance of a multidisciplinary system is inevitably affected by various sources of uncertainties, usually categorized as aleatory (e.g. input variability) or epistemic (e.g. model uncertainty) uncertainty. In the framework of design under uncertainty, all sources of uncertainties should be aggregated to assess the uncertainty of system quantities of interest (QOIs). In a multidisciplinary design system, uncertainty propagation refers to the analysis that quantifies the overall uncertainty of system QOIs resulting from all sources of aleatory and epistemic uncertainty originating in the individual disciplines. However, due to the complexity of multidisciplinary simulation, especially the coupling relationships between individual disciplines, many uncertainty propagation approaches in the existing literature only consider aleatory uncertainty and ignore the impact of epistemic uncertainty. In this paper, we address the issue of efficient uncertainty quantification of system QOIs considering both aleatory and epistemic uncertainties. We propose a spatial-random-process (SRP) based multidisciplinary uncertainty analysis (MUA) method that, subsequent to SRP-based disciplinary model uncertainty quantification, fully utilizes the structure of SRP emulators and leads to compact analytical formulas for assessing statistical moments of uncertain QOIs. The proposed method is applied to a benchmark electronics packaging problem. To demonstrate the effectiveness of the method, the estimated low-order statistical moments of the QOIs are compared to the results from Monte Carlo simulations."
84,"Aircraft sizing, route network design, demand estimation and allocation of aircraft to routes are different facets of the air transportation optimization problem that can be viewed as individual “systems,” since they can be conducted independently. In fact, there is a large body of literature that investigates each of these as a stand-alone problem. In this regard, the air transportation design optimization problem can be viewed as an optimal system-of-systems (SoS) design problem. The resulting mixed variable programming problem cannot be solved all-in-one (AiO) because its size and complexity grow exponentially with increasing number of network nodes. In this work, we use a nested multidisciplinary formulation and the Mesh Adaptive Direct Search (MADS) optimization algorithm to solve the optimal SoS design problem. The expansion of a regional Canadian airline’s network to enable national operations is considered as an example."
85,"The Alternating Direction Method of Multipliers (ADMM) is a distributed algorithm suitable for quasi-separable problems in Multi-disciplinary Design Optimization. Previous authors have studied the convergence and complexity of the ADMM algorithm by treating it as an instance of the proximal point algorithm. In this paper, those previous results are extended to an alternate form of the ADMM algorithm applied to the quasi-separable problem. Secondly, a dynamic penalty parameter updating heuristic for the ADMM algorithm is introduced and compared against a previously proposed updating heuristic. The proposed updating heuristic was tested on a distributed linear model fitting example and performed favorably against the other heuristic and the fixed penalty parameter scheme."
86,"As system design problems increase in complexity, researchers seek approaches to optimize such problems by coordinating the optimizations of decomposed sub-problems. Many methods for optimization by decomposition have been proposed in the literature among which, the Augmented Lagrangian Coordination (ALC) method has drawn much attention due to its efficiency and flexibility. The ALC method involves a quadratic penalty term, and the initial setting and update strategy of the penalty weight are critical to the performance of the ALC. The weight in the traditional weight update strategy always increases and previous research shows that an inappropriate initial value of the penalty weight may cause the method not to converge to optimal solutions."
87,"The complexity of design and optimization tasks of modern products which cannot be carried out by a single expert or by a single design team motivated the development of the field of decomposition-based optimization. In general, the process of decomposition-based optimization consists of two procedures: (1) Partitioning the original problem into sub-problems according to the design disciplines, components or other aspects; and (2) Coordinating these sub-problems to guarantee that the aggregation of their optimal solutions results in a feasible and optimal solution for the original problem. Much current work focuses on alternative approaches for these two procedures."
88,"Design engineers and decision-makers across various fields are constantly working to make optimal design decisions for multidisciplinary engineering systems in an effort to improve performance and reduce costs. The multiple disciplines that decision-makers are forced to consider can range from different physical components of a system, to competing physical phenomena influencing a component (e.g. flow forces and structural strength), to completely separate models of interest to a system (e.g. engineering performance and lifecycle cost). The common element that all these decision-making scenarios share is the presence of couplings between the considered disciplines (or subsystems). How the values for these coupling parameters are determined within a decision-making or optimization framework is the subject of countless research efforts. At present the multidisciplinary design optimization (MDO) community has settled on a few proven techniques such Collaborative Optimization (CO) and Analytic Target Cascading (ATC). However, current MDO techniques have issues that limit their effectiveness in solving various MDO problems. Many of these strategies require close coordination between subsystem optimization solvers, require significant effort by decision-makers to pose their problems in a suitable format, and/or can have large computational efficiency problems due to the fact that they involve solving nested optimization problems. In an effort to alleviate some of these issues and make MDO easier to implement and more computationally efficient, a new sequential MDO algorithm called Cooperative Design Optimization (CDO) is proposed. The CDO approach functions through a series of subsystem optimizations using a successively smaller cooperation space. The cooperation space is analogous to the design space of a traditional optimization problem, but includes only the coupling parameters that are a factor in multiple subsystems. A single iteration of the approach can be thought of as a negotiation between all of the subsystems regarding the boundaries of the cooperation space. To facilitate this cooperation, each subsystem optimization problem is restructured as a multi-objective optimization problem so that a Pareto set of optimal solutions, or agreeable design alternatives, are produced. As a result, after all subsystem optimizations are accomplished, each subsystem’s obtained Pareto optimal solution sets are compared with respect to the coupling parameters and new bounds in the cooperation space are determined for the next iteration. This process allows each subsystem to be optimized completely independently within the boundaries of the agreed upon cooperation space while coordination is achieved through an analysis of obtained optimal solutions for each subsystem. Iterations, or negotiations, are repeated until an acceptable solution or set of solutions is obtained for all included subsystems through this cooperative process."
89,"A practical, flexible, versatile, and heterogeneous distributed computing framework is presented that simplifies the creation of small-scale local distributed computing networks for the execution of computationally expensive black-box analyses. The framework is called the Dynamic Service-oriented Optimization Computing Framework (DSOCF), and is designed to parallelize black-box computation to speed up optimization runs. It is developed in Java and leverages the Apache River project, which is a dynamic Service-Oriented Architecture (SOA). A roulette-based real-time load balancing algorithm is implemented that supports multiple users and balances against task priorities, which is superior to the rigid pre-set wall clock limits commonly seen in grid computing. The framework accounts for constraints on resources and incorporates a credit-based system to ensure fair usage and access to computing resources. Experimental testing results are shown to demonstrate the effectiveness of the framework."
90,"This paper presents the development of an Analytical Target Cascading (ATC) Multidisciplinary Design Optimization (MDO) framework for a steady-state engine calibration optimization problem. The implementation novelty of this research is the use of the ATC framework to formulate the complex multi-objective engine calibration problem, delivering a considerable enhancement compared to the conventional 2-stage calibration optimization approach [1]. A case study of a steady-state calibration optimization of a Gasoline Direct Injection (GDI) engine was used for the calibration problem analysis as ATC. The case study results provided useful insight on the efficiency of the ATC approach in delivering superior calibration solutions, in terms of “global” system level objectives (e.g. improved fuel economy and reduced particulate emissions), while meeting “local” subsystem level requirements (such as combustion stability and exhaust gas temperature constraints). The ATC structure facilitated the articulation of engineering preference for smooth calibration maps via the ATC linking variables, with the potential to deliver important time saving for the overall calibration development process."
91,"The challenge of designing complex engineered systems with long service lives can be daunting. As customer needs change over time, such systems must evolve to meet these needs. This paper presents a method for evaluating the reconfigurability of systems to meet future needs. Specifically we show that excess capability is a key factor in evaluating the reconfigurability of a system to a particular need, and that the overall system reconfigurability is a function of the system’s reconfigurability to all future needs combined. There are many examples of complex engineered systems; for example, aircraft, ships, communication systems, spacecraft and automated assembly lines. These systems cost millions of dollars to design and millions to replicate. They often need to stay in service for a long time. However, this is often limited by an inability to adapt to meet future needs. Using an automated assembly line as an example, we show that system reconfigurability can be modeled as a function of usable excess capability."
92,"Structural representations for interfaces between modules and components in a product vary widely in the literature. After reviewing several structural approaches to interface definition, a new weighted design dependency measure is described. The new representation takes into account both six different types of interfaces as well as their relative strength and frequency within a product architecture. The resulting design dependency measure provides a means for designers to quantify the change resistance in a product. In this paper, we investigate the use of this new design dependency measure to drive module identification. Specifically, we compare the resulting modules obtained by optimizing Design Structure Matrices (DSMs) using standard 0-1 representations of the interfaces to those obtained using the new design dependency measure. The results indicate that the weighted design dependency measure leads to more a logical definition of modules that maximizes within module dependencies and minimizes interactions between modules."
93,"Systems that can be reconfigured are valuable in situations where a single artifact must perform several different functions well, and are especially important in cases where system demands are not known a priori. Design of reconfigurable systems present unique challenges compared to fixed system design. Increasing reconfigurable capability improves system utility, but also increases system complexity and cost. In this article a new design strategy is presented for the design of reconfigurable systems for multiability. This study is limited to systems where all system functions are known a priori, and only continuous means of reconfiguration are considered. Designing such a system requires determination of (1) what system features should be reconfigurable, and (2) what should the range of reconfigurability of these features be. The new design strategy is illustrated using a reconfigurable delta robot, which is a parallel manipulator that can be adapted to perform a variety of manufacturing operations. In this case study the tradeoff between end effector stiffness and speed is considered over two separate manipulation tasks."
94,"Product family design optimization is a cost-efficient concept for achieving the best tradeoff between commonalization and diversification of products. When design functions are computationally intensive and thus viewed as black-boxes, the product family design becomes more challenging. In this study a two-stage platform configuration and product family design optimization method with generalized commonality is proposed for scale-based families involving black-box functions. The platform configuration is unknown and multiple sub-platforms are allowed. In this study, the main parameters used towards the family design include a non-conventional sensitivity analysis, the detachability property of each variable, and the variation of individual optimal values for each design variable. Metamodeling techniques are employed to provide both the non-conventional sensitivity and correlation intensities information, which leads to significant savings in the number of function calls. Efficiency of this method is tested through designing a scalable family of universal electric motors. Compared to a number of previously developed methods, the proposed method yields a design solution with acceptable performance loss after commonalization, and better value for the aggregated preference objective function while satisfying all the performance constraints."
95,"The implications of decision analysis (DA) on engineering design are well known. Recently, the authors proposed decision topologies (DT) as a visual method for making design decisions and proved that they are consistent with normative decision analysis. This paper addresses the practical issue of assessing DTs for a decision maker (DM) using their responses, particularly under uncertainty. This is critical to encoding decision maker preferences so that further analysis and mathematical optimization can be performed using the correct set of preferences. We show how multiattribute DTs can be directly assessed from DM responses. Four methods are shown to evolutionarily assess DTs among which one that requires the DM to rank alternatives and another where a utility function is first assessed. It is also shown that preferences under uncertainty can be easily incorporated. In addition, we show that topologies can be constructed using single attribute topologies similarly to multi-linear functions in utility analysis. This incremental construction simplifies the process of topology construction. The reverse problem of inferring single attribute DTs is also presented. The proposed assessment methods are used on a design decision making problem of a welded beam."
96,"In reliability design, allocating redundancy through various optimization methods is one of the important ways to improve the system reliability. Generally, in these redundancy allocation problems, it is assumed that failures of components are independent. However, under this assumption failure rates can be underestimated since failure interactions can significantly affect the performance of systems. This paper first proposed an analytical model describing the failure rates with failure interactions. Then a Modified Analytic Hierarchy Process (MAHP) is proposed to solve the redundancy allocation problems for systems with failure interactions. This method decomposes the system into several blocks and deals with those down-sized blocks before diving deep into the most appropriate component for redundancy allocation. Being simple and flexible, MAHP provides an intuitive way to design a complex system and complex explicit objective functions for the entire system is not required in the proposed approach. More importantly, with the help of the proposed analytical failure interaction model, MAHP can capture the effect of failure interactions. Results from case studies clearly demonstrate the applicability of the analytical model for failure interactions and MAHP for reliability design."
97,"The reliability and robustness are critical aspects for engineering systems. In a complex system, usually multiple functions need to be achieved. In such cases, there exist optimal mappings between functions and physical components. In this paper, a new approach, Axiomatic Fault Tree Analysis (AFTA), is proposed to improve the system reliability and robustness simultaneously. In AFTA, system robustness is improved by using Axiomatic Design (AD) to decouple the system to an extent where a group of functions have a corresponding relationship with only one physical component. Moreover, Fault Tree Analysis and Axiomatic Design have been integrated in AFTA by using two new indices indicating the importance degrees of functions and physical components in a reliability sense, which provides a guideline for reliability improvement. A case study of a complex system is used to illustrate the procedure and applicability of AFTA. Theoretical calculations are used to verify AFTA for non-repairable systems, and multi-agent based simulations are for repairable systems. Both the theoretical and simulated results show that the system reliability has been improved significantly without large additional cost."
98,"If a limit-state function involves time, the associated reliability is defined within a period of time. The extreme value of the limit-state function is needed to calculate the time-dependent reliability, and the extreme value is usually highly nonlinear with respect to random input variables and may follow a multimodal distribution. For this reason, a surrogate model of the extreme response along with Monte Carlo simulation is usually employed. The objective of this work is to develop a new method, called the Efficient Global Optimization Reliability Analysis (EGORA), to efficiently build the surrogate model. EGORA is based on the Efficient Global Optimization (EGO) method. Different from the current method that generates training points for random variables and time independently, EGORA draws training points for the two types of input variables simultaneously and therefore accounts for their interaction effects. The other improvement is that EGORA only focuses on high accuracy at or near the limit state. With the two improvements, the new method can effectively reduce the number of training points. Once the surrogate model of the extreme response is available, Monte Carlo simulation is applied to calculate the time-dependent reliability. Good accuracy and efficiency of EGORA are demonstrated by three examples."
99,"The authors have recently proposed a ‘decision-based’ framework to design and maintain repairable systems. In their approach, a multiobjective optimization problem is solved to identify the best design using multiple short and long-term statistical performance metrics. The design solution considers the initial design, the system maintenance throughout the planning horizon, and the protocol to operate the system. Analysis and optimization of complex systems such as a microgrid is however, computationally intensive. The problem is exacerbated if we must incorporate flexibility in terms of allowing the microgrid architecture and its running protocol to change with time. To reduce the computational effort, this paper proposes an approach that “learns” the working characteristics of the microgrid and quantifies the stochastic processes of the total load and total supply using autoregressive time-series. This allows us to extrapolate the microgrid operation in time and eliminate therefore, the need to perform a full system simulation for the entire long-term planning horizon. The approach can be applied to any repairable system. We show that building in flexibility in the design of repairable systems is computationally feasible and leads to better designs."
100,"The failure rate of dynamic systems with random parameters is time-varying even for linear systems excited by a stationary random input. In this paper, we propose a simulation-based method to estimate this time-varying failure rate. The input and output stochastic processes are discretized using a small time step to calculate the trajectories of the output stochastic process accurately through simulation. The planning horizon (time of interest) is then partitioned into a series of longer correlated time intervals and the Saddlepoint approximation (SPA) is employed to estimate the distribution of maximum response and thus obtain the probability of failure in each time interval. Using the same simulated trajectories with SPA, a time-dependent copula is built to provide the correlation between the response in each time interval and the response up to that time interval. The time-varying failure rate is finally estimated at each discrete time, using the probability of failure in each time interval and the correlation information from the estimated copula. The effectiveness of the proposed method is illustrated with a vehicle vibration example."
101,"A stochastic multiscale modeling technique is proposed to construct coarse scale representation of a fine scale model for use in engineering design problems. The complexity of the fine scale heterogeneity under uncertainty is replaced with the homogenized coarse scale parameters by seeking agreement between the responses at both scales. Generalized polynomial chaos expansion is implemented to reduce the dimensionality of propagating uncertainty through scales and the computational costs of the upscaling method. It is integrated into a hybrid optimization procedure with the genetic algorithm and sequential quadratic programming. Two structural engineering problems that involve uncertainties in elastic material properties and geometric properties at fine scales are presented to demonstrate the applicability and merit of the proposed technique."
102,"Dynamic reliability is defined as the probability that an engineered system successfully performs the predefined functionality over a certain period of time considering time-variant operation condition and component deterioration. In practice, it is still a major challenge to conduce dynamic reliability analysis due to the prohibitively high computational costs. In this study, a confidence-based meta-modeling approach is proposed for efficient sensitivity-free dynamic reliability analysis, referred to as double-loop adaptive sampling (DLAS). In DLAS a Gaussian process (GP) model is constructed to approximate extreme system responses over time, so that Monte Carlo simulation (MCS) can be employed directly to estimate dynamic reliability. A qualitative confidence measure is proposed to evaluate the accuracy of dynamic reliability estimation while using the MCS approach based on developed GP models. To improve the confidence, a double-loop adaptive sampling scheme is developed to efficiently update the GP model in a sequential manner, by considering system input variables and time concurrently in double sampling loops. The model updating process can be terminated once the user defined confidence target is satisfied. The DLAS approach does not require computationally expensive sensitivity analysis, thus substantially improves the efficiency of dynamic reliability assessment. Two case studies are used to demonstrate the effectiveness of DLAS for dynamic reliability analysis."
103,"This paper presents an integrated performance measure approach (iPMA) for system reliability assessment considering multiple dependent failure modes. An integrated performance function is developed to envelope all component level failure events, thereby enables system reliability approximation by considering only one integrated system limit state. The developed integrated performance function possesses two critical properties. First, it represents exact joint failure surface defined by multiple component failure events, thus no error will be induced due to the integrated limit state function in system reliability computation. Second, smoothness of the integrated performance on system failure surface can be guaranteed, therefore advanced response surface techniques can be conveniently employed for response approximation. With the developed integrated performance function, the maximum confidence enhancement based sequential sampling method is adopted as an efficient component reliability analysis tool for system reliability approximation. To furthermore improve the computational efficiency, a new constraint filtering technique is developed to adaptively identify active limit states during the iterative sampling process without inducing any extra computational cost. One case study is used to demonstrate the effectiveness of system reliability assessment using the developed iPMA methodology."
104,"According to order of approximation, there are two types of analytical reliability analysis methods; first-order reliability method and second-order reliability method. Even though FORM gives acceptable accuracy and good efficiency for mildly nonlinear performance functions, SORM is required in order to accurately estimate the probability of failure of highly nonlinear functions due to its large curvature. Despite its necessity, SORM is not commonly used because the calculation of the Hessian is required. To resolve the heavy computational cost in SORM due to the Hessian calculation, a quasi-Newton approach to approximate the Hessian is introduced in this study instead of calculating the Hessian directly. The proposed SORM with the approximated Hessian requires computations only used in FORM leading to very efficient and accurate reliability analysis. The proposed SORM also utilizes the generalized chi-squared distribution in order to achieve better accuracy. Furthermore, an SORM-based inverse reliability method is proposed in this study as well. A reliability index corresponding to the target probability of failure is updated using the proposed SORM. Two approaches in terms of finding more accurate most probable point using the updated reliability index are proposed and compared with existing methods through numerical study. The numerical study results show that the proposed SORM achieves efficiency of FORM and accuracy of SORM."
105,"An accurate input probabilistic model is necessary to obtain a trustworthy result in the reliability analysis and the reliability-based design optimization (RBDO). However, the accurate input probabilistic model is not always available. Very often only insufficient input data are available in practical engineering problems. When only the limited input data are provided, uncertainty is induced in the input probabilistic model and this uncertainty propagates to the reliability output which is defined as the probability of failure. Then, the confidence level of the reliability output will decrease. To resolve this problem, the reliability output is considered to have a probability distribution in this paper. The probability of the reliability output is obtained as a combination of consecutive conditional probabilities of input distribution type and parameters using Bayesian approach. The conditional probabilities that are obtained under certain assumptions and Monte Carlo simulation (MCS) method is used to calculate the probability of the reliability output. Using the probability of the reliability output as constraint, a confidence-based RBDO (C-RBDO) problem is formulated. In the new probabilistic constraint of the C-RBDO formulation, two threshold values of the target reliability output and the target confidence level are used. For effective C-RBDO process, the design sensitivity of the new probabilistic constraint is derived. The C-RBDO is performed for a mathematical problem with different numbers of input data and the result shows that C-RBDO optimum designs incorporate appropriate conservativeness according to the given input data."
106,"A new reliability analysis method is proposed for time-dependent problems with limit-state functions of input random variables, input random processes and explicit in time using the total probability theorem and the concept of composite limit state. The input random processes are assumed Gaussian. They are expressed in terms of standard normal variables using a spectral decomposition method. The total probability theorem is employed to calculate the time-dependent probability of failure using a time-dependent conditional probability which is computed accurately and efficiently in the standard normal space using FORM and a composite limit state of linear instantaneous limit states. If the dimensionality of the total probability theorem integral (equal to the number of input random variables) is small, we can easily calculate it using Gauss quadrature numerical integration. Otherwise, simple Monte Carlo simulation or adaptive importance sampling is used based on a pre-built Kriging metamodel of the conditional probability. An example from the literature on the design of a hydrokinetic turbine blade under time-dependent river flow load demonstrates all developments."
107,"In this paper, convex modeling based topology optimization with load uncertainty is presented. The load uncertainty is described using the non-probabilistic based unknown-but-bounded convex model, and the strain energy based topology optimization problem under uncertain loads is formulated. Unlike the conventional deterministic topology optimization problem, the maximum possible strain energy under uncertain loads is selected as the new objective in order to achieve a safe solution. Instead of obtaining approximated solutions as used before, an exact solution procedure is presented. The problem is first formulated as a single level optimization problem, and then rewritten as a two-level optimization problem. The upper level optimization problem is solved as a deterministic topology optimization with the load which generated from the worst structure response in the lower level problem. The lower level optimization problem is to identify this worst structure response, and it is found equivalent to an inhomogeneous eigenvalue problem. Three different cases are discussed for accurately evaluating the global optima of the lower level optimization problem, while the corresponding sensitivities are derived individually. With the function value and sensitivity information ready, the upper level optimization problem can be solved through existing gradient based optimization algorithms. The effectiveness of the proposed convex modeling based topology optimization is demonstrated through different numerical examples."
108,"While Robust Optimization has been utilized for a variety of design problems, application of Robust Design to the control of large-scale systems presents unique challenges to assure rapid convergence of the solution. Specifically, the need to account for uncertainty in the optimization loop can lead to a prohibitively expensive optimization using existing methods when using robust optimization for control. In this work, a robust optimization framework suitable for operational control of large scale systems is presented. To enable this framework, robust optimization uses a utility function for the objective, dimension reduction in the uncertainty space, and a new algorithm for evaluating probabilistic constraints. The proposed solution accepts the basis in utility theory, where the goal is to maximize expected utility. This allows analytic gradient and Hessian calculations to be derived to reduce the number of iterations required. Dimension reduction reduces uncertain functions to low dimensional parametric uncertainty while the new algorithm for evaluating probabilistic constraints is specifically formulated to reuse information previously generated to estimate the robust objective. These processes reduce the computational expense to enable robust optimization to be used for operational control of a large-scale system. The framework is applied to a multiple-dam hydropower revenue optimization problem, then the solution is compared with the solution given by a non-probabilistic safety factor approach. The solution given by the framework is shown to dominate the other solution by improving upon the expected objective as well as the joint failure probability."
109,"Evidence theory has a strong ability to deal with the epistemic uncertainty, based on which the uncertain parameters existing in many complex engineering problems with limited information can be conveniently treated. However, the heavy computational cost caused by its discrete property severely influences the practicability of evidence theory, which has become a main difficulty in structural reliability analysis using evidence theory. This paper aims to develop an efficient method to evaluate the reliability for structures with evidence variables, and hence improves the applicability of evidence theory for engineering problems. A non-probabilistic reliability index approach is introduced to obtain a design point on the limit-state surface. An assistant area is then constructed through the obtained design point, based on which a small number of focal elements can be picked out for extreme analysis instead of using all the elements. The vertex method is used for extreme analysis to obtain the minimum and maximum values of the limit-state function over a focal element. A reliability interval composed of the belief measure and the plausibility measure is finally obtained for the structure. Two numerical examples are investigated to demonstrate the effectiveness of the proposed method."
110,"Level-set approaches are a family of domain classification techniques that rely on defining a scalar level-set function (LSF), then carrying out the classification based on the value of the function relative to one or more thresholds. Most continuum topology optimization formulations are at heart, a classification problem of the design domain into structural materials and void. As such, level-set approaches are gaining acceptance and popularity in structural topology optimization. In conventional level set approaches, finding an optimum LSF involves solution of a Hamilton-Jacobi system of partial differential equations with a large number of degrees of freedom, which in turn, cannot be accomplished without gradients information of the objective being optimized. A new approach is proposed in this paper where design variables are defined as the explicit values of the LSF at knot points, then a Kriging model is used to interpolate the LSF values within the rest of the domain so that classification into material or void can be performed. Perceived advantages of the explicit level-set (ELS) approach include alleviating the need for gradients of objectives and constraints, while maintaining a reasonable number of design variables that is independent from the mesh size. A hybrid genetic algorithm (GA) is then used for solving the optimization problem(s). An example problem of a short cantilever is studied under various settings of the ELS parameters in order to infer the best practice recommendations for tuning the approach. Capabilities of the approach are then further demonstrated by exploring its performance on several test problems."
111,"This work presents a new approach for solving nonlinear mixed discrete-continuous variable problems with constraints. The proposed method falls under the category of direct search methods for discrete variables. Different from the traditional direct search methods that determine the search direction based on decreasing objective function within the feasible space, a relative sensitivity that jointly considers change in objective and constraint functions is introduced in this work to help determining the search direction. For feasible discrete points, the coordinate direction with the maximum relative sensitivity is taken as the search direction, so that the objective function value decreases the fastest with minimum increase in constraint values. For infeasible points, the search direction is determined by the minimum relative sensitivity, so that the points can be dragged into the feasible region with constraints decreasing the fastest and minimum increase of the objective. In addition, in order to reduce the number of constraints and calculate the relative sensitivity, a constraint aggregation technique with Kreisselmeier-Steinhauser function is applied to transform all constraints into an equivalent differentiable inequality constraint. The efficacy and accuracy of the proposed approach is demonstrated with different types of test problems and application to a design problem. The proposed method has advantages in solving nonlinear mixed discrete-continuous variable problems with constraints compared to other existing methods."
112,"The present work proposes a new algorithm for the optimization of cutting parameters in the high speed drilling of woven composites. The cutting parameters under consideration are the feed rate and the spindle speed. Three performance parameters are to be minimized. These are the exit delamination, the surface roughness and the thrust force. These performance parameters are observed experimentally. One of the challenges that face the experimental testing of these parameters is the high cost of the drilling tools and specimen materials. Therefore, the minimization of the number of experimental tests is a necessary requirement. The algorithm presented hybridizes Kriging as a meta-modeling technique with evolutionary multi-objective optimization to optimize the cutting parameters while intelligently selecting the new set of cutting parameters in each iteration. After starting with a factorial design of the search space, and after testing the performance criteria at these points, the algorithm fits a multi-dimensional surface using Kriging. This step is followed by an evolutionary search on the fitted model. The search spreads a population of search points in the direction of better performance criteria as well as in the direction of un-sampled space. The previous two steps are conducted iteratively for a pre-defined number of iterations. In the final iteration, the population of search points is clustered to yield a small number of new points at which the new experiments will be conducted. The whole process is iterated until the maximum number of allowable experiments is achieved. The algorithm is tested using an existing set of previously published experimental data that are dense enough to predict the actual response surface of the performance criteria. Results showed that the algorithm smartly moved into the direction of higher performance criteria with a low number of experimental trials."
113,"Armored vehicles have to survive multiple threats such as projectile or land mines. The shocks induced by these threats can harm vehicle occupants or damage sensitive electronic components. Therefore, a goal of modern armored vehicle design is to reduce transmitted shocks to critical components. In this paper, finite element (FE) models of an armored vehicle prototype having the internal space frame structure with the aforementioned features are developed. One model comprises of only solid elements, while another model is created with purely beam elements. The beam elements model is used for optimization studies whose objective is to reduce the shocks within the vehicle, due to mine blast while maintaining its overall structural integrity. The thickness of the rubberized shock mitigation layer at the joints of the space frame is varied during the optimization process. The optimization problem is solved using the Successive Heuristic Quadratic Approximation (SHQA) algorithm, which combines successive quadratic approximation with an adaptive random search while varying the bounds of the search space. The entire optimization process is carried out within the MATLAB environment. The results show that a significant reduction in the shock can be achieved using this approach."
114,"In this paper the Eigenvalue-Superposition of Convex Models (ESCM) based topology optimization method for solving topology optimization problems under external load uncertainties is presented. The load uncertainties are formulated using the non-probabilistic based unknown-but-bounded convex model. The sensitivities are derived and the problem is solved using gradient based algorithm. The proposed ESCM based method yields the material distribution which would optimize the worst structure response under the uncertain loads. Comparing to the deterministic based topology optimization formulation the ESCM based method provided more reasonable solutions when load uncertainties were involved. The simplicity, efficiency and versatility of the proposed ESCM based topology optimization method can be considered as a supplement to the sophisticated reliability based topology optimization methods."
115,"For the machining and assembly of mechanical parts, their secure fixation in a defined position is crucial. To achieve this task, flexible fixture devices (FFDs) are the industry standard for small and medium batch-sizes. Unlike dedicated fixtures, FFDs allow for the fixation of different part shapes, increasing their applicability and economic efficiency. Aiming to create a low-cost and autonomous FFD, a reconfigurable vise with adaptable jaws was developed. The jaws can be machined to a variety of shapes to securely hold prismatic and cylindrical parts. In this paper, a spatial grammar approach for the computational design synthesis of these customizable jaws is presented. Different sets of rules for the generation of 3D solid models of vise jaws based on the model of the workpiece to be held are developed and realized in a CAD environment. The approach is verified by generating jaw designs for example parts."
116,"The design of manufacturing systems in hazardous environments is complex, requiring interdisciplinary knowledge to determine which components and operators (human or robotic) are feasible. When conceptualizing designs, some options may be overlooked or unknowingly infeasible due to the design engineers’ lack of knowledge in a particular field or ineffective communication of requirements between disciplines. Computational design tools can help alleviate many of the problems encountered in this design task. We create a knowledge-based system (KBS) utilizing CLIPS to automate the synthesis of conceptual manufacturing system designs in radioactive environments. The KBS takes a high-level functional description of a process and uses FBS modeling to generate multiple designs with generic components retrieved from a database and low-level manufacturing task sequences. Using this approach, many options are explored and operator task compatibility is directly addressed. The KBS is applied to the design of glovebox processing systems at Los Alamos National Laboratory (LANL)."
117,"This paper deals with a new approach for the development of a gearbox of a new kind of variable transmissions, the independently controllable transmission (ICT). It provides a continuous output speed which is independent of the input speed. In the beginning the requirements of a gearbox are mentioned. Design guidelines are the starting point for the design process of the ICT gearbox. Some of these guidelines will be applied on the gearbox of the ICT to model rules which were needed for the design process. The process of the product design of the gearbox can be improved more efficiently by using multidisciplinary design optimization with evolutionary algorithms and topology optimization. This paper describes an approach for the development of this gearbox and the preparation of the optimization models based on a process chain, which is a guide for the next steps of creating the design process in detail. The first step is a parametric CAD model which consists of different parts and provides the complete design space of the gearbox."
118,"This paper presents a new technique for topology optimization of fluid channels using generative design methods. The proposed method uses the generative abilities of graph grammars with simulation and analysis power of conventional CFD methods. The graph grammar interpreter GraphSynth is used to carry out graph transformations, which define different topologies for a given multi-inlet multi-outlet problem. The generated graphs are first transformed into meaningful 3D shapes. These solutions are then analyzed by a CFD solver to find the optimum. The effectiveness of the proposed method is checked by solving a variety of available test problems and comparing them with those found in the literature. Furthermore by solving complex problems the robustness and effectiveness of the method is tested."
119,"Particle Swarm Optimization is a population based globalized search algorithm that mimics the behavior of swarms. It belongs to the larger class of evolutionary algorithms as widely used stochastic technique in the global optimization field. Since the PSO is population based, it requires no auxiliary information, such as the gradient of the problem."
120,"This paper shows how reliability block diagrams can be used as a decision making tool. The premise behind the idea is that classical decision analysis while very powerful, does not provide tractability in assessing utility functions and their use in making decisions. Our recent work has shown that visual representation of systems using a reliability block diagram can be used to describe a decision situation. In decision making, we called these block diagrams decision topologies. We show that decision topologies can be used to make many engineering decisions and can replace decision analysis for most decisions. The paper proves that at the limit, using decision topologies is entirely consistent with decision analysis for both single attribute and multiattribute cases. The main advantages of the proposed method are that (1) it provides a visual representation of a decision situation, (2) it can easily model tradeoffs, (3) it allows binary attributes, (4) it can be used when limited information is available, and (5) it can be used in a low-fidelity sense to quickly make a decision. The paper details the theoretical basis of the proposed method and highlights its benefits. An example is used to demonstrate how decision topologies can be used in practice."
121,"We investigate consumer preference interactions in visual choice-based conjoint analysis, where the conjoint attributes are parameters that define shapes shown to the respondent as images. Interaction effects are present when preference for the level of one attribute is dependent on the level of another attribute. When interaction effects are negligible, a main-effects fractional factorial experimental design can be used to reduce data requirements and survey cost. This is particularly important when the presence of many parameters or levels makes full factorial designs intractable. However, if interaction effects are relevant, a main-effects design creates biased estimates and potentially misleading conclusions. Most conjoint studies assume interaction effects are negligible; however, interactions may play a larger role for shape parameters than for other types of attributes. We conduct preliminary tests on this assumption in three visual conjoint studies. The results suggest that interactions can be either negligible or dominant in visual conjoint, depending on both consumer preferences and shape parameterization. When interactions are anticipated, it is possible in some cases to re-parameterize the shape such that interactions in the new space are negligible. Generally, we suggest that randomized designs are better than fractional factorial designs at avoiding bias due to the presence of interactions and/or the organization of profiles into choice sets."
122,"In distributed design systems, designers are related to each other through couplings, however they have limited control over the design variables. Any inconsistency in the design system can result in design conflicts through these couplings. Modeling designer attitudes can help to understand inconsistencies and manage conflicts in design processes. We expand the bottom-up design approach through agent-based modeling techniques to another level where designers can make decisions directly on their wellbeing values that represent how their desires are satisfied. Set-based design and constraint programming techniques are used to explore the imprecision of the design activities. Monte Carlo simulations are performed to evaluate the performance of our approach. The results show that the number of design conflicts and their harshness can be lowered when the design process is defined with our approach."
123,"This paper develops and explores the interface between two related concepts in design decision making. First, design decision making is a "
124,"Product complexity has been studied as an important factor to decrease the cost and time of the development process. With this purpose, prior research has included the development of design complexity metrics as a method to assess and decrease complexity. Recent studies have also focused on the comparison of complexity metrics for the particular case of medical devices development (MDD). However, the major issue relevant to MDD has not been addressed; the relationship between FDA regulations and the device complexity is not clarified. Therefore, to increase MDD safety and decrease the time to market, we must understand the regulatory decision process and rules. In this paper, we investigate the relation between different complexity metrics and FDA’s decision time using a sample of 100 hip replacement devices. Bayesian network learning is used to explore in detail local relationships between different variables, both complexity measures and product variables. This relationship was found significant for the first two clusters of the analysis. However, for a third cluster it is speculated that FDA decision time does not depend solely upon the degree of medical device complexity. Company or organization relevant variables could be playing a greater role than just complexity. Additional questions are drawn based on the results that must be investigated."
125,"The produced power and the thrust force exerted on the wind turbine are two conflicting objectives in the design of a floating horizontal axis wind turbine. Meanwhile, the variations in design variables and design environment parameters are unavoidable. The variations include the small variations in the design variables due to manufacturing errors, and the large variations in the wind speed. Therefore, two robustness indices are introduced in this paper. The first one characterizes the robustness of multi-objective optimization problems against small variations in the design variables and the design environment parameters. The second robustness index characterizes the robustness of multi-objective optimization problems against large variations in the design environment parameters. The robustness of the solutions based on the two robustness indices is treated as a vector defined in the robustness function space. As a result, the designer can compare the robustness of all Pareto optimal solutions and make a decision. Finally, the multi-objective robust optimization design of a fixed-speed horizontal axis wind turbine illustrates the proposed methodology."
126,"Larger onshore wind farms are often installed in phases, with discrete smaller sub-farms being installed and becoming operational in succession until the farm as a whole is completed. An extended pattern search (EPS) algorithm that selects both local turbine position and geometry is presented that enables the installation of a complete farm in discrete stages, exploring optimality of both incremental sub-farm solutions and the completed project as a whole. The objective evaluation is the maximization of profit over the life of the farm, and the EPS uses modeling of cost based on an extensive cost analysis by the National Renewable Energy Laboratory (NREL). The EPS uses established wake modeling to calculate the power development of the farm, and allows for the consideration of multiple or overlapping wakes."
127,"Energy extraction from ocean waves and conversion to electrical energy is a promising form of renewable energy, yet achieving economic viability of wave energy converters (WECs) has proven challenging. In this article, the design of a heaving cylinder WEC will be explored. The optimal plant (i.e. draft and radius) design space with respect to the design’s optimal control (i.e. power take-off trajectory) for maximum energy production is characterized. Irregular waves based on the Bretschneider wave spectrum are considered. The optimization problem was solved using a pseudospectral method, a direct optimal control approach that can incorporate practical design constraints, such as power flow, actuation force, and slamming. The results provide early-stage guidelines for WEC design. Results show the resonance frequency required for optimal energy production with a regular wave is quite different than the resonance frequency found for irregular waves; specifically, it is much higher."
128,"An exponential growth of photovoltaic (PV) technologies in the past decade has paved a path to a sustainable solar-powered world. The development of alternative PV technologies with low-cost and high-stability materials has attracted a growing amount of attention. One of these alternatives is the use of second generation thin film PV technologies. However, even in the presence of their bandgap properties, a major issue faced by most thin film solar cells is the low output efficiency due to manufacturing variability and uncertain operating conditions. Thus, to ensure the reliability and performance robustness of the thin film PV technologies, the design of the solar cell is studied. To represent the thin film PV technologies, a copper gallium (di)selenide (CIGS) solar cell model is developed and optimized with Reliability-based Robust Design Optimization (RBRDO) method. This model takes into account the variability of the structure and the material properties of the CIGS solar cells, and assumes an ideal-weather operating condition. This study presents a general methodology to optimize the design of the CIGS PV technologies and could be used to facilitate the development and assessment of new PV technologies with more robust performance in efficiency and stability."
129,"Fresh water availability is essential for the economic development in small communities in remote areas. In desert climate, where naturally occurring fresh water is scarce, seawater or brackish water from wells is often more abundant. Since water desalination approaches are energy intensive, a strong motivation exists for the design of cost-effective desalination systems that utilize the abundant renewable energy resource; solar energy. This paper presents an optimization model of a solar-powered reverse osmosis (RO) desalination system. RO systems rely on pumping salty water at high pressure through semi-permeable membrane modules. Under sufficient pressure, water molecules will flow through the membranes, leaving salt ions behind, and are collected in a fresh water stream. Since RO system are primarily powered via electricity, the system model incorporates photovoltaic (PV) panels, and battery storage for smoothing out fluctuations in the PV power output, as well as allowing system operation for a number of hours after sunset. Design variables include sizing of the PV solar collectors, battery storage capacity, as well as the sizing of the RO system membrane module and power elements. The objective is to minimize the cost of unit volume produced fresh water, subject to constraints on production capacity. A genetic algorithm is used to generate and compare optimal designs for two different locations near the Red Sea and Sinai."
130,"This paper significantly advanced the hybrid measure-correlate-predict (MCP) methodology, enabling it to account for the variations of both wind speed and direction. The advanced hybrid MCP method used the recorded data of multiple reference stations to estimate the long-term wind condition at the target wind plant site with greater accuracy than possible with data from a single reference station. The wind data was divided into different sectors according to the wind direction, and the MCP strategy was implemented for each wind sector separately. The applicability of the proposed hybrid strategy was investigated using four different MCP methods: (i) linear regression; (ii) variance ratio; (iii) artificial neural networks; and (iv) support vector regression. To implement the advanced hybrid MCP methodology, we used the hourly averaged wind data recorded at six stations in North Dakota between the years 2008 and 2010. The station Pillsbury was selected as the target plant site. The recorded data at the other five stations (Dazey, Galesbury, Hillsboro, Mayville, and Prosper) was used as reference station data. The best hybrid MCP strategy from different MCP algorithms and reference stations was investigated and selected from the 1,024 combinations. The accuracy of the hybrid MCP method was found to be highly sensitive to the combination of individual MCP algorithms and reference stations used. It was also observed that the best combination of MCP algorithms was strongly influenced by the length of the correlation period."
131,"In this paper we present a two-step approach for the design of a system of Plug-in Hybrid Electric Vehicle (PHEV) charging stations. Our approach consists of a simulation model and a mathematical model. The simulation model formulates the charging station’s ability to meet charging demand by using discrete event simulation. The mathematical model formulates the design decisions made when designing the charging stations, i.e. locations and configurations of charging stations, using the compromise Decision Support Problem (cDSP). Waiting time, service time, number of slots (chargers) and demand are key inputs for the simulation model. Output of the simulation model, which is the service level of the charging stations, is used as an input for the mathematical model. By compromising between maximizing the service level, maximizing the demand coverage, minimizing the installation cost for slots and minimizing distance between charging stations and demand nodes, design decisions are taken in the mathematical model. Our focus in this paper is on the method which is widely applicable. However the approach is presented and evaluated for a data set from Dallas County, Texas."
132,"For semi-isolated communities, fresh water may be scarce; however, brackish water or seawater can be easily accessed. This provides a drive to develop optimum-cost desalination system for such communities. This paper presents the optimization of a water-heated solar-powered humidification-dehumidification (HDH) desalination system with variable saline water flow rate. The design variables include the sizing of solar collector, storage tank and its internal heat exchanger, humidifier and dehumidifier. A program was developed to predict performance based on selected weather data file and optimize the system for minimum unit cost of produced fresh water. System cost is predicted via different first-order estimators. A tailored optimization technique is used and compared to a genetic algorithm procedure in the design optimization for local climate and market. A case study develops an optimum desalination plant for the Red Sea near the city of Hurgada and compared to previously developed system."
133,"The creation of wakes, with unique turbulence characteristics, downstream of turbines significantly increases the complexity of the boundary layer flow within a wind farm. In conventional wind farm design, analytical wake models are generally used to compute the wake-induced power losses, with different wake models yielding significantly different estimates. In this context, the wake behavior, and subsequently the farm power generation, can be expressed as functions of a series of key factors. A quantitative understanding of the relative impact of each of these factors is paramount to the development of more reliable power generation models; such an understanding is however missing in the current state of the art in wind farm design. In this paper, we quantitatively explore how the farm power generation, estimated using four different analytical wake models, is influenced by the following key factors: (i) incoming wind speed, (ii) land configuration, and (iii) ambient turbulence. The sensitivity of the maximum farm output potential to the input factors, when using different wake models, is also analyzed. The extended Fourier Amplitude Sensitivity Test (eFAST) method is used to perform the sensitivity analysis. The power generation model and the optimization strategy is adopted from the Unrestricted Wind Farm Layout Optimization (UWFLO) framework. In the case of an array-like turbine arrangement, both the first-order and the total-order sensitivity analysis indices of the power output with respect to the incoming wind speed were found to reach a value of 99%, irrespective of the choice of wake models. However, in the case of maximum power output, significant variation (around 30%) in the indices was observed across different wake models, especially when the incoming wind speed is close to the rated speed of the turbines."
134,"The robust optimization presented in this paper is formulated to assist in early-stage wind farm development. It can help wind farm developers predict project viability and can help landowners predict where turbines will be placed on their land. A wind farm layout is optimized under multiple sources of uncertainty. Landowner participation is represented with a novel uncertain model of willingness-to-accept monetary compensation. An uncertain wind shear parameter and economies-of-scale cost reduction parameter are also included. Probability Theory, Latin Hypercube Sampling, and Compromise Programming are used to form the robust design problem and minimize the two objectives: the normalized mean and standard deviation of Cost-of-Energy. The results suggest that some landowners that will only accept high levels of compensation are worth pursuing, while others are not."
135,"As more and more companies offer product families rather than individual products, the competitive advantage of product platforming is shrinking. In order to compete companies need to link marketing and engineering so that designers are able to make decisions about critical trade-offs between cost and performance. The current methods for market-driven platform designs use traditional product costing where indirect costs are assigned to individual products based on relative production quantities. Because of increasing product diversity and decreasing direct labor costs, the ratio of indirect costs to total cost of products is increasing. A method for use during the design stage of top-down product family design is needed to assign indirect costs to individual products based on the product’s consumption of indirect resources. An activity-based costing method for top-down product family design is presented here. This method allows the designer to model indirect costs as a function of engineering attributes, creating a framework for top-down product platform optimization that provides a more accurate estimation of cost than traditional product costing methods. An illustrative example shows that an activity-based costing model predicts different profitability from a traditional costing system for a number of different motor designs."
136,"Although discrete choice analysis has been shown to be useful for modeling consumer preferences and choice behaviors in the field of engineering design, information of choice set composition is often not available in majority of the collected consumer purchase data. When a large set of choice alternatives exist for a product, such as automotive vehicles, randomly choosing a small set of product alternatives to form a choice set for each individual consumer will result in misleading choice modeling results. In this work, we propose a data-analytics approach to mine existing data of choice sets and predict the choice set for each individual customer in a new choice modeling scenario where the choice set information is lacking. The proposed data-analytics approach integrates product association analysis, network analysis, consumer segmentation, and predictive analytics. Using the J.D. Power vehicle survey as the existing choice set data, we demonstrate that the association network approach is capable of recognizing and expressively summarizing meaningful product relations in choice sets. Our method accounts for consumer heterogeneity using the stochastic generation algorithm where the probability of selecting an alternative into a choice set integrates the information of customer profile clusters and products chosen frequencies. By comparing multiple multinomial logit models using different choice set compositions, we show that the choice model estimates are sensitive to the choice set compositions and our proposed method leads to improved modeling results. Our method also provides insights into market segmentation that can guide engineering design decisions."
137,"This article illustrates how variance in the predictive distribution of the profit objective function in a design for market systems model can be decomposed into two components using a simulation based Bayesian approach introduced in the econometrics literature. The first component, intrinsic uncertainty, would be retained in the model even if the model calibration parameter values, such as parameters representing customer preferences, were known with certainty. The second component, extrinsic uncertainty, stems from lack of precision regarding model calibration parameters such as customer preferences. The simulation based approach overcomes a key problem in decomposing uncertainty for the typical design for market systems problem by overcoming the difficulties associated with analytical treatment of non-normal distributions. The variance decomposition approach is demonstrated for the design of a handheld grinder power tool. Following the same Bayesian decision analysis framework the variance simulation method can be applied to other design for market system problems with other objective functions and with additional sources of uncertainty."
138,"In recent years, the number of products that can be tailored to consumers’ needs and desires has increased dramatically; there are many opportunities to individualize the colors, materials or options of products. However, current trends indicate that the future consumer will not be satisfied with mere material and color choices, but will desire control over form as well. While it is technically feasible to allow consumers to partially mass-customize the form of products subject to functional and production constraints through the use of a generative design system, the question of how the control of form should be presented to the user arises. The issue becomes especially important when the product form is based on complex morphologies, which require in-depth knowledge of their parameters to be able to control them fully. In this paper, we discuss this issue and present and test two strategies for controlling complex forms in consumer-oriented generative design systems, one offering the user full control over the design (“total control” strategy), while the other automatically generates designs for the user (“no control” strategy). The implementation of those two control strategies in a generative design system for two categories of products (bookshelf and table) and five types of morphologies are described and tested with a number of design interested participants to estimate their level of satisfaction with the two control strategies. The empirical study shows that the participants enjoyed both the total control and no control strategies. The development of the full control modes for the five morphologies was on the other hand not straightforward, and in general, making the controls meaningful to the consumer can be difficult with complex morphologies. It seems that a consumer-oriented generative design system with two different control strategies, as the ones presented in this article, would offer the most satisfaction."
139,"In this paper we propose a method that includes stochastic and robust models for designing the network structure of a three-tiered supply chain involving suppliers, manufacturers and retailers under random demands of markets and by considering disruption probabilities in the procurement facilities and connecting links of the chain. Demands of the markets are as assumed to be random variables with normal distribution and the disruption of the chain is formulated by defining different scenarios. Having substitutable facilities and extra production capacities makes it possible to overcome the negative effects of demand fluctuations and disruption of facilities. First we determine the optimal number, location and capacity of facilities in the first and third tiers and the best flow of material and product throughout the chain in a way to maximize the expected profit from the chain. Then to reduce the difference between the performances of the chain in different scenarios, we propose the ISP-Robust technique based on P-Robust technique to make the model robust. We discuss the differences and advantages of this technique in comparison to the P-Robust technique. We illustrate our method using data from the petrochemical industry."
140,"The use of online, user-generated content for consumer preference modeling has been a recent topic of interest among the engineering and marketing communities. With the rapid growth of many different types of user-generate content sources, the tasks of reliable opinion extraction and data interpretation are critical challenges. This research investigates one of the largest and most-active content sources, Twitter, and its viability as a content source for preference modeling. Support Vector Machine (SVM) is used for sentiment classification of the messages, and a Twitter query strategy is developed to categorize messages according to product attributes and attribute levels. Over 7,000 messages are collected for a smartphone design case study. The preference modeling results are compared with those from a typical product review study, including over 2,500 product reviews. Overall, the results demonstrate that consumers do express their product opinions through Twitter; thus, this content source could potentially facilitate product design and decision-making via preference modeling."
141,"Initial populations for genetic algorithms are often created using randomly generated designs in an effort to maximize the genetic diversity in the design space. However, research indicates that the inclusion of solutions generated based on domain knowledge (i.e. non-random solutions) can notably improve the performance of the genetic algorithm with respect to solution performance and/or computational cost for convergence. This performance increase is extremely valuable for computationally expensive problems, such as product line optimization. In prior research, the authors demonstrated these improvements for product line design problems where market share of preference was the performance objective. Initial product line solutions were constructed from products that had the largest product-level utility for individual respondents. However, this simple product identification strategy did not adequately scale to accommodate the richer design problem associated with multiple objectives. This paper extends the creation of targeted initial populations to multiobjective product line design problems by using the objectives of the problem, instead of product level utility, to identify candidate designs. A MP3 player and vehicle feature packaging product line design problems are used to demonstrate this approach and assess the improvement of this modification."
142,"When design decisions are informed by consumer choice models, uncertainty in the choice model and its share predictions creates uncertainty for the designer. We take a first step in investigating the variation in and accuracy of market share predictions by characterizing fit and forecast accuracy of multinomial logit, mixed logit, and nested logit models over a variety of utility function specifications for the US light duty new vehicle market. Using revealed preference data for years 2004–2006, we estimate a multinomial logit model for each combination of a chosen set of utility function covariates found in the literature. We then use each of the models to predict vehicle shares for the 2007 market and examine several metrics to measure fit and predictive accuracy. We find that the best models selected using any of the proposed metrics outperform random guessing yet retain substantial error in fit and prediction for individual vehicle models. For example, with no information (random guessing) 30% of share predictions are within 0.2% absolute share error in a market with an average share of ∼0.4%, whereas for the best models 70% are within 0.2% (for the 2007 vehicle market this translates to an error of ∼33,000 units sold). Share predictions are sensitive to the presence of utility covariates but less sensitive to the form. Models that perform well on one metric tend to perform well on the other metrics as well. In particular, models selected for best fit have comparable forecast error to those with the best forecasts, and residual error in model fit is a major source of forecast error."
143,"Designing a marketable automobile for target markets is an important challenge in the globalized market environment. What makes the vehicle planning even more challenging includes various factors such as engineering feasibility, change of market environment, enforced regulations, and ongoing advancement of technology. In this circumstance, the complexity of automobile design arises from the fact that both the target market’s customer preferences and engineering characteristics should be taken into account in balanced as well as quantitative manners."
144,"The optimal maintenance scheduling of systems with degrading components is highly coupled with the design of the system and various uncertainties associated with the system, including the operating conditions, the interaction of different degradation profiles of various system components, and the ability to measure and predict degradation using prognostics and health management (PHM) technologies. Due to this complexity, designers need to understand the correlations and feedback between the design variables and lifecycle parameters to make optimal decisions. A framework is proposed for the high level integration of design, component degradation, and maintenance decisions. The framework includes constructing screening models for rapid design evaluation, defining a multi-objective robust optimization problem, and using sensitivity studies to compare trade-offs between different design and maintenance strategies. A case example of power plant condenser is used to illustrate the proposed framework and advise how designers can make informed comparisons between different design concepts and maintenance strategies under highly uncertain lifecycle conditions."
145,"This paper presents a new system design platform and approaches leading to the development of resilient engineered systems through integrating design of system functions and prognosis of function failures in a unified design framework. Failure prognosis plays an increasingly important role in complex engineered systems since it detects, diagnoses, and predicts the system-wide effects of adverse events, therefore enables a proactive approach to deal with system failures at the life cycle use phase. However, prognosis of system functional failures has been largely neglected in the past at early system design stage, mainly because quantitative analysis of failure prognosis in the early system design stage is far more challenging than these activities themselves that have been mainly carried out at the use phase of a system life cycle. In this paper, a generic mathematical formula of resilience and predictive resilience analysis will be introduced, which offers a unique way to consider lifecycle use phase failure prognosis in the early system design stage and to systematically analyze their costs and benefits, so that it can be integrated with system function designs concurrently to generate better overall system designs. Engineering design case studies will be used to demonstrate the proposed design for resilience methodology."
146,"Efficient health diagnostics provides benefits such as improved safety, improved reliability, and reduced costs for the operation and maintenance of engineered systems. This paper presents a multi-attribute classification fusion approach which leverages the strengths provided by multiple membership classifiers to form a robust classification model for structural health diagnostics. Health diagnosis using the developed approach consists of three primary steps: (i) fusion formulation using a k-fold cross validation model; (ii) diagnostics with multiple multi-attribute classifiers as member algorithms; and (iii) classification fusion through a weighted majority voting with dominance system. State-of-the-art classification techniques from three broad categories (i.e., supervised learning, unsupervised learning, and statistical inference) were employed as the member algorithms. The proposed classification fusion approach is demonstrated with a bearing health diagnostics problem. Case study results indicated that the proposed approach outperforms any stand-alone member algorithm with better diagnostic accuracy and robustness."
147,"Fault tree analysis (FTA) is an effective method of ensuring the security and safety of the product by identifying all the possible causes of the problem and fixing them. However, it is not easy for a designer to construct a complete fault tree about various physical phenomena without any misunderstanding or oversight, and some computerized method of managing (i.e., storing, searching and utilizing) knowledge about FTA is needed. To solve the problem, the authors have proposed and studied a method and software tool for knowledge management of FTA based on quantity dimension indexing as a design knowledge management method to avoid ambiguity of literal expression about physical phenomena. In the previous method and software, however, fault values of quantities were limited as just above- and below-normal, and dynamic phenomena such as oscillation could not be described. In this paper, the authors introduce a systematically classified definition of fault values as above/below normal, one-side/both-sides, constant/varying, monotonic/non-monotonic and sudden/gradual, and expand the computerized systematic approach to FTA. Feasibility of the method was examined by applying it to fault tree examples made in a company."
148,"This paper develops a Copula-based sampling method for data-driven prognostics and health management (PHM). The principal idea is to first build statistical relationship between failure time and the time realizations at specified degradation levels on the basis of off-line training data sets, then identify possible failure times for on-line testing units based on the constructed statistical model and available on-line testing data. Specifically, three technical components are proposed to implement the methodology. First of all, a generic health index system is proposed to represent the health degradation of engineering systems. Next, a Copula-based modeling is proposed to build statistical relationship between failure time and the time realizations at specified degradation levels. Finally, a sampling approach is proposed to estimate the failure time and remaining useful life (RUL) of on-line testing units. Two case studies, including a bearing system in electric cooling fans and a 2008 IEEE PHM challenge problem, are employed to demonstrate the effectiveness of the proposed methodology."
149,"The use of biomass cookstoves to meet household energy needs has a profound impact on the life and health of individuals, families, and communities in the developing world. This paper introduces an experimentally validated heat transfer analysis model for use during the conceptual design process of a biomass cookstove to be used in the developing world. This steady-state model of a shielded, natural-draft biomass cookstove fitted with a flat-bottomed pot with pot-shield was developed using published experimental data that included 63 variations of 15 operating, geometrical, and material variables. The model provides the essential information needed to support decision making during the cookstove conceptual design process by predicting heat transfer efficiency as a function of stove geometry, construction material, firepower, and fuel moisture content."
150,"This paper reviews the findings of several engineering researchers and practitioners on the topic of design for the developing world. We arrange these findings into eight guiding principles aimed at helping those who are searching for effective and sustainable approaches for design for the developing world. The findings reviewed come from the mechanical engineering discipline, as well as from other engineering disciplines. For each principle, we provide references to various studies as a means of supporting the principle. We also provide a detailed example of each principle. Finally, based on our own experience and based on the many papers reviewed, we provide a succinct list of suggestions for using each principle. Ultimately, we believe that the stated principles help overcome the challenges of design for the developing world, which are often dominated by designer unfamiliarity with poverty and foreign culture, as well as by the constraint of extreme affordability."
151,"In some villages the use of wood cooking stoves accounts for more than three-quarters of total village energy use. Because of this the design of clean, affordable, and desirable cooking stoves can have a dramatic impact on human health and the local economy. Unfortunately, too often development projects fail. For example, an estimated 30% of water projects in sub-Saharan Africa have failed prematurely in the last 20 years, and only 10% of cooking stove programs started in the 1980s were operational two years after startup. Similar anecdotal evidence suggests a mixed record of success for other energy, infrastructure, health, and sanitation projects in the developing world. In part, these failures occur because of a lack of design questions and design methods to identify consumer need and preference during the problem definition phase of the product design. Because isolated rural villages are generally far from the design engineers’ previous experiences it is even more important to gather in-depth primary data in isolated rural villages. Based on data collected during in-depth field visits to villages in rural West Africa during a village energy study this paper proposes a structured process for collecting the data necessary to design cookstoves that meet local needs, fit within local contexts, and create an aspirational experience that fosters a sustainable solution."
152,"Kerosene, candles, and disposable batteries are commonplace in the developing world for rural domestic lighting. These technologies come with negative health and environmental effects that are well documented and often form the basis for engineering design. The immediate and near-term concerns that families experience on a daily basis are also important — economics, quality of light, and quality of service. Families in off-grid rural villages often spend more than half of their energy-related expenditures on domestic lighting. Many technologies have been implemented to provide low-cost and renewable power for lighting, yet these efforts have had a mixed record of success due to persistent financial barriers, issues of consumer acceptance and adoption, and a variety of technical complications. The incidence of these problems can be reduced by completing a techno-economic comparison of alternatives during conceptual design. This paper compares three major categories of off-grid domestic lighting projects: (1) centralized electrification with a micro-grid, (2) battery charging stations, and (3) solar lanterns. The HOMER Energy software is used to compare these options using data gathered from rural villages in Africa. To offer a comparison to existing options available, this paper provides a full financial comparison to a base case — kerosene lanterns — to suggest financing strategies and business models for the options investigated."
153,"The complexity of today’s highly engineered products is rooted in the interwoven architecture defined by its components and their interactions. Such structures can be viewed as the adjacency matrix of the associated dependency network representing the product architecture. To evaluate a complex system or to compare it to other systems, numerical assessment of its structural complexity is essential. In this paper, we develop a quantitative measure for structural complexity and apply the same to real-world engineered systems like gas turbine engines. It is observed that low topological complexity implies centralized architectures and that higher levels of complexity generally indicate highly distributed architectures. We posit that the development cost varies non-linearly with structural complexity. Empirical evidence of such behavior is presented from the literature and preliminary results from simple experiments involving assembly of simple structures further strengthens our hypothesis. We demonstrate that structural complexity and modularity are not necessarily negatively correlated using a simple example. We further discuss distribution of complexity across the system architecture and its strategic implications for system development efforts."
154,"Designing complex systems often requires consideration of many components interacting across vast scales of space and time, thus producing highly challenging design spaces to search. In particular, nano-based technologies may require considerations of how nanoscale (10"
155,"In this paper, a genetic algorithm (GA) is used to discover interaction rules for a cellular self-organizing (CSO) system. The CSO system is a group of autonomous, independent agents that perform tasks through self-organization without any central controller. The agents have a local neighborhood of sensing and react only to other agents within this neighborhood. Their interaction rules are a simple set of direction vectors based on a flocking model. The five local interaction rules are assigned relative weights, and the agents self-organize to display some emergent behavior at the system level. The engineering challenge is to identify which sets of local rules will cause certain desired global behaviors. The global required behaviors of the system, such as flocking or exploration, are translated into a fitness function that can be evaluated at the end of a multi-agent based simulation run. The GA works by tuning the relative weights of the local interaction rules so that the desired global behavior emerges, judged by the fitness function. The GA approach is shown to be successful in tuning the weights of these interaction rules on simulated CSO systems, and, in some cases, the GA actually evolved qualitatively different local interaction “strategies” that displayed equivalent emergent capabilities."
156,"Significant technological advances in sensing and communication promote the use of large sensor networks to monitor structural systems, identify damages, and quantify damage levels. Prognostics and health management (PHM) technique has been developed and applied for a variety of safety-critical engineering structures, given the critical needs of the structure health state awareness. The PHM performance highly relies on real-time sensory signals which convey the structural health relevant information. Designing an optimal structural sensor network (SN) with high detectability is thus of great importance to the PHM performance. This paper proposes a generic SN design framework using a detectability measure while accounting for uncertainties in material properties and geometric tolerances. Detectability is defined to quantify the performance of a given SN. Then, detectability analysis will be developed based on structural simulations and health state classification. Finally, the generic SN design framework can be formulated as a mixed integer nonlinear programming (MINLP) using the detectability measure and genetic algorithms (GAs) will be employed to solve the SN design optimization problem. A power transformer study will be used to demonstrate the feasibility of the proposed generic SN design methodology."
157,"This paper presents a complex network and graph spectral approach to calculate the resiliency of complex engineered systems. Resiliency is a key driver in how systems are developed to operate in an unexpected operating environment, and how systems change and respond to the environments in which they operate. This paper deduces resiliency properties of complex engineered systems based on graph spectra calculated from their adjacency matrix representations, which describes the physical connections between components in a complex engineered systems. In conjunction with the adjacency matrix, the degree and Laplacian matrices also have eigenvalue and eigenspectrum properties that can be used to calculate the resiliency of the complex engineered system. One such property of the Laplacian matrix is the algebraic connectivity. The algebraic connectivity is defined as the second smallest eigenvalue of the Laplacian matrix and is proven to be directly related to the resiliency of a complex network. Our motivation in the present work is to calculate the algebraic connectivity and other graph spectra properties to predict the resiliency of the system under design."
158,"In designing a microstructural materials system, there are several key questions associated with design representation, design evaluation, and design synthesis: how to quantitatively represent the design space of a heterogeneous microstructure system using a small set of design variables, how to efficiently reconstruct statistically equivalent microstructures for design evaluation, and how to quickly search for the optimal microstructure design to achieve the desired material properties. This paper proposes a new descriptor-based methodology for designing microstructural materials systems. A descriptor-based characterization method is proposed to provide a quantitative representation of material morphology using a small set of microstructure descriptors covering features of material composition, dispersion status, and phase geometry at different levels of representation. A descriptor-based multi-phase microstructure reconstruction algorithm is developed which allows efficient stochastic reconstruction of microstructures for Finite Element Analysis (FEA) of material behavior. The choice of descriptors for polymer nanocomposites is verified by establishing a mapping between the finite set of descriptors and the infinite dimensional correlation function. Finally, the descriptor-based representation allows the use of parametric optimization approach to search the optimal microstructure design that meets the target material properties. To improve the search efficiency, this paper employs state-of-the-art computational design methods such as Design of Experiment (DOE), metamodeling, statistical sensitivity analysis, and multi-objective optimization. The proposed methodology is demonstrated using the design of a polymer nanocomposites system."
159,"Integrated Materials and Products Design (IMPD) differs in the way that materials as well as product layout are designed or optimized in a concurrent manner to meet design requirements. IMPD allows the specific performance required in a product to be achieved by tailoring materials and product, since system performance will not be limited by a pre-chosen material employed in conventional, material-selection-based design. In this study, Blast Resistance Panels (BRPs) with square honeycomb core are designed based on this new design approach to further enhance the performance of BRPs."
160,"This paper presents a design approach for developing meso-structure for high shear flexure in considering distribution of strain energy for a unit cell. Currently, flexible components are often designed with elastomers to take advantage of their unique properties of low shear modulus and high elongation. However, elastomers exhibit high loss modulus at a high frequency when they are subjected to cyclic loading. As a design requirement to find an alternative material in one of the sub systems in the extraterrestrial rover, materials with high elongation but low energy loss is investigated using a meso-structure design approach. In this paper, an approach to design a meso-structure exhibiting shear flexure is developed by conducting comparative studies of shear flexure on three equivalent configurations: auxetic, honeycomb, and sinusoidal. Based on this comparative study, a new hypothesis is proposed that specific strain energy distribution pattern in these meso-structure has a direct impact on the high shear flexure performance. This proposition is verified by developing a new meso-structure, termed ‘S’-Type, which is compared with auxetic and sinusoidal auxetic meso-structures on their shear flexure ability. It is shown from this comparative analysis that the ‘S’-Type meso-structure exhibits higher shear flexure than the other two meso-structures at 5, 10, 20, and 40 MPa of effective shear moduli. Hence, based on this result, a four-step design approach is proposed to design future meso-structures with high shear flexure."
161,"The design protocols of meso-structures to satisfy given properties have been an interesting area in material science. Having various mechanical properties by modifying the topology of unit cells is a major issue in developing new meso-structures. This paper reviews different design methodologies to design meso-structures. For each method, an algorithm is presented and compared. Computational methods including topology optimization, parametric optimization, and synthesis methods are among the most popular methods for design meso-structures. Ultimately, it is found that there is a gap in systematic design methods for developing new meso-structure architectures as the current methods are limited to parametric sizing and selection. This gap will be addressed in future research."
162,"In discrete topology optimization, material state is either solid or void and there is no topology uncertainty caused by intermediate material state. A common problem of the current discrete topology optimization is that boundaries are unsmooth. Unsmooth boundaries are caused by corners in topology solutions. Although the outer corner cutting and inner corner filling strategy can mitigate corners, it cannot eliminate them. 90-degree corners are usually mitigated to 135-degree corners under the corner handling strategy. The existence of corners in topology solutions is because of the subdivision model. If regular triangles are used to subdivide design domains, corners are inevitable in topology solutions. To eradicate corner from any topology solution, a subdivision model is introduced in this paper for the discrete topology optimization of structures. The design domain is discretized into quadrilateral design cells and every quadrilateral design cell is further subdivided into triangular analysis cells that have a curved hypotenuse. With the presented subdivision model, all boundaries and connections are smooth in any topology solution. The proposed subdivision approach is demonstrated by two discrete topology optimization examples of structures."
163,"The introduction of cellular materials models in topology optimization allows designers to achieving significant weight reductions in structural applications. However, higher material savings and increased performance can be achieved if the material and the structure topologies are concurrently designed. The objective of this paper is to incorporate and establish a design methodology to obtaining optimal macro-scale structures and the corresponding optimal meso-scale periodic material designs in continuum design domains. The proposed approach makes use of homogenization theory to establish communication bridges between both material and structural scales. The periodicity constraint makes such cellular materials manufacturable. Penalization methods are used to obtaining binary solutions in both scales. This proposed methodology is demonstrated in the design of compliant mechanisms and structures of minimum compliance. The results demonstrate potential benefits when this multi-scale design algorithm when applied to the design of ultra-lightweight structures."
164,"Recent research in the field of composite materials has shown that it is theoretically possible to produce composite materials with macroscopic mechanical stiffness and loss properties that surpass those of conventional composites. This research explores the possibility of designing and fabricating these composite materials by embedding small volume fractions of negative stiffness inclusions in a continuous host material. Achieving high stiffness and loss from these materials by design, however, is a nontrivial task. This paper presents a hierarchical multiscale material model for these materials, coupled with a set-based, multilevel design approach based on Bayesian network classifiers. Bayesian network classifiers are used to map promising regions of the design space at each hierarchical modeling level, and then the maps are intersected to identify sets of multilevel or multiscale solutions that are likely to provide desirable system performance. Length scales range from the behavior of the structured microscale negative stiffness inclusions to the effective properties of mesoscale composite materials to the performance of an illustrative macroscale component — a vibrating beam coated with the high stiffness, high loss composite material."
165,"In finite element analysis, inertia relief solves the response of an unconstrained structure subject to constant or slowly varying external loads with static analysis computational cost. It is very attractive to utilize it in topology optimization to design structures under unbalanced loads, such as in impact and drop phenomena. In this paper, regional strain energy formulation and inertia relief is integrated into topology optimization to design protective structure under unbalanced loads. For background, the equations of inertia relief are introduced and a commonly used solving method is revisited. Then the regional strain energy formulation for topology optimization with inertia relief is proposed and its sensitivity is derived from the adjoint method. Based on the solving method, the sensitivity is evaluated term by term to simplify the results. The simplified sensitivity can be calculated easily using the output of commercial finite element packages. Finally, the effectiveness of this formulation is shown in the first example and the proposed regional strain energy formulation for topology optimization with inertia relief are presented and discussed in the protective structure design examples."
166,"Ceramic matrix composites (CMC) have been widely studied to tailor desired properties at high temperatures. However, research applications involving design tool development for multi-phase material design are at an early stage of development. While numerical CMC modeling provides significant insight on the material performance, the computational cost of the numerical simulations and the type of variables involved in these models are a hindrance for the effective application of design methods. This technical challenge heightens with the need of considering the uncertainty of material processing and service. For this reason, few design researchers have addressed the design paradox that accompanies the rapid design space expansion in CMC material design."
167,"Rheological material properties are high-dimensional function-valued quantities, such as frequency-dependent viscoelastic moduli or non-Newtonian shear viscosity. Here we describe a process to model and optimize design targets for such rheological material functions. For linear viscoelastic systems, we demonstrate that one can avoid specific a priori assumptions of spring-dashpot topology by writing governing equations in terms of a time-dependent relaxation modulus function. Our approach embraces rheological design freedom, connecting system-level performance to optimal material functions that transcend specific material classes or structure. This technique is therefore material agnostic, applying to any material class including polymers, colloids, metals, composites, or any rheologically complex material. These early-stage design targets allow for broadly creative ideation of possible material solutions, which can then be used for either material-specific selection or later-stage design of novel materials."
168,"Product and design analytics is emerging as a promising area for the analysis of large-scale data and reflection of the extracted knowledge for the design of optimal system. The Continuous Preference Trend Mining (CPTM) algorithm and a framework that are proposed in this study address some fundamental challenges in the context of product and design analytics. The first contribution is the development of a new predictive trend mining technique that captures a hidden trend of customer purchase patterns from large accumulated transactional data. Different from traditional, static data mining algorithms, the CPTM does not assume the stationarity, and dynamically extract valuable knowledge of customers over time. By generating trend embedded future data, the CPTM algorithm not only shows higher prediction accuracy in comparison with static models, but also provide essential properties that could not be achieved with a previous proposed model: avoiding an over-fitting problem, identifying performance information of constructed model, and allowing a numeric prediction. The second contribution is a predictive design methodology in the early design stage. The framework enables engineering designers to optimize product design over multiple life cycles while reflecting customer preferences and technological obsolescence using the CPTM algorithm. For illustration, the developed framework is applied to an example of tablet PC design in leasing market and the result shows that the selection of optimal design is achieved over multiple life cycles."
169,"In engineering design, time-consuming simulations may be needed to find the input-output relationship of a system. High Dimensional Model Representation (HDMR) alleviates the need for intensive simulation by approximating the system’s design space with a surrogate model. Although HDMR can provide an overview, specific regions of interest to the designer may require higher accuracy. This paper presents a tool to visualize and interactively improve HDMR accuracy in specified regions of the design space. Regions of the HDMR are selected by iterative brushing in two-dimensional scatterplot planes. Once a region is chosen, designers may concentrate sampling within its bounds to improve the model locally. Regions can be also improved by modeling the error with a localized radial basis function (RBF) metamodel. The effect of local refinement was further evaluated with localized performance metrics. Testing of the tool shows that it can effectively display and improve HDMR models in regions of interest, if there are variables which have a dominating influence on the output."
170,"A growing area of research in the engineering community is the use of data and analytics for transforming information into knowledge to design better systems, products, and processes. Data-driven decisions can be made in the early, middle, and late stages in a design process where customer needs are identified and understood, a final concept for a design is chosen, and usage data from the deployed product is captured, respectively. Design Analytics (DA) is a paradigm for improving the core information-to-knowledge transformations in these stages of a design process resulting in better performing and functioning products that reflect both explicit and implicit customer needs. In this paper, a simulator is used to model usage of a hypothetical refrigerator and generate artificial data driven by four different customer behavior profiles with variation. The population of customers is randomly divided among the four behavior profiles so that the underlying customer preferences are unknown to the experimenter prior to data analysis. The purpose of the simulation is to illustrate the use of DA in the late stage of a design process to improve the transition from an existing product to the next generation product. Metrics are developed to analyze the product usage data, and both prevailing and subtle usage trends are identified. After conclusions are made, the study proceeds to the early and middle stages of a subsequent design process where a hypothetical next-generation refrigerator is conceptualized."
171,"Crowdsourced evaluation is a promising method for evaluating attributes of design concepts that require human input. One factor in obtaining good evaluations is the ratio of high-ability to low-ability participants within the crowd. In this paper we introduce a Bayesian network model capable of finding participants with high design evaluation ability, so that their evaluations may be weighted more than those of the rest of the crowd. The Bayesian network model also estimates a score of how well each design concept performs with respect to a design attribute without knowledge of the true scores. Monte Carlo simulation studies tested the quality of the estimations on a variety of crowds consisting of participants with different evaluation ability. Results suggest that the Bayesian network model estimates design attribute performance scores much closer to their true values than simply weighting the evaluations from all participants in the crowd equally. This finding holds true even when the group of high ability participants is a small percentage of the entire crowd."
172,"We examine the problem of eliciting the most preferred designs of a user from a finite set of designs through iterative pairwise comparisons presented to the user. The key challenge is to select proper queries (i.e., presentations of design pairs to the user) in order to minimize the number of queries. Previous work formulated elicitation as a blackbox optimization problem with comparison (binary) outputs, and a heuristic search algorithm similar to Efficient Global Optimization (EGO) was used to solve it. In this paper, we propose a query algorithm that minimizes the expected number of queries directly, assuming that designs are embedded in a known space and user preference is a linear function of design variables. Besides its theoretical foundation, the proposed algorithm shows empirical performance better than the EGO search algorithm in both simulated and real-user experiments. A novel approximation scheme is also introduced to alleviate the scalability issue of the proposed algorithm, making it tractable for a large number of design variables or of candidate designs."
173,"The primary objective in precision machining is usually to attain excellent dimensional accuracy and surface finish. In addition, complimentary objectives such as cost and production rate are also important. Proper selection of cutting parameters can profoundly affect both primary and secondary machining performance objectives. While simplified and/or empirical models exist for machining processes, none of those models provides accurate prediction of the dynamic cutting forces, which in turn govern the obtainable quality of the machined surfaces. Finite element analysis (FEA) via ABAQUS/Explicit is adopted in this paper for predicting the machining dynamic cutting forces. Rake and clearance angles, as well as cutting speed are set as the design variables for optimization. Since the machining model requires significant computational resources, economizing the number of FEA runs is desirable. The optimization approach adopted is based off Efficient Global Optimization (EGO), where Kriging models are trained to predict the underlying behavior of the machining process via a finite set of sample points. New sample points are then generated via a multi-objective genetic algorithm that seeks locations of optima and/or high uncertainty in the Kriging models. Machining performance of the new samples is then evaluated via FEA, the Kriging models are re-trained and the process is repeated until one of termination criteria is met. The application study presented is an orthogonal cutting test for ultra-precision micro-cutting using diamond tools."
174,"With the increase in computer-controlled hybrid machining (e.g. mill-turn machines), one needs to discern what features of a part are created during turning (i.e. with a lathe cutter) versus those created by milling. Given a generic part shape, it is desirable to extract the turnable and non-turnable features in order to obtain feasible machining plans. A novel approach for automating this division and for defining the resulting turning operations in a hybrid process is proposed in this paper. The algorithm is based on identifying the dominant rotational-axis and performing several non-uniform lateral cross-sections to quickly generate the “as lathed” model. The part is then subtracted from the original model to isolate the non-turnable features. Next, resulting model and features are translated to a label rich graph and fed into a grammar reasoning tool to produce feasible manufacturing plans. The setup design is also studied against the tolerances specified by the designer. Performance of the algorithm has been tested on several examples ranging from simple to complex parts."
175,"This paper concerns about modeling tolerance accumulation in parallel assemblies using a spatial math model, the T-Map. In this paper, a specific case in 3D is discussed where an Accumulation Tolerance-Map is modeled when two parts arranged in parallel support a target part between the datum and the functional target feature. By understanding how much of variation from the supporting parts contribute to variations of the target feature, a designer can better utilize the tolerance budget when assigning values to individual tolerances."
176,"Increased use of recycled material in high-end structural components based on wrought alloys is the goal of the EC project Suplight. The project proposes a framework for multi objective optimization. In this paper, a tolerance plugin module used in this project is described. The tolerance plugin module aims at controlling if the geometrical variation requirements on part level are fulfilled. The variation in the part stems from variation in material and process parameters and the relationship between variation in process and material parameters is estimated using designed computer experiments. Moreover, the tolerance plugin module offers an automatically generated meta-model, based on principal component analysis, for handling part variation that allows for faster Monte Carlo simulations and a format that can be used in variation simulations in succeeding assembly steps. The functionality is illustrated using two cases studies; one for investigation of geometrical part variation due to a stamping process and one for investigation of geometrical part variation due to a sheet metal forming process."
177,"In process planning of machined part, machining feature recognition and representation, feature-based generative process planning, and the process intermediate model generation are the key issues. While many research results have been achieved in recent years, the complete modeling of machining features, process operations, and the 3D models in process planning are still need further research to make the techniques to be applied in practical CAPP systems. In this paper, a machining feature definition and classification method is proposed for the purpose of process planning based on 3D model. Machining features are defined as the surfaces formed by a serious of machining operation. The classification scheme of machining features is proposed for the purpose of feature recognition, feature-based machining operations reasoning, and knowledge representation. Recognized from B-Rep representation of design model, machining features are represented by adjacent graph and organized by feature relations. The machining process plan is modeled as operations and steps, which is the combination and sequencing of machining feature’s process steps. The process intermediate models (PIM) are important for process documentation, analysis and NC programming. An automatic PIM generation approach is proposed using local operations directly on B-Rep model. The proposed data structure and algorithm is adopted in the development of CAPP tool on solid modeler ACIS/HOOPS."
178,"Optimization of nonlinear (or linear state-dependent) dynamic systems often requires system simulation. In many cases the associated state derivative evaluations are computationally expensive, resulting in simulations that are significantly slower than real-time. This makes the use of optimization techniques in the design of such systems impractical. Optimization of these systems is particularly challenging in cases where control and physical systems are designed simultaneously. In this article, an efficient two-loop method, based on surrogate modeling, is proposed for solving dynamic system design problems with computationally expensive derivative functions. A surrogate model is constructed for only the derivative function instead of the complete system analysis, as is the case in previous studies. This approach addresses the most expensive element of system analysis (i.e., the derivative function), while limiting surrogate model complexity. Simulation is performed based on the surrogate derivative functions, preserving the nature of the dynamic system, and improving estimation accuracy. The inner loop solves the system optimization problem for a given derivative function surrogate model, and the outer loop updates the surrogate model based on optimization results. This solution approach presents unique challenges. For example, the surrogate model approximates derivative functions that depend on both design and state variables. As a result, the method must not only ensure accuracy of the surrogate model near the optimal design point in the design space, but also the accuracy of the model in the state space near the state trajectory that corresponds to the optimal design. This method is demonstrated using two simple design examples, followed by a wind turbine design problem. In the last example, system dynamics are modeled using a linear state-dependent model where updating the system matrix based on state and design variable changes is computationally expensive."
179,"Metamodel based design optimization (MBDO) algorithms have attracted considerable interests in recent years due to their special capability in dealing with complex optimization problems with computationally expensive objective and constraint functions and local optima. Conventional unimodal-based optimization algorithms and stochastic global optimization algorithms either miss the global optimum frequently or require unacceptable computation time. In this work, a generic testbed/platform for evaluating various MBDO algorithms has been introduced. The purpose of the platform is to facilitate quantitative comparison of different MBDO algorithms using standard test problems, test procedures, and test outputs, as well as to improve the efficiency of new algorithm testing and improvement. The platform consists of a comprehensive test function database that contains about 100 benchmark functions and engineering problems. The testbed accepts any optimization algorithm to be tested, and only requires minor modifications to meet the test-bed requirements."
180,"Mode Pursuing Sampling (MPS) was developed as a global optimization algorithm for optimization problems involving expensive black box functions. MPS has been found to be effective and efficient for problems of low dimensionality, i.e., the number of design variables is less than ten. A previous conference publication integrated the concept of trust regions into the MPS framework to create a new algorithm, TRMPS, which dramatically improved performance and efficiency for high dimensional problems. However, although TRMPS performed better than MPS, it was unproven against other established algorithms such as GA. This paper introduces an improved algorithm, TRMPS2, which incorporates guided sampling and low function value criterion to further improve algorithm performance for high dimensional problems. TRMPS2 is benchmarked against MPS and GA using a suite of test problems. The results show that TRMPS2 performs better than MPS and GA on average for high dimensional, expensive, and black box (HEB) problems."
181,"Discrete Mode Pursuing Sampling (D-MPS) is a method used for optimization of expensive black-box functions with discrete variables. The type of discrete space where all possible combinations of discrete values of all variables are valid design points is called a full grid of points or a “regular grid” in this paper. A regular structure for sampling data is a requirement when D-MPS is applied. This paper presents two new indexing methods, i.e. “"
182,"While computers are continually getting faster, physical models of complex systems grow more sophisticated and keep pace. Using metamodels can dramatically reduce the time it takes to evaluate a solution for a complex system; however, while the chief virtue of metamodels is that they approximate more computationally expensive models, this is also their main drawback. Metamodels are approximations, and as such they behave differently than the more sophisticated models they approximate. While the metamodels may be accurate approximations, they may also introduce new interdependencies in the response outputs that may hinder search algorithms during optimization. Understanding the impact of the approximation on the subsequent search thus becomes an important part of the problem as models that were computationally expensive ten to fifteen years ago may now run fast enough for use in optimization. In this paper, we use Sobol′ global sensitivity analysis to compare the search performance of a new auto-adaptive many objective evolutionary algorithm solving a challenging product family design problem with both the original analysis and a second-order response surface approximation of the original analysis. Interdependencies in the response outputs are found to result from the problem formulation used rather than the underlying model in this case. Search operator selection by the auto-adaptive evolutionary algorithm is shown to be consistent with the model sensitivities found by global sensitivity analysis."
183,"Estimation of density algorithms (EDAs) have been developed for optimization of discrete, continuous, or mixed discrete and continuous simulation-based design problems. EDAs construct a probability distribution on the set of highest performing designs and sample the distribution for the next generation of solutions. In previous work, the authors have demonstrated how classifier-guided sampling can also be used for discrete variable, discontinuous design space exploration. In this paper we develop the rationale for using classifier-guided sampling as a simple step beyond EDAs that not only improves the characterization of the highest performing designs but also identifies the poorly performing designs and exploits that information for faster convergence to optimal solutions. The resulting method is novel in its use of Bayesian priors to model the inherent uncertainty in a probability distribution that is based on a limited number of samples from the design space. The new classifier-guided method is applied to several example problems and convergence rates are presented that compare favorably to random search and a basic EDA implementation."
184,"Traditionally, understanding indoor space utilization in a typical design setting has been based on observation methodologies, where researchers document team interactions, space utilization and design activities using qualitative observation techniques. The authors of this paper propose a data mining driven methodology aimed at modeling the utilization of indoor design spaces using trajectory pattern data. Using indoor Radio-frequency identification (RFID) technology, researchers are able to collect trajectory data which can then be used to quantify the distribution of space usage patterns over time and predict future regions of interest. The proposed methodology consists of two phases: i) trajectory partitioning and ii) line segment clustering. For the first phase, trajectories are partitioned into line segments, based on unique user characteristics. In the second phase, a data mining clustering algorithm is employed to group line segments into different clusters based on a distance function. Since individual trajectories may exhibit similar movement patterns, the proposed methodology can help designers better understand how design spaces are utilized and how team dynamics evolve over time, depending on the specific design task being executed. A 3,500 square foot design space was used for the semester long study that included design teams supervised by teaching assistants. The results provide insight into the underutilization of certain regions of the design space and proposes directions towards an optimal design space methodology."
185,"Globalized marketplaces are necessitating the consideration of the needs of users from a variety of national and international regions. Relevant body dimensions are known to play a key role in influencing users’ physical interactions with products. The main challenge in designing these products is the unavailability of comprehensive anthropometric databases for detailed analyses and decision-making. This paper presents a new method to this end. Z-scores are computed for each body measure of every individual in a reference population; this can be any population for which a comprehensive database is available. Next, descriptive statistical information (e.g., means, standard deviations, by-percentile values) from numerous studies and surveys are used to estimate distributions of the required body dimensions. Finally, the z-score values from the reference population are utilized to sample from the aforementioned distributions in order to synthesize the requisite virtual target population of users. The z-score method is demonstrated in the context of two existing populations: U.S. military in the late 1980s (ANSUR) and Japanese youth from the early 1990s. Despite certain stated limitations, which are topics of future work in this line of research, the method is shown to be accurate, easy-to-apply, and robust in terms of underlying assumptions."
186,"Assessing the number of users defined by a set of specific usage attributes in a given usage contextual situation is not always an obvious task in a market segmentation process. Although new approaches in design and marketing seem to be more sensitive to the adequacy of a design concept with the usage scenarios, these methods do not systematically consider the various usage situations. The present article puts forward a methodology intending to build a usage scenarios space in which the input data is thoroughly collected and validated. This methodology is applied to the complex and multifactorial issue of falls among the elderly in the Metropolitan France. In this paper, numerous medical publications have been made to study influential factors of fall situations. However, even solution providers for fall prevention and teleassistance ignore the real situational coverage of their solutions. As a result, “usage scenarios space” is built using an appropriate segmentation of usage contexts (here, fall situations) and user characteristics. These data are used for a design oracle to predict (simulate) the various and multiple usage scenarios."
187,"A respirator protects its user by sealing the user’s face and filtering hazardous particles from environment. However, faceseal leakages of users-respirators always happen. First, this study investigated a computer-aided technique for designing a well fitted N95 filtering facepiece respirator (FFR) for a subject. The customized N95 FFR includes a customized contact area, a center filtering area, two straps and a nasal clip. Five base contact areas of National Institution for Occupational Safety and Health (NIOSH) headforms were created and the customized contact area was modeled using the mapping relationship between the subject and a NIOSH headform. The center filtering area was designed by considering constraints of N95 FFR shape. Second, this study used simulation-based approaches, including the FE method and computational fluid dynamics (CFD) method, to assess the performance of the customized N95 FFR on the subject. The contact pressure and the faceseal leakage from the subject-customized N95 FFR combination were compared with the results from the subject-existing N95 FFRs combinations. The comparison showed that the customized N95 FFR provided the subject an optimized contact pressure distribution and no faceseal leakage."
188,"This paper investigates two methods for the prediction of onset of slip in gait during the transition from single support to double support. The first method employs an optimization-based gait prediction simulation to determine the threshold of walking velocity that results in stable gait for one stride length and one subject. In order to determine the threshold, the simulation is carried out with progressively increasing walking velocities (initial conditions) until the gait becomes unstable. The zero moment point (ZMP) and support region are employed as criteria for stable gait, i.e. as long as the ZMP remains within the support region throughout the duration of gait, the gait is said to be stable. The second method employs a probabilistic simulation to predict the likelihood of slip, where the likelihood of slip is related to the available and required friction for gait."
189,"Every day, people are presented with tasks that are completed with very little mechanical effort such as turning a door knob, turning a screw driver, and grabbing a cup to move it to a new location and/or orientation. These tasks are often overlooked in the mechanical study of human movement due to the fact they carry with them very little biomechanical costs or effort. However, from a cognitive standpoint, these tasks carry high complexity. For example, the simple task of grabbing a cup and flipping it over is very easy mechanically and can be effortlessly achieved by a two year old. However, it is impossible to predict or simulate this motion without major intervention in the form of explicit constraints defining the task. The model itself cannot decide in which orientation the hand should assume in order to grasp the object. Also, it cannot decide where on the object the hand should be placed. These aspects must be assumed by the researcher and constrained in the formulation. In other words, digital human models, such as optimization-based motion prediction models, are unable to plan actions. This implies that for each task, a unique optimization formulation is needed in order to predict the motion/posture needed to complete each task. This paper presents a new method for task planning prediction within optimization based posture and motion prediction. It provides a new single optimization formulation that allows for the prediction of multiple unique manual manipulation tasks. The method is based on observations made from experimental studies on cognitive motor planning."
190,"Traditional user experience (UX) models are mostly qualitative in terms of its measurement and structure. This paper proposes a quantitative UX model based on cumulative prospect theory. It takes a decision making perspective between two alternative design profiles. However, affective elements are well-known to have influence on human decision making, the prevailing computational models for analyzing and simulating human perception on UX are mainly cognition-based models. In order to incorporate both affective and cognitive factors in the decision making process, we manipulate the parameters involved in the cumulative prospect model to show the affective influence. Specifically, three different affective states are induced to shape the model parameters. A hierarchical Bayesian model with a technique called Markov chain Monte Carlo is used to estimate the parameters. A case study of aircraft cabin interior design is illustrated to show the proposed methodology."
191,"This paper reports the development of a low-cost sensor-based glove device using commercially available components that can be used to obtain position, velocity and acceleration data for individual fingers of the hand. Optical tracking of the human hand and finger motion is a challenging task due to the large number of degrees of freedom (DOFs) packed in a relatively small space. We propose methods to simplify the hand motion capture by utilizing accelerometers and adopting a reduced marker protocol."
192,"This paper presents a method to identify the exact Pareto front for a multi-objective optimization problem. The developed technique addresses the identification of the Pareto frontier in the cost space and the Pareto set in the design space for both constrained and unconstrained optimization problems. The proposed approach identifies a "
193,"Multi-Objective Robust Optimization (MORO) can find Pareto solutions to multi-objective engineering problems while keeping the variation of the solutions being within an acceptable range when parameters vary. While the literature reports on many techniques in MORO, few papers focus on the implementation of Multi-Objective Differential Evolution (MODE) for robust optimization and the performance improvement of solutions. In this paper, MODE is first modified and implemented for robust optimization, formulating a new MODE-RO algorithm. To improve the solutions’ quality of MODE-RO, a new hybrid MODE-SQP-RO algorithm is further proposed, where Sequential Quadratic Programming (SQP) is incorporated to enhance the local search. In the hybrid algorithm, two criteria, indicating the convergence speed of MODE-RO and the switch between MODE and SQP are proposed respectively. One numerical and one engineering examples are tested to demonstrate the applicability and performance of the proposed algorithms. The results show that MODE-RO is effective in solving Multi-Objective Robust Optimization problems; while on the average, MODE-SQP-RO significantly improves the quality of robust solutions with comparable numbers of function evaluations."
194,"In the presence of multiple optimal solutions in multi-modal optimization problems and in multi-objective optimization problems, the designer may be interested in the robustness of those solutions to make a decision. Here, the robustness is related to the sensitivity of the performance functions to uncertainties. The uncertainty sources include the uncertainties in the design variables, in the design environment parameters, in the model of objective functions and in the designer’s preference. There exist many robustness indices in the literature that deal with small variations in the design variables and design environment parameters, but few robustness indices consider large variations. In this paper, a new robustness index is introduced to deal with large variations in the design environment parameters. The proposed index is bounded between zero and one, and measures the probability of a solution to be optimal with respect to the values of the design environment parameters. The larger the robustness index, the more robust the solution with regard to large variations in the design environment parameters. Finally, two illustrative examples are given to highlight the contributions of this paper."
195,"Interval is an alternative to probability distribution in quantifying epistemic uncertainty for reliability analysis when there is a lack of data to fit a distribution with good confidence. It only requires the information of lower and upper bounds. The propagation of uncertainty is analyzed by solving interval-valued constraint satisfaction problems (CSPs). By introducing logic quantifiers, quantified constraint satisfaction problems (QCSPs) can capture more semantics and engineering intent than CSPs. Sensitivity analysis (SA) takes into account of variations associated with the structure and parameters of interval constraints to study to which extent they affect the output. In this paper, a global SA method is developed for QCSPs, where the effects of quantifiers and interval ranges on the constraints are analyzed based on several proposed metrics, which indicate the levels of indeterminacy for inputs and outputs as well as unsatisfiability of constraints. Two vehicle design problems are used to demonstrate the proposed approach."
196,"Many engineering design optimization problems involve multiple conflicting objectives, which today often are obtained by computational expensive finite element simulations. Evolutionary multi-objective optimization (EMO) methods based on surrogate modeling is one approach of solving this class of problems. In this paper, multi-objective optimization of a disc brake system to a heavy truck by using EMO and radial basis function networks (RBFN) is presented. Three conflicting objectives are considered. These are: 1) minimizing the maximum temperature of the disc brake, 2) maximizing the brake energy of the system and 3) minimizing the mass of the back plate of the brake pad. An iterative Latin hypercube sampling method is used to construct the design of experiments (DoE) for the design variables. Next, thermo-mechanical finite element analysis of the disc brake, including frictional heating between the pad and the disc, is performed in order to determine the values of the first two objectives for the DoE. Surrogate models for the maximum temperature and the brake energy are created using RBFN with polynomial biases. Different radial basis functions are compared using statistical errors and cross validation errors (PRESS) to evaluate the accuracy of the surrogate models and to select the most accurate radial basis function. The multi-objective optimization problem is then solved by employing EMO using the strength Pareto evolutionary algorithm (SPEA2). Finally, the Pareto fronts generated by the proposed methodology are presented and discussed."
197,"There is a need for a stronger theoretical understanding of Multidisciplinary Design Optimization (MDO) within the field. Having developed a differential geometry framework in response to this need, we consider how standard optimization algorithms can be modeled using systems of ordinary differential equations (ODEs) while also reviewing optimization algorithms which have been derived from ODE solution methods. We then use some of the framework’s tools to show how our resultant systems of ODEs can be analyzed and their behaviour quantitatively evaluated. In doing so, we demonstrate the power and scope of our differential geometry framework, we provide new tools for analyzing MDO systems and their behaviour, and we suggest hitherto neglected optimization methods which may prove particularly useful within the MDO context."
198,"Small scale turbomachines in domestic heat pumps reach high efficiency and provide oil-free solutions which improve heat-exchanger performance and offer major advantages in the design of advanced thermodynamic cycles. An appropriate turbocompressor for domestic air based heat pumps requires the ability to operate on a wide range of inlet pressure, pressure ratios and mass flows, confronting the designer with the necessity to compromise between range and efficiency. Further the design of small-scale direct driven turbomachines is a complex and interdisciplinary task. Textbook design procedures propose to split such systems into subcomponents and to design and optimize each element individually. This common procedure, however, tends to neglect the interactions between the different components leading to suboptimal solutions. The authors propose an approach based on the integrated philosophy for designing and optimizing gas bearing supported, direct driven turbocompressors for applications with challenging requirements with regards to operation range and efficiency. Using previously validated reduced order models for the different components an integrated model of the compressor is implemented and the optimum system found via multi-objective optimization. It is shown that compared to standard design procedure the integrated approach yields an increase of the seasonal compressor efficiency of more than 12 points. Further a design optimization based sensitivity analysis allows to investigate the influence of design constraints determined prior to optimization such as impeller surface roughness, rotor material and impeller force. A relaxation of these constrains yields additional room for improvement. Reduced impeller force improves efficiency due to a smaller thrust bearing mainly, whereas a lighter rotor material improves rotordynamic performance. A hydraulically smoother impeller surface improves the overall efficiency considerably by reducing aerodynamic losses. A combination of the relaxation of the 3 design constraints yields an additional improvement of 6 points compared to the original optimization process. The integrated design and optimization procedure implemented in the case of a complex design problem thus clearly shows its advantages compared to traditional design methods by allowing a truly exhaustive search for optimum solutions throughout the complete design space. It can be used for both design optimization and for design analysis."
199,"This work suggests a two-stage approach for robust optimal design of 6-DOF haptic devices based on a sequence of deterministic and probabilistic analyses with a multi-objective genetic algorithm and the Monte-Carlo method. The presented model-based design robust optimization approach consider simultaneously the kinematic, dynamic, and kinetostatic characteristics of the device in both a constant and a dexterous workspace in order to find a set of optimal design parameter values for structural configuration and dimensioning. Design evaluation is carried out based on local and global indices, like workspace volume, quasi-static torque requirements for the actuators, kinematic isotropy, dynamic isotropy, stiffness isotropy, and natural frequencies of the device. These indices were defined based on focused kinematic, dynamic, and stiffness models. A novel procedure to evaluate local indices at a singularity-free point in the dexterous workspace is presented. The deterministic optimization approach neglects the effects from variations of design variables, e.g. due to tolerances. A Monte-Carlo simulation was carried out to obtain the response variation of the design indices when independent design parameters are simultaneously regarded as uncertain variables. It has been observed that numerical evaluation of performance indices depends of the type of workspace used during optimization. To verify the effectiveness of the proposed procedure, the performance indices were evaluated and compared in constant orientation and in dexterous workspace."
200,"This paper demonstrates how solution quality for multiobjective optimization problems can be improved by altering the selection phase of a multiobjective genetic algorithm. Rather than the traditional roulette selection used in algorithms like NSGA-II, this paper adds a goal switching technique to the selection operator. Goal switching in this context represents the rotation of the selection operator among a problem’s various objective functions to increase search diversity. This rotation can be specified over a set period of generations, evaluations, CPU time, or other factors defined by the designer. This technique is tested using a set period of generations before switching occurs, with only one objective considered at a time. Two test cases are explored, the first as identified in the Congress on Evolutionary Computation (CEC) 2009 special session and the second a case study concerning the market-driven design of a MP3 player product line. These problems were chosen because the first test case’s Pareto frontier is continuous and concave while being relatively easy to find. The second Pareto frontier is more difficult to obtain and the problem’s design space is significantly more complex. Selection operators of roulette and roulette with goal switching were tested with 3 to 7 design variables for the CEC 09 problem, and 81 design variables for the MP3 player problem. Results show that goal switching improves the number of Pareto frontier points found and can also lead to improvements in hypervolume and/or mean time to convergence."
201,"During the design of complex systems, a design process may be subjected to stochastic inputs, interruptions, and changes. These design impulses can have a significant impact on the transient response and converged equilibrium for the design system. We distinguish this research by focusing on the interactions between local and architectural impulses in the form of designer mistakes and dissolution, division, and combination impulses, respectively. We find that local impulses tend to slow convergence but systems subjected to dissolution/division impulses still favor parallel arrangements. The strategy to mitigate combination impulses is unaffected by the presence of local impulses."
202,"Modular product platforms have been shown to provide substantial cost and time savings while still allowing companies to offer a variety of products. As a result, a multitude of product platform methods have been developed over the last decade within the design research community. However, comparison and integration of suitable methods is difficult since the methods have, for the most part, been developed in isolation from one another. In reviewing the literature in modularity and product platforms, we create a generic set of twelve platform design activities. We then examine a set of product platform development processes used at several different companies, and from this form a generic sequence of the activities. We then associate the various developed methods to the sequence, thereby enabling the chaining together of the various modular and platform design methods developed by the community."
203,"Product development is characterized by continuous updating of existing solutions in order to cope with new market requirements. Families of product variants are used to satisfy the needs of new potential customers and penetrate new market niches."
204,"A product family with a common platform paradigm can increase the flexibility and responsiveness of the product-manufacturing process and help take away market share from competitors that develop one product at a time. The recently developed Comprehensive Product Platform Planning (CP"
205,"A seven-step framework for sorting proposed concepts of system changes / reconfigurations is presented that seeks to characterize the overall ramifications on system architecture. This framework is intended for use immediately following a concept generation phase. The framework uses three simple questions: “What level of the system design does this concept apply to?” “What levels of the system design does the concept impact?” and “What is the severity of this impact?” A flowchart leads the designer through these questions and assigns each concept a classification from one to five based on the answers. Class one concepts have little to no impact on the rest of the system architecture. They can be included with little fear of massive change propagation and system redesign. Class five concepts carry large changes to system architecture and therefore should be included only if they can be shown to be highly beneficial, or if there remains enough design freedom such that the cost of changing the system architecture is minimal. Meanwhile, class five concepts are likely to have much higher potential to create revolutionary design. A case study is used to demonstrate the application of the sorting framework in the context of a Mars rover mission. Several example concepts are provided to illustrate key insights from the case study. Convergence of the framework is explored by comparing the authors’ results to a second test done by a new design team."
206,"To achieve “green profit” in their business, manufacturers who produce both new and remanufactured products must optimize their pricing and production decisions simultaneously. They must determine the buy-back price and take-back quantity of end-of-life products as well as the selling prices and production quantities of new and remanufactured products. With an aim to assist in optimal pricing and production planning, this paper presents a mixed-integer programming model that optimizes the three prices (of buyback, new and remanufactured products) and the corresponding production plan simultaneously. The model considers the two conflicting objectives of maximizing economic profitability and maximizing environmental impact saving. The model helps address potential barriers to remanufacturing, which include limited economic, and/or environmental sustainability of remanufacturing, imbalance between the supply of end-of-life products and the market demand for remanufactured products, and cannibalization of the sales of new products. The developed model is illustrated with an example of engine water pump."
207,"Publishing is the process of developing and producing content for distribution to the public. In the past, the publishing process heavily relies on printing as the method of content production. This causes voracious consumption and waste of natural resources. In today’s sweeping trend of digitization that is featured by the increasing popularity of various smart devices, the publishing process is undergoing a profound transition from the traditional printing-reliant publishing model to the new digital publishing model. Such a transition brings great opportunities for the publishing process to achieve better sustainability by evolving towards a product service system. This paper intends to advance the publishing process from the product service integration perspective. Above all, a general product-service integration framework is developed to describe the interdependent relationships among key stakeholders and elements in the publishing value chain. Furthermore, several specific publishing PSS are discussed. Finally, these publishing PSS are evaluated and compared from the value creation perspective."
208,"In competitive market environments, strategies that adding services to products for sales promotion are now moved to integrate products and services for satisfying diverse customer needs, and the number of these cases is gradually increasing. Trends of integrating products and services lead to the emergence of a product-service system (PSS). To implement and embody a PSS solution in new product development, a comprehensive design framework is allowed designers to facilitate the design factors of the PSS in complex business environments. The objective of this paper is to propose a PSS model to identify design factors for developing products and services by integrating object-oriented concepts and blueprinting in context of a business ecosystem. The proposed model is developed based on relationship between products and services matching with their design factors. The products and the services are then brought together to form a PSS. Functions and processes can be categorized to identify the design factors in different levels using the object-oriented concepts. Interaction between products and services lies on a PSS platform to form a product service system in blueprinting. To demonstrate of the effectiveness of the proposed model, we use a case study involving a smart phone."
209,"Although energy consumption during use can cause a majority of a product’s environmental impact, the relationship between a product’s usage context and its environmental performance is rarely considered in design evaluations. Probabilistic graphical models (PGMs) provide the capability of evaluating uncertainty and variability of product use in addition to correlating the results with aspects of the usage context. This research demonstrates a method for representing the usage context as a PGM through the use of a lightweight vehicle design example. The demonstration PGM is constructed from factors such as driver behavior, alternative driving schedules, and residential density, which are related to local conditional probability distributions derived from publicly available data sources. Unique scenarios are then assembled from sets of conditions on these factors to provide insight into sources of variance in lifetime energy use. The vehicle example demonstrates that implementation of realistic usage scenarios via a PGM can provide a much higher fidelity investigation of energy savings during use than commonly found in the literature and that distinct scenarios can have significantly different implications for the effectiveness of lightweight vehicle designs."
210,"This work is concerned with the time-dependent mechanism reliability defined over a period of time where a certain motion output is required. An envelope approach is proposed to improve the accuracy of the time-dependent mechanism reliability analysis. The envelope function of the motion error over the time period is created. Since the envelope function is not explicitly related to time, the time-dependent problem is converted into a time-independent problem. Then the envelope function is approximated by piecewise hyper-planes. To find the expansion points of the hyper-planes, the approach linearizes the motion error at the means of random dimension variables, and this approximation is accurate because the tolerances or the variances of the dimension variables are small. Then the expansion points are found with the maximum probability density at the failure threshold. The time-dependent mechanism reliability is then estimated by a multivariable normal distribution function at the expansion points. As an example, analytical equations are derived for a four-bar function generating mechanism. The numerical example shows the significant accuracy improvement."
211,"Fatigue damage analysis is critical for systems under stochastic loadings. To estimate the fatigue reliability at the design level, a hybrid reliability analysis method is proposed in this work. The First Order Reliability Method (FORM), the inverse FORM, and the peak distribution analysis are integrated for the fatigue reliability analysis at the early design stage. Equations for the mean value, the zero upcrossing rate, and the extreme stress distributions are derived for problems where stationary stochastic processes are involved. Then the fatigue damage is analyzed with the peak counting method. The developed methodology is demonstrated by a simple mathematical example and is then applied to the fatigue reliability analysis of a shaft under stochastic loadings. The results indicate the effectiveness of the proposed method in predicting fatigue damage and reliability."
212,"Inverse simulation is an inverse process of direct simulation. It determines unknown input variables of the direct simulation for a given set of simulation output variables. Uncertainties usually exist, making it difficult to solve inverse simulation problems. The objective of this research is to account for uncertainties in inverse simulation in order to produce high confidence in simulation results. The major approach is the use of the maximum likelihood methodology, which determines not only unknown deterministic input variables but also the realizations of random input variables. Both types of variables are solved on the condition that the joint probability density of all the random variables is maximum. The proposed methodology is applied to a traffic accident reconstruction problem where the simulation output (accident consequences) is known and the simulation input (velocities of the vehicle at the beginning of crash) is sought."
213,"Traditionally gears are designed using design standards such as AGMA, ISO, etc. These design standards include a large number of “design factors” accounting for various uncertainties related to geometry, load and material uncertainties. As the knowledge about these uncertainties increases, it becomes possible to include them systematically in the gear design procedure, thereby reducing the number of empirical design factors. In this paper a method is proposed to eliminate two design factors (viz., factor of safety in contact and reliability factor) used in standard AGMA-based design procedures through the formal introduction of uncertainty in the magnitude of load and material properties. The proposed method is illustrated via the design of an automotive gear with a desired reliability, cost, and robustness. The solutions obtained are encouraging and in-line with the existing knowledge about gear design, and thus reinforces the possibility of schematically reducing the aforementioned design factors."
214,"Uncertainty is inevitable in real world. It has to be taken into consideration, especially in engineering optimization; otherwise the obtained optimal solution may become infeasible. Robust optimization (RO) approaches have been proposed to deal with this issue. Most existing RO algorithms use double-looped structures in which a large amount of computational efforts have been spent in the inner loop optimization to determine the robustness of candidate solutions. In this paper, an advanced approach is presented where no optimization run is required to be performed for robustness evaluations in the inner loop. Instead, a concept of Utopian point is proposed and the corresponding maximum variable/parameter variation will be obtained by just solving a set of linear equations. The obtained robust optimal solution from the new approach may be conservative, but the deviation from the true robust optimal solution is very small given the significant improvement in the computational efficiency. Six numerical and engineering examples are tested to show the applicability and efficiency of the proposed approach, whose solutions and computational time are compared with those from a similar but double-looped approach, SQP-RO, proposed previously."
215,"Time-dependent reliability is the probability that a system will perform its intended function successfully for a specified time. Unless many and often unrealistic assumptions are made, the accuracy and efficiency of time-dependent reliability estimation are major issues which may limit its practicality. Monte Carlo simulation (MCS) is accurate and easy to use but it is computationally prohibitive for high dimensional, long duration, time-dependent (dynamic) systems with a low failure probability. This work addresses systems with random parameters excited by stochastic processes. Their response is calculated by time integrating a set of differential equations at discrete times. The limit state functions are therefore, explicit in time and depend on time-invariant random variables and time-dependent stochastic processes. We present an improved subset simulation with splitting approach by partitioning the original high dimensional random process into a series of correlated, short duration, low dimensional random processes. Subset simulation reduces the computational cost by introducing appropriate intermediate failure sub-domains to express the low failure probability as a product of larger conditional failure probabilities. Splitting is an efficient sampling method to estimate the conditional probabilities. The proposed subset simulation with splitting not only estimates the time-dependent probability of failure at a given time but also estimates the cumulative distribution function up to that time with approximately the same cost. A vibration example involving a vehicle on a stochastic road demonstrates the advantages of the proposed approach."
216,"We have recently proposed a new method for combined design optimization and calibration-based validation using a sequential approach with variable-size local domains of the design space and statistical bootstrap techniques. Our work was motivated by the fact that model validation in the entire design space may be neither affordable nor necessary. The method proceeds iteratively by obtaining test data at a design point, constructing around it a local domain in which the model is considered valid, and optimizing the design within this local domain. Due to test variability, it is important to know how many tests are needed to size each local domain of the sequential optimization process. Conducting an unnecessarily large number of tests may be inefficient, while a small number of tests may be insufficient to achieve the desired validity level. In this paper, we introduce a technique to determine the number of tests required to account for their variability by sizing the local domains accordingly. The goal is to achieve a desired level of model validation in each domain using the correlation between model data at the center and any other point in the local domain. The proposed technique is illustrated by means of a piston design example."
217,"The definition of reliability may not be readily applicable for repairable systems. Our recent work has shown that multiple metrics are needed to fully account for the performance of a repairable system under uncertainty. Optimal tradeoffs among a minimal set of metrics can be used in the design and maintenance of these systems. A minimal set of metrics provides the most information about the system with the smallest number of metrics using a set of desirable properties. Critical installations such as a remote microgrid powering a military installation require a careful consideration of cost and repair strategies. This is because of logistical challenges in performing repairs and supplying necessary spare parts, particularly in unsafe locations. This paper shows how a minimal set of metrics enhances decision making in such a scenario. It enables optimal tradeoffs between critical attributes in decision making, while guaranteeing that all important performance measures are satisfied. As a result, cost targets and inventory planning can be achieved in an optimal way. We demonstrate the value of the proposed approach using a US Army smart-charging microgrid installation."
218,"In physics-based engineering modeling and uncertainty quantification, distinguishing the effects of two main sources of uncertainty — calibration parameter uncertainty and model discrepancy — is challenging. Previous research has shown that identifiability can sometimes be improved by experimentally measuring multiple responses of the system that share a mutual dependence on a common set of calibration parameters. In this paper, we address the issue of how to select the most appropriate subset of responses to measure experimentally, to best enhance identifiability. We propose a preposterior analysis approach that, prior to conducting the physical experiments but after conducting computer simulations, can predict the degree of identifiability that will result using different subsets of responses to measure experimentally. We quantify identifiability via the posterior covariance of the calibration parameters, and predict it via the preposterior covariance from a modular Bayesian Monte Carlo analysis of a multi-response Gaussian process model. The proposed method is applied to a simply supported beam example to select two out of six responses to best improve identifiability. The estimated preposterior covariance is compared to the actual posterior covariance to demonstrate the effectiveness of the method."
219,"This paper presents a maximum confidence enhancement based sequential sampling approach for simulation-based design under uncertainty. In the proposed approach, the ordinary Kriging method is adopted to construct surrogate models for all constraints and thus Monte Carlo simulation (MCS) is able to be used to estimate reliability and its sensitivity with respect to design variables. A cumulative confidence level is defined to quantify the accuracy of reliability estimation using MCS based on the Kriging models. To improve the efficiency of proposed approach, a maximum confidence enhancement based sequential sampling scheme is developed to update the Kriging models based on the maximum improvement of the defined cumulative confidence level, in which a sample that produces the largest improvement of the cumulative confidence level is selected to update the surrogate models. Moreover, a new design sensitivity estimation approach based upon constructed Kriging models is developed to estimate the reliability sensitivity information with respect to design variables without incurring any extra function evaluations. This enables to compute smooth sensitivity values and thus greatly enhances the efficiency and robustness of the design optimization process. Two case studies are used to demonstrate the proposed methodology."
220,"Stochastic differential equation (SDE) and Fokker-Planck equation (FPE) are two general approaches to describe the stochastic drift-diffusion processes. Solving SDEs relies on the Monte Carlo samplings of individual system trajectory, whereas FPEs describe the time evolution of overall distributions via path integral alike methods. The large state space and required small step size are the major challenges of computational efficiency in solving FPE numerically. In this paper, a generic continuous-time quantum walk formulation is developed to simulate stochastic diffusion processes. Stochastic diffusion in one-dimensional state space is modeled as the dynamics of an imaginary-time quantum system. The proposed quantum computational approach also drastically accelerates the path integrals with large step sizes. The new approach is compared with the traditional path integral method and the Monte Carlo trajectory sampling."
221,"This study presents a reliability analysis of vehicle sideslip and rollover in highway horizontal curves, mainly focusing on exit ramps and interchanges. To accurately describe failure modes of a ground vehicle, analytic models for sideslip and rollover are derived considering nonlinear characteristics of vehicle behavior using the commercial software, TruckSim"
222,"This paper proposes a methodology for sampling-based design optimization in the presence of interval variables. Assuming that an accurate surrogate model is available, the proposed method first searches the worst combination of interval variables for constraints when only interval variables are present or for probabilistic constraints when both interval and random variables are present. Due to the fact that the worst combination of interval variables for probability of failure does not always coincide with that for a performance function, the proposed method directly uses the probability of failure to obtain the worst combination of interval variables when both interval and random variables are present. To calculate sensitivities of constraints and probabilistic constraints with respect to interval variables by the sampling-based method, the behavior of interval variables at the worst case is defined by utilizing the Dirac delta function. Then, Monte Carlo simulation is applied to calculate constraints and probabilistic constraints with the worst combination of interval variables, and their sensitivities. The important merit of the proposed method is that it does not require gradients of performance functions and transformation from X-space to U-space for reliability analysis after the worst combination of interval variables is obtained, thus there is no approximation or restriction in calculating the sensitivities of constraints or probabilistic constraints. Numerical results indicate that the proposed method can search the worst case probability of failure with both efficiency and accuracy and that it can perform design optimization with mixture of random and interval variables by utilizing the worst case probability of failure search."
223,"As a metamodeling method, Kriging has been intensively developed for deterministic design in the past few decades. However, Kriging is not able to deal with the uncertainty of many engineering processes. By incorporating the uncertainty of data, Stochastic Kriging methods has been developed to analyze and predict random simulation results, but the results cannot fit the problem with uncertainty well. In this paper, deterministic Kriging are extended to stochastic space theoretically, where a novel form of Stochastic Kriging that fully considers the intrinsic uncertainty of data and number of replications is proposed on the basis of finite inputs. It formulates a more reasonable optimization problem via a stochastic process, and then derives the spatial correlation models underlying a random simulation. The obtained results are more general than Kriging, which can fit well with many uncertainty-based problems. Three examples will illustrate the method’s application through comparison with the existing methods: the novel method shows that the results are much closer to reality."
224,"User productivity is a key component of an integrated sustainable building design framework, as a consideration in addition to operating and construction costs associated with sustainable building practices. Research has shown that employee productivity increases as visible light levels rise in the workplace. Incorporating efficient lighting systems into sustainable building techniques can potentially increase user productivity while reducing electricity costs. This paper presents a single criterion approach that captures the trade-offs between costs, users, and current building standards. A model has been created to explore the feasible design space of a commercial workspace by populating a repository of both active and passive lighting components that can be accessed to generate various lighting designs. A genetic algorithm is used to optimize potential lighting choices in a given workspace, and allows the designer to explore Pareto optimal productivity solutions that best fit the desired application. The result is an optimal solution for a given workspace lighting configuration that captures user productivity while minimizing operating costs."
225,"The present work introduces a new Finite Element methodology that can be used directly in calculating and optimizing the thickness of a mechanical structure. This method depends on the constant strain triangle model after assuming different thicknesses at the elements nodes. A linear relation between the nodes thicknesses is assumed and a new stiffness matrix is created. Nodes thicknesses are optimized using the developed HGP method to reach uniform stress among the structure to satisfy the constrained allowable stress designated by the designer. Examples and sample applications are employed for comparisons and their results culminate in removing unnecessary elements and increasing the thickness, which is subjected to high stresses. Results indicate marked improvements and potential for topology optimization."
226,"This paper presents a multi-objective design optimization study of a vehicle suspension system with passive variable stiffness and active damping. Design of suspension systems is particularly challenging when the effective mass of the vehicle is subject to considerable variation during service. Perfectly maintaining the suspension performance under the variable load typically requires a controlled actuator in order to emulate variable stiffness. This is typically done through a hydraulic or pneumatic system, which can be too costly for small/medium pickup trucks. The system in this paper employs two springs with an offset to the second spring so that it engages during large deformation only, thereby providing passive variable stiffness without expensive hydraulics. The system damping is assumed to be controlled via variable viscosity magnetizable fluid, which can be implemented in a compact, low-power setup. Simulation of the suspension system is performed by numerically solving the nonlinear equations of motion for a quarter-vehicle mode subject to excitation from a road profile over a set period of time. A performance index from the literature is evaluated for the suspension system for the cases of minimum and maximum weight, and the two indices values are regarded as objectives in a multi-objective problem. As the individual objectives are prone to having local optima, the multi-objective problem is prone to having a disjointed Pareto-space. To deal with this issue, a modification is proposed to a multi-objective genetic algorithm. The algorithm performance is investigated via analytical test functions as well as a design case of the suspension system. Results show a reduction in the system’s spring size, with the Pareto point obtained from the proposed diffusion model, without compromising the system’s performance."
227,"In this paper, a generalized approach is developed to optimize column configuration subjected to buckling load. The configuration utilizes B-spline contour to provide more freedom to model the column shape. Previous columns in literature use tapered or parabolic tapered for configuration. This work considers hinged-hinged columns of circular solid cross-sectional area. Two sample applications are optimized using Genetic Algorithm with the finite difference method to satisfy the buckling constraints. The length and load are fixed. The objective is to minimize the volume considering the cross-sectional diameters as the design variables. B-Spline quadratic with three and five control points and cubic with five control points are applied. The proposed configuration is compared with tapered and parabolic tapered columns. Results show that continuity provides a better optimum against column buckling than other tapered columns. Even though volume is more than some configurations by about 1.67%, but those configurations would not satisfy buckling constraints over the entire length of the column."
228,"Military engagements are continuing the movement toward automated and unmanned vehicles for a variety of military tasks. One important piece of automating is the ability to perform path planning quickly, safely, and reliably for unmanned aerial systems (UASs). Path planners often ignore the UAS’s ability to perform the maneuvers necessary to fly through the specified waypoints, instead relying on them to fly as close as possible. To date, path planning algorithms have been limited to providing only a single solution without input from an operator. This project attempts to incorporate operator experience in the path planning process. Particle Swarm Optimization (PSO) allows the generation of multiple optimized three-dimensional flight paths for the operator to choose from. This paper improves upon a previously developed method by incorporating flight mechanics equations into the optimization problem formulation. The new problem formulation ensures that the returned flight paths do not violate maximum load factor (G-force), minimum velocity (stall velocity), or the minimum turning radius."
229,"This paper presents a framework for simulation-based design optimization of computationally-expensive problems, where economizing the generation of sample designs is highly desirable. Various meta-modeling schemes are used in practice in order to approximate the input-output relationships in the designed system and suggest candidate locations in the design space where high quality designs are likely to be found. One such popular approach is known as "
230,"The non-probabilistic-based structural design optimization problems with external load uncertainties are often solved through a two-level approach. However there are several challenges in this method. Firstly, to assure the reliability of the design, the lower level problem must be solved to its global optimality. Secondly, the sensitivity of the upper level problem cannot be analytically derived. To overcome these challenges, a new method based on the Eigenvalue-Superposition of Convex Models (ESCM) is proposed in this paper. The ESCM method replaces the global optimum of the lower level problem by a confidence bound, namely the ESCM bound, and with which the two-level problem can be formulated into a single level problem. The advantages of the ESCM method in efficiency and stability are demonstrated through numerical examples."
231,"In this paper, a new graph grammar representation is proposed to reason about the manufacturability of solid models. The knowledge captured in the graph grammar rules serves as a virtual machinist in its ability to recognize arbitrary geometries and match them to various machine operations. Firstly, a given part is decomposed into multiple sub-volumes, where each sub-volume is assumed to be machined in one operation or to be non-machinable. The decomposed part is converted into a graph so that graph grammar rules can determine the machining details. For each operation, rules determine the face on the part that the tool enters, the type of tools used, the type of machine used, and how the part is fixed within the machine. A candidate plan is a feasible sequence of all of the necessary machining operations needed to manufacture this part. If a given geometry is not machinable, the rules will fail to find operations for all of the partitions."
232,"Active learning refers to the mechanism of querying users to accomplish a classification task in machine learning or a conjoint analysis in econometrics with minimum cost. Classification and conjoint analysis have been introduced to design research to automate design feasibility checking and to construct marketing demand models, respectively. In this paper, we review active learning algorithms from computer and marketing science, and establish the mathematical commonality between the two approaches. We compare empirically the performance of active learning and static D-optimal design on simulated classification and conjoint analysis test problems with labelling noise. Results show that active learning outperforms D-optimal design when query size is large or noise is small."
233,"Assembly time estimation is traditionally a time intensive manual process requiring detailed geometric and process information to be available to a human designer. As a result of these factors, assembly time estimation is rarely applied during early design iterations. This paper explores the possibility that the assembly time estimation process can be automated while reducing the level of design detail required. The approach presented here trains artificial neural networks (ANNs) to estimate the assembly times of vehicle sub-assemblies at various stages using properties of the connectivity graph at that point as input data. Effectiveness of estimation is evaluated based on the distribution of estimates provided by a population of ANNs trained on the same input data using varying initial conditions. Results suggest that the method presented here can complete the time estimation of an assembly process with +/− 15% error given an initial sample of manually estimated times for the given sub-assembly."
234,"Requirement change propagation, the process in which a change to one requirement results in additional requirement changes when otherwise this change would not have been needed, occurs frequently and must be managed. Multiple approaches exist, and have been readily published, for predicting requirement change propagation, analyzing change how a change to one requirement may propagate forward to other, related requirements (global level). However, the type of change encountered within a single requirement (localized level) has not been thoroughly studied and could be used to assist in the global analysis of requirement change propagation. This paper seeks to begin to fill this gap by identifying types of change requirements may encounter. By surveying research performed in the realm of requirement change, a taxonomy of change types is developed. To computationally analyze the changes, the localized requirement changes are represented through syntactical elements to identify which requirements’ parts of speech is affected. Using part of speech language rules, the identification of requirement change type is automatically identified. Further, the automatic identification of requirement change type is used to assist in predicting change propagation, a process currently automated. This bridges the gap between localized and global requirement change in an automated, systematic manner."
235,"Soliciting and expressing the preferences of a decision maker in engineering design is critical. In general, the preferences vary through time, complicating the design of engineering systems. In this article, we propose that if parameterized utility functions are used to model the preferences, the time-varying characteristics of the parameters can provide valuable information on the likely decisions the decision maker can make at a future time. To model the time-dependent uncertainty in preferences, we use parameterized utility functions with the parameters characterized by stochastic processes and demonstrate how the design process is affected by stationarity properties of the random parameters. We work in the normative utility theoretic domain and show a property of the multiplicative utility function that allows us to use the common Black-Scholes-Merton options pricing model from finance, to account for variability in preferences with time. Finally, we discuss how to modify the design process so that optimal products are ready when there is a future need for them. The applicability of our approach is demonstrated using a cell phone example."
236,"Optimization is needed for effective decision based design (DBD). However, a utility function assessed a priori in DBD does not usually capture the preferences of the decision maker over the entire design space. As a result, when the optimizer searches for the optimal design, it traverses (or ends up) in regions where the preference order among different solutions is different from the actual order. For a highly non-convex design space, this can lead to convergence to a grossly suboptimal design depending on the initial design. In this article, we propose two approaches to alleviate this issue. First, we map the trajectory of the solution as generated by the optimizer and generate ranking questions that are presented to the designer to verify the correctness of the utility function. We then propose backtracking rules if a local utility function is very different from the initially assessed function. We demonstrate our methodology using a mathematical example and a welded beam design problem."
237,"Management of product design projects becomes increasingly difficult as the complexity of products increases. For better management of such projects, well-considered preliminary coordination of design processes is essential. This paper proposes a method for coordination in the design process, which comprises two phases: 1) division of the design work into smaller tasks and sequencing them and 2) establishment of management activities. To facilitate this coordination, an integrated model of a product, process, and organization is proposed."
238,"In this work, safety of motorcycle helmet design is investigated by using standard oblique impact test method. First, testing procedure is explained and test rig mechanism is introduced. Next, standard impact tests are performed on helmets. Data are collected using a tri-axial linear accelerometer embedded inside the headform and a high speed camera for measuring rotational acceleration. Then, results are studied and compared to injurious limit for human head injury. It is shown that during an oblique impact rotational acceleration can easily surpass the safe limit while the linear acceleration is well below the safe limit."
239,"Dual-crane lifting has been generally used with the need of erection and installation of large equipment. Choosing proper locations for two mobile cranes is an important work as well as a difficulty in design of heavy lifting plan. So, this paper proposes an optimization method of location for cooperative lifting of dual-crane. This approach starts with determining search field and maximum step length, and then finds out load’s possible initial locations by bisection method; secondly calculates work envelope of dual-crane which will be dispersed to find all dual-crane’s possible locations; after this, it puts cranes in their respective possible locations corresponding to each load’s location and simulates the whole lifting course while carries out collision detection to exclude crash happened locations; finally, creates weighted optimization function, uses enumeration method to traverse all collision-free locations, and gains optimal location of dual-crane and load. At last, this approach has been used in an actual engineering case and optimal location is got, from which we can see its feasibility and validity."
240,"Developing multiple-generation of products has become a mainstream tactic in today’s markets. The most notable case is Apple Inc.’s huge success with its iPod, iPhone and iPad product lines. Multiple-generation product lines require carefully planned strategies. Under a multiple-generation product development strategy, companies introduce a line of products to the market instead of introducing a single product to better utilize technology assets and resources in an elongated time span. For such product development and launch scenarios, cannibalization can occur, however. That is, multiple product generations compete in the same market and partition the company’s market shares. In the paper, we propose a new framework to predict the sales and introduction timing for every product generation in a multiple-generation product line while considering cannibalization. We demonstrate a case study implementing the proposed framework on Apple Inc.’s iPhone product line. The results show that the forecast performance of the model matches the realized real data."
241,"Model development decisions are critical in the early phases of engineering design. Engineering models serve as representations of reality that help designers understand input/output relationships, answer ‘what-if’ questions, and find optimal design solutions. Upon making model development decisions, the designer commits a large percentage of the costs associated with reaching design goals/objectives. The decisions dictate cost-drivers such as experimental setups and computation time. Unfortunately, the desire to develop the most accurate model competes with the desire to reduce costs. The designer is ultimately required to make trade-offs between attributes when choosing the best model development decision. Hence it is critical to develop tools for selecting the model development decision that appropriately balances trade-offs. A framework is proposed for model development decision-making. Conjoint Analysis (CA) is implemented in order to handle trade-offs among attributes. Thus, the framework can be used to make optimal decisions based on the assessment of multiple attributes. Moreover, the framework addresses the uncertainty that exists early in model design. Imprecision in model parameters are estimated and propagated through the model. In particular, the proposed decision framework is employed to select the optimal model development decision with respect to the final phase of experimentation. Preference intervals are evaluated in order to choose which final experimentation to perform. The decision framework proves to be useful for making model development decisions under uncertainty by considering the preference of multiple attributes and the imprecision of said attributes that is prevalent in early model development phases."
242,"Reliable and economical fabrication of metallic parts with complicated geometries is of considerable interest for the aerospace, medical, automotive, tooling and consumer products industries. In an effort to shorten the time-to-market, decrease the manufacturing process chain, and cut production costs of products produced by these industries, research has focused on the integration of multiple unit manufacturing processes into one machine. The end goal is to reduce production space, time, and manpower requirements."
243,"Efficient and accurate analysis of materials behavior across multiple scales is critically important in designing complex materials systems with exceptional performance. For heterogeneous materials, apparent properties are typically computed by averaging stress-strain behavior in a statistically representative cell. To be statistically representative, such cells must be larger and are often computationally intractable, especially with standard computing resources. In this research, a stochastic reassembly approach is proposed for managing the information complexity and reducing the computational burden, while maintaining accuracy, of apparent property prediction of heterogeneous materials. The approach relies on a hierarchical decomposition strategy that carries the materials analyses at two levels, the RVE (representative volume element) level and the SVE (statistical volume element) level. The hierarchical decomposition process uses clustering methods to group SVEs with similar microstructure features. The stochastic reassembly process then uses t-testing to minimize the number of SVEs to garner their own apparent properties and fits a random field model to high-dimensional properties to be put back into the RVE. The RVE thus becomes a coarse representation, or “mosaic,” of itself. Such a mosaic approach maintains sufficient microstructure detail to accurately predict the macro-property but becomes far cheaper from a computational standpoint. A nice feature of the approach is that the stochastic reassembly process naturally creates an apparent-SVE property database. Thus, material design studies may be undertaken with SVE-apparent properties as the building blocks of a new material’s mosaic. Some simple examples of possible designs are shown. The approach is demonstrated on polymer nanocomposites."
244,"There has been a significant amount of research investigating the design task of concept analysis, and much research on reconfigurable system design. Despite previous efforts, further research is still needed that explores how concept analysis should best be conducted for reconfigurable systems. Because reconfigurable systems have multiple configurations and different performance levels, additional information is required to understand each concept, making the design selection process more demanding. Desirable functions, concepts for each phase, and concepts for transition methods between configurations all need to be evaluated. In this paper, the fundamental challenges of reconfigurable system analysis are identified. A framework is then developed to evaluate the many options a designer may face when performing the concept analysis phase of reconfigurable system design, identifying the influence of reconfigurable systems on the process. A mini unmanned aerial vehicle case study is used to demonstrate how the framework is applied, and how existing concept analysis tools can be adapted to account for additional criteria. The paper concludes with a review of the contributions of this work and identifies areas of future work."
245,"Reconfigurable structures that are enabled through the integration of multiple materials are important for future design and manufacturing practice. We investigate one of such reconfigurable structures — an origami sheet, which can be designed based on a 3D object and unfolded into a 2D sheet with complex creases. A fabrication approach based on a hybrid manufacturing process by integrating layer-based additive manufacturing and silicon molding techniques is developed. Related challenges on designing creases for given folding requirements and the related material properties are discussed. A novel structure design is presented to ensure the fabricated creases that are in soft materials can be folded and unfolded without failures. The design method can be applied to different scale levels. The origami sheets for test cases in different complexity have been tested. The experimental results illustrate that the designed and fabricated origami sheets can be folded and used for product components with reconfigurable shapes."
246,"The aim of designing the wind turbine blades is to improve the power capture ability. Since the rotor control technology is currently limited to controlling the rotor rotational speed and the pitch of the blades, an increasing concern has been given to the morphing blades. In this paper, a simplified morphing blade is introduced, which has a linear twisted distribution along the span and its shape can be controlled by adjusting the root twisted angle and the tip twisted angle of the blade. Moreover, to evaluate the performances of the wind turbine blades, a numerical code based on the blade element momentum theory is developed and validated. The blade of the NREL Phase VI wind turbine is taken as a reference blade, and the optimization problems associated with the morphing blade and pitch control blade are both formulated. The optimal results show that the morphing blade gives better results than the pitch control blade in terms of produced power. Under the assumption that in a given site, the annual average wind speed is known and the wind speed follows the Rayleigh distribution, we can evaluate the annual energy produced by these three blade types. While the annual average wind speed varies from 5 m/s to 15 m/s, the results show that the optimal morphing blade can increase 23.9 percent to 71.4 percent in annual energy production while the optimal pitch control blade can increase 22.5 percent to 67.4 percent in annual energy production, over the existing twisted pitch fixed blade."
247,"This paper presents a multi-level Extended Pattern Search algorithm (EPS) to optimize both the local positioning and geometry of wind turbines on a wind farm. Additionally, this work begins to draw attention to the effects of atmospheric stability on wind farm power development. The wind farm layout optimization problem involves optimizing the local position and size of wind turbines such that the aerodynamic effects of upstream turbines are reduced, thereby increasing the effective wind speed at each turbine, allowing it to develop more power. The extended pattern search, employed within a multi-agent system architecture, uses a deterministic approach with stochastic extensions to avoid local minima and converge on superior solutions compared to other algorithms. The EPS presented herein is used in an iterative, hierarchical scheme — an overarching pattern search determines individual turbine positioning, then a sub-level EPS determines the optimal hub height and rotor for each turbine, and the entire search is iterated. This work also explores the wind shear profile shape to better estimate the effects of changes in the atmosphere, specifically the changes in wind speed with respect to height on the total power development of the farm. This consideration shows how even slight changes in time of day, hub height, and farm location can impact the resulting power. The objective function used in this work is the maximization of profit. The farm installation cost is estimated using a data surface derived from the National Renewable Energy Laboratory (NREL) JEDI wind model. Two wind cases are considered: a test case utilizing constant wind speed and unidirectional wind, and a more realistic wind case that considers three discrete wind speeds and varying wind directions, each of which is represented by a fraction of occurrence. Resulting layouts indicate the effects of more accurate cost and power modeling, partial wake interaction, as well as the differences attributed to including and neglecting the effects of atmospheric stability on the wind shear profile shape."
248,"The focus of this paper is on policy design problems related to large scale complex systems such as the decentralized energy infrastructure. In such systems, the policy affects the technical decisions made by stakeholders (e.g., energy producers), and the stakeholders are coordinated by market mechanisms. The decentralized decisions of the stakeholders affect the sustainability of the overall system. Hence, appropriate design of policies is an important aspect of achieving sustainability. The state-of-the-art computational approach to policy design problem is to model them as bilevel programs, specifically mathematical programs with equilibrium constraints. However, this approach is limited to single-objective policy design problems and is based on the assumption that the policy designer has complete information of the stakeholders’ preferences. In this paper, we take a step towards addressing these two limitations. We present a formulation based on the integration of multi-objective mathematical programs with equilibrium constraints with games with vector payoffs, and Nash equilibra of games with incomplete preferences. The formulation, along with a simple solution approach, is presented using an illustrative example from the design of feed-in-tariff (FIT) policy with two stakeholders. The contributions of this paper include a mathematical formulation of the FIT policy, the extension of computational policy design problems to multiple objectives, and the consideration of incomplete preferences of stakeholders."
249,"In this paper we describe the development of an interactive visualization tool to support the design and evaluation of microgrid architectures in ultra low energy communities. The work is motivated by recent Department of Defense regulations to reduce energy costs at and increase energy conservation at military installations. Using two sets of energy analysis models derived from existing energy modeling software packages, we illustrate how such a design environment can be used to (1) run a fast, low fidelity model to support an initial trade space exploration, (2) understand key trends and relationships, (3) filter microgrid architectures based on desired constraints, (4) identify architectures of interest, (5) run high fidelity analyses for architectures of interest, and (6) select an architecture and use a map view to change device type locations. The process is demonstrated through a web-based design environment that we prototyped and applied to two design examples. In both cases, promising microgrid architectures are identified from an initial set of 500 randomly generated designs. Manual adjustments of the position and location of the device types were used to further improve system performance. The end result in each case was a microgrid architecture that offered low fixed and operating costs based on the assumed electrical and thermal loads. The prototype effectively illustrates how Visual Analysis might be performed during Steps 4 & 5 of the Army’s Real Property Master Planning Process. Future enhancements to support the design decision-making process are also discussed."
250,"Fresh water availability is essential for the economic growth and development, especially in small and semi-isolated communities. In some of these communities fresh water may be scarce, yet brackish water from wells or seawater is often abundantly available. This motivates a need for cost-effective desalination at small scale capacity driven by renewable energy sources. This paper presents an integrated optimization model of a solar powered humidification-dehumidification (HDH) desalination system. The system under investigation is a water-heated system. The design variables include the sizing of solar collectors, storage tank, humidifier and dehumidifier, as well as air circulation flow rate and operating temperature. The objective of the optimization is to minimize the unit cost of the produced fresh water. Thermodynamic performance prediction is done by solving energy and mass balance equations for each of the system components, with consideration to hourly-varying solar irradiance that corresponds to a typical one year cycle. System cost is predicted via first-order estimators. A genetic algorithm is used to obtain the designs optimized for local climate and market. A case study discusses a desalination plant on the Red Sea near the city of Hurgada."
251,"Advances in high performance sensing and signal processing technology enable the development of failure prognosis tools for wind turbines to detect, diagnose, and predict the system-wide effects of failure events. Although prognostics can provide valuable information for proactive actions in preventing system failures, the benefits have not been fully utilized for the operation and maintenance decision making of wind turbines. This paper presents a generic failure prognosis informed decision making tool for wind farm operation and maintenance while considering the predictive failure information of individual turbine and its uncertainty. In the presented approach, the probabilistic damage growth model is used to characterize individual wind turbine performance degradation and failure prognostics, whereas the economic loss measured by monetary values and environmental performance measured by unified carbon credits are considered in the decision making process. Based on the customized wind farm information inputs, the developed decision making methodology can be used to identify optimum and robust strategies for wind farm operation and maintenance in order to maximize the economic and environmental benefits concurrently. The efficacy of proposed prognosis informed maintenance strategy is compared with the condition based maintenance strategy and demonstrated with the case study."
252,"In a country like France, electricity consumption devoted to domestic lighting represents nearly a fifth of the total energy consumption of a building. The use of electric lighting is influenced by several factors such as the building’s structural characteristics, the activities of its occupants, the lighting equipments, and the level of natural light. Designers do take into account, in their energy models, the influence of occupants on the building’s overall energy consumption. However, these models still have some drawbacks regarding the comprehension of real “occupants’ energy behaviors” which play an important role in the discrepancies between predicted and real energy consumptions. The behavioral factors behind occupants’ usage trends of energy are still not thoroughly explored. Therefore, it is assumed that a better comprehension of these behaviors and consumption mechanisms could lead to the identification of technical solutions and energy saving potentials, thus resulting in a more robust building design."
253,"The U.S. Department of Defense (DoD) has recently shown an interest in incorporating resource efficiency into decision-making processes, including decisions that pertain to Forward Operating Military Base Camp (FOB) equipment. Often deployed in environments without access to grid utilities, FOBs require costly deliveries by land or air of resources such as fuel and fresh water. Resource-efficient FOB designs have the potential to reduce supply costs, but competing objectives and uncertain operational conditions complicate the design process. For example, integration of solar photovoltaic panels into existing designs has the potential to reduce the need to burn fuel in generators, however solar panels have up-front logistical and monetary costs that limit widespread use. There are also uncertainties associated with available solar energy and camp electrical loads. The research described here uses computer modeling and simulation of a real FOB subsystem under different operational scenarios to develop configurations of solar panels and batteries that, when integrated with an existing FOB design, maximize resource savings but minimize logistical and monetary costs, showing the benefit of a holistic design strategy that accounts for scenario variation. This research will also show that while different hardware configurations prove most efficient under different scenarios and objectives, certain hardware configurations provide good performance under all scenarios and objectives."
254,"Wind farm design deals with the optimal placement of turbines in a wind farm. Past studies have focused on energy-maximization, cost-minimization or revenue-maximization objectives. As land is more extensively exploited for onshore wind farms, wind farms are more likely to be in close proximity with human dwellings. Therefore governments, developers, and landowners have to be aware of wind farms’ environmental impacts. After considering land constraints due to environmental features, noise generation remains the main environmental/health concern for wind farm design. Therefore, noise generation is sometimes included in optimization models as a constraint. Here we present continuous-location models for layout optimization that take noise and energy as objective functions, in order to fully characterize the design and performance spaces of the optimal wind farm layout problem. Based on Jensen’s wake model and ISO-9613-2 noise calculations, we used single- and multi-objective genetic algorithms (NSGA-II) to solve the optimization problem. Preliminary results from the bi-objective optimization model illustrate the trade-off between energy generation and noise production by identifying several key parts of Pareto frontiers. In addition, comparison of single-objective noise and energy optimization models show that the turbine layouts and the inter-turbine distance distributions are different when considering these objectives individually. The relevance of these results for wind farm layout designers is explored."
255,"Prognostics aims at determining whether a failure of an engineered system (e.g., a nuclear power plant) is impending and estimating the remaining useful life (RUL) before the failure occurs. The traditional data-driven prognostic approach involves the following three steps: (Step 1) construct multiple candidate algorithms using a training data set; (Step 2) evaluate their respective performance using a testing data set; and (Step 3) select the one with the best performance while discarding all the others. There are three main challenges in the traditional data-driven prognostic approach: (i) lack of robustness in the selected standalone algorithm; (ii) waste of the resources for constructing the algorithms that are discarded; and (iii) demand for the testing data in addition to the training data. To address these challenges, this paper proposes an ensemble approach for data-driven prognostics. This approach combines multiple member algorithms with a weighted-sum formulation where the weights are estimated by using one of the three weighting schemes, namely the accuracy-based weighting, diversity-based weighting and optimization-based weighting. In order to estimate the prediction error required by the accuracy- and optimization-based weighting schemes, we propose the use of the "
256,"System health diagnostics provides diversified benefits such as improved safety, improved reliability and reduced costs for the operation and maintenance of engineered systems. Successful health diagnostics requires the knowledge of system failures. However, with an increasing complexity it is extraordinarily difficult to have a well-tested system so that all potential faulty states can be realized and studied at product testing stage. Thus, real time health diagnostics requires automatic detection of unexampled faulty states through the sensory signals to avoid sudden catastrophic system failures. This paper presents a hybrid inference approach (HIA) for structural health diagnosis with unexampled faulty states, which employs a two-fold inference process comprising of preliminary statistical learning based anomaly detection and artificial intelligence based health state classification for real time condition monitoring. The HIA is able to identify and isolate the unexampled faulty states through interactively detecting the deviation of sensory data from the known health states and forming new health states autonomously. The proposed approach takes the advantages of both statistical approaches and artificial intelligence based techniques and integrates them together in a unified diagnosis framework. The performance of proposed HIA is demonstrated with a power transformer and roller bearing health diagnosis case studies, where Mahalanobis distance serves as a representative statistical inference approach."
257,"The success of health prognostics of engineering systems will allow engineers to shift the traditional breakdown and time based maintenance to the state-of-art predictive and condition-based maintenance. Performing the right type of maintenance activity at the right time will minimize maintenance costs and the downtime of engineering systems. However, techniques and methodologies for health prognostics are typically application-specific. This paper aims at developing a generic real time sensor-based prognostic methodology for predicting residual life of engineering systems by modeling explicit relationship between the failure time and the time realizations at different degradation levels. Specifically, a Copula based sampling method is proposed with four technical components for off-line training and on-line life prediction. First of all, degradation signals are pre-processed to have non-decreasing degradation data sets. Next, degradation data sets are dicretized into a certain number of degradation levels with associated time realizations. Then, explicit statistical dependence modeling between the failure time and the time realizations at different degradation levels is conducted using the Bayesian Copula approach and the semi-Copula model. Finally, probability density function of the failure time and the residual life are efficiently predicted using the sampling method provided that we know some true time realizations at a certain number of degradation levels. Residual life predictions of electric cooling fans are employed to demonstrate the proposed method."
258,"Sensors are being increasingly used for real–time health monitoring of complex systems. The measured quantities are expected to provide real–time information about the state of the system, its subsystems, components, and internal and external physical parameters. A complex system normally requires many sensors to extract required information from the sensed environment. The increasing costs of aging systems and infrastructures have become a major concern and real–time health monitoring systems could ensure increased safety and reliability of these systems. Real–time system health monitoring, assesses the state of systems’ health and, through appropriate data processing and interpretation, can predict the remaining life of the system. This paper introduces a method based on Bayesian networks and attempts to find optimum locations of sensors for the best estimate a system health. Information metrics are used for optimized sensor placement based on the value of information that each possible sensor placement scenario provides."
259,"Consumer preferences can serve as an effective basis for determining key product attributes necessary for market success, allowing firms to optimally allocate time and resources toward the development of these critical attributes. However, identification of consumer preferences can be challenging, particularly for technology-push products that are still early on in the technology diffusion S-curve, which need an additional push to appeal to the early majority. This paper presents a method for revealing preferences from actual market data and technical specifications. The approach is explored using three machine learning methods: Artificial Neural Networks, Random Forest decision trees, and Gradient Boosted regression applied on the residential photovoltaic panel industry in California, USA. Residential solar photovoltaic installation data over a period of 5 years from 2007–2011 obtained from the California Solar Initiative is analyzed, and 3 critical attributes are extracted from a pool of 34 technical attributes obtained from panel specification sheets. The work shows that machine learning methods, when used carefully, can be an inexpensive and effective method of revealing consumer preferences and guiding design priorities."
260,"While discrete choice analysis is prevalent in capturing consumers’ preferences and describing their choice behaviors in product design, the traditional choice modeling approach assumes that each individual makes independent decisions, without considering the social impact. However, empirical studies show that choice is social — influenced by many factors beyond engineering performance of a product and consumer attributes. To alleviate this limitation, we propose a new choice modeling framework to capture the dynamic influence from social network on consumer adoption of new products. By introducing the social influence attributes into the choice utility function, the social network simulation is integrated with the traditional discrete choice analysis in a three-stage process. Our study shows the need for considering social impact in forecasting new product adoption. Using hybrid electric vehicle as an example, our work illustrates the procedure of social network construction, social influence evaluation, and choice model estimation based on data from National Household Travel Survey. Our study also demonstrates several interesting findings on the dynamic nature of new technology adoption and how social network may influence consumers’ “green attitude” in hybrid electric vehicle adoption."
261,"This article describes an advance in design optimization that includes consumer purchasing decisions. Decision-Based Design optimization commonly relies on Discrete Choice Analysis (DCA) to forecast sales and revenues for different product variants. Conventional DCA, which represents consumer choice as a compensatory process through maximization of a smooth utility function, has proven to be reasonably accurate at predicting choice and interfaces easily with engineering models. However the marketing literature has documented significant improvement in modeling choice with the use of models that incorporate non-compensatory (descriptive) and compensatory (predictive) components. The non-compensatory component can, for example, model a “consider-then-choose” process in which potential customers first narrow their decisions to a small set of products using heuristic screening rules and then employ a compensatory evaluation to select from this set. This article demonstrates that ignoring consider-then-choose behavior can lead to sub-optimal designs, and that optimality cannot be “recovered” by changing marketing variables alone. A new computational approach is proposed for solving optimal design problems with consider-then-choose models whose screening rules are based on conjunctive (logical “and”) rules. Computational results are provided using three state-of-the-art commercial solvers (matlab, KNITRO, and SNOPT)."
262,"Researchers in Decision-Based Design have asserted that business objectives, e.g. profits, should replace engineering requirements or performance metrics as the objective for engineering design. Using profits as the objective for engineering design, however, requires modeling consumer preferences and competition between firms. Game theoretic “design-then-pricing” models—i.e. product design with price competition—provide an important framework for integrating consumer preferences and competition when design decisions must be made before prices are decided by a firm or by its competitors. This article proposes a method for solving design-then-pricing problems that exhibits improved efficiency and reliability, relative to existing methods. Numerical results for a vehicle design example using three solvers—matlab, KNITRO, and SNOPT—to validate this claim. We also highlight the importance of checking the Second-Order Sufficient Conditions in design-then-pricing problems that use Mixed Logit models of demand."
263,"Due to global climate change, increase in pollution along with reduced quantity of drinking water compared to the total volume of water, the scarcity of potable water is declining gradually. Researchers have become increasingly interested in efficient design of treatment processes, but, there is a lack of research to investigate appropriate, applicable, low cost and simple water treatment processes for underprivileged communities. Providing safe drinking water in these communities is more challenging due to limitation of resources and infrastructure."
264,"Modular products have the potential to significantly reduce the financial risks associated with purchasing an income generating product in the developing world. Their modular nature allows a product to adapt to the changing needs of the customer (changing views of affordability due to increase in income potential). In a previous work by the authors, an optimization-based modular product design method was developed and implemented in the design of a modular irrigation pump for poverty alleviation. This paper revisits this modular pump example with the purpose of physically validating the ability of the method to identify theoretical progressively affordable modular products. This paper gives a summary of the method, presents the theoretical pump design, and compares the performance of the theoretical design to a physical prototype of the pump. Based on observations from this comparison, the authors conclude that the method is a feasible approach to engineering-based poverty alleviation."
265,"Collaborative products are created by combining components from two or more products to result in an additional product that performs previously unattainable tasks. The resulting reduction in cost, weight, and size of a set of products needed to perform a set of functions makes collaborative products useful in the developing world. In this paper, a method for designing a set of products for optimal individual and collaborative performance is introduced. This is accomplished by: (i) characterizing the collaborative design space of the product set and collaborative product, (ii) defining areas of acceptable Pareto offset, (iii) identifying the combinations of designs that fall within the defined areas of acceptable Pareto offset for each product, and (iv) selecting the optimal set of product designs. An example is provided to illustrate this method and demonstrate its usefulness in designing collaborative products for both the developed and developing world. We conclude that the presented method is a novel, and useful, approach for balancing the inherent trade-offs between the performance of collaborative products and the product sets used to create them."
266,"Electricity is a critical need for the rural poor in developing countries. Often this need is met with disposable batteries. This results in high cost and problems with disposal. For example it was recently reported that an isolated rural village in West Africa with a population of 770 uses more than 21,000 disposable batteries per year and that purchase of these batteries accounts for 20–40% of household expense. As a result many organizations are seeking way to meet the need for village energy. This paper presents a case study of one such experience. In this study the efforts to meet the lighting needs of a cluster of eight rural villages with a population of approximately 8,000 people are discussed. A key aspect of this discussion is the challenge of creating a continuing and sustainable village lighting solution. In this case the technology chosen to implement a lighting system was a distributed micro-grid managed locally in each village. The success of this lighting grid has been in large part due to the continuing support of the local micro-grid system both financially and through continued engagement to maintain and upgrade the micro-grid systems."
267,"Integrating products of basic technology research and development efforts into Large Complex Systems (LCSs) requires systematic approaches. It has been observed that because of the complexity associated with LCSs, no single structured design method will suffice for integrating new technologies into an LCS. In this work, we explore through the literature how an integrated design approach involving the Design Structure Matrix (DSM) with several design methods (mainly those involving other matrix-based methods) might support the introduction of new technologies into large complex facilities. The survey presented in the paper could provide support for future investigations on how to align the outcomes of R&D processes with the requirements of introducing new technologies in target LCSs. Also it could help in developing future understandings about transitioning basic outcomes of R&D into technology products and services."
268,"Distributed design systems fundamentally preserve individual design subsystem secrecy by limiting communication across subsystems. The natural secrecy of distributed design makes it difficult for design process managers to determine the appropriate order of subsystems in the design process. In this paper, we discuss a social network theory based heuristic to prescribe the optimal order of design subsystems. We call the order of the design subsystems process architecture and we leverage concepts like ‘distance,’ ‘bridging,’ and degree centrality’ to analyze the aggregate design system and identify preferable solution process architectures. Our network theory approach only requires a manager to know which subsystems share design information. We distinguish this research from previous work by empirically validating the heuristic against a genetic algorithm for 80 randomly generated distributed design systems. The heuristic performs well against the genetic algorithm and beats it in the majority of cases. Moreover, it does so without requiring any function evaluations."
269,"Modern systems are difficult to design because there are a significant number of potential alternatives to consider. The specification of an alternative includes an architecture (which describes the components and connections of the system) and component sizings (the sizing parameter for each component). In current practice, designers rely mainly on their experience and intuition to select a desired architecture without much computational support and then spend most of their effort on optimizing component sizings. In this paper, an approach for representing an architecture selection as a mixed-integer linear programming optimization is presented; existing solvers are then used to identify promising candidate architectures at early stages of the design process. Mathematical programming is a common optimization technique, but it is rarely used for architecture selection because of the difficulty of manually formulating an architecture selection as a mathematical program. In this paper, the formulation is presented in a modular fashion so that model transformations can be applied to transform a problem formulation that is convenient for designers into the mathematical programming optimization. A modular superstructure representation is used to model the design space; in a superstructure a union of all potential architectures is represented as a set of discrete and continuous variables. Algebraic constraints are added to describe both acceptable variable combinations and system behavior to allow the solver to eliminate clearly poor alternatives and identify promising alternatives. The framework is demonstrated on the selection of an actuation subsystem for a hydraulic excavator, although the solution approach would be similar for most mechanical systems."
270,"Technology development is facing increased challenges as engineers begin to tackle the problem domains with greater uncertainty. Future engineered systems must be able to function in unpredictable environments such as deep ocean, rough terrain, and outer space while performing uncertain tasks like hazardous waste cleanup and search-and-rescue missions. Furthermore, the increasing size of engineered systems introduces unplanned interdependencies of components. Complex systems can provide the adaptability in order to manage uncertainties that traditional systems cannot. As the uncertainty of the problem domain increases, engineering design methods must be advanced in order to properly address the changing needs and constraints. This paper proposes a new approach inspired by natural phenomena in order to extend the design envelope towards an "
271,"Complexity metrics have been developed for multiple applications such as consumer products, software, trajectory selection and assembly systems. Although existing complexity metrics were developed to reduce product design and development costs, their lack of simplicity in formulation and robustness has limited their applicability. This paper proposes a standard methodology for comparing and evaluating these metrics and introduces dimensions of complexity that should be considered towards the goal of developing a generalizable product complexity measure. To this end, this paper introduces variables that integrate multiple facets of complexity into a single metric."
272,"Identifying customer needs and preferences is one of the most important tasks in design process. Typically, a variation of interview based approaches is used to conduct need and preference analysis. In this paper, a new approach based on text mining online (internet based) customer reviews to supplement traditional methods of need and preference analysis is considered. The key idea underlying the proposed approach is to partition online customer generated product reviews into segments that evaluate the individual attributes of a product (e.g zoom capability and support of different image formats in a camcorder). Additionally, the proposed method also identifies the importance (ranking) that customers place on each product attributes. The method is demonstrated on 100 customer reviews submitted for camcorders on epinions.com over a two year period."
273,"We define preference elicitation as an interaction, consisting of a sequence of computer queries and human implicit feedback (binary choices), from which the user’s most preferred design can be elicited. The difficulty of this problem is that, while a human-computer interaction must be short to be effective, query algorithms usually require lengthy interactions to perform well. We address this problem in two steps. A black-box optimization approach is introduced: The query algorithm retrieves and updates a user preference model during the interaction and creates the next query containing designs that are both likely to be preferred and different from existing ones. Next, a heuristic based on accumulated elicitations from previous users is employed to shorten the current elicitation by querying preferred designs from previous users (the “crowd”) who share similar preferences to the current one."
274,"Every time a customer selects a product from the shelf they make a purchase decision based on trade-offs between available offerings. The available products often exhibit feature excess at a price premium, feature deficiency at a price discount, or some combination of both. By purchasing one of these products a customer experiences some degree of sacrifice. This paper proposes the use of choice-based conjoint analysis and hierarchical Baysian modeling to calculate the perceived utility of a customer’s ideal product and the perceived utility of the best current alternative in the market. A customer’s sacrifice gap, a quantity that mass customization seeks to minimize, is defined as the difference between these values. This paper quantifies a market-average sacrifice gap and uses it in a theoretical product platform customization scenario. This scenario examines the effects of offering customization options on one attribute of a product at a time on a customer-centric objective (sacrifice gap) and a firm-centric objective (aggregate contribution). The results are also used to examine how customer sacrifice is minimized at an individual-level."
275,"The main purpose of this study is to use an R-test measurement device to estimate the geometric location error of the axis of rotation of five-axis machine tools. The error model of CNC machine tool describes the relationship between the individual error source and its effects on the overall position errors. This study based ISO230 to construct a geometric error model used to measure errors in the five-axis machine tools for the R-test measurement device. This model was then used to reduce the five-axis geometric error model based solely on the location error of the axis of rotation. Moreover, based on the simplified model and the overall position errors measured by the R-test with path K4, the location errors of rotary axes and ball position errors can be estimated very accurately with the least square estimation method. Finally, paths K1 and K2 were used as testing paths. The results of the test showed that the model built in this study is accurate and is effective in estimating errors."
276,"This paper concerns the role of geometric imperfections on assemblies in which the location of a target part is dependent on supports at two features. In some applications, such as a turbo-machine rotor that is supported by a series of parts at each bearing, it is the interference or clearance at a functional target feature, such as at the blades that must be controlled. The first part of the paper relates the limits of location for the target part to geometric imperfections of other parts when stacked-up in parallel paths. In other applications where parts are flexible, deformations are induced when parts in parallel are assembled together by clamping. Presuming that perfectly manufactured parts have been designed to fit perfectly together and produce zero deformations, the clamping-induced deformations result entirely from the imperfect geometry produced during manufacture. The magnitudes and types of these deformations are a function of part dimensions and material stiffnesses, and they are limited by design tolerances that control manufacturing variations. The last part of the paper relates the limits on stresses in parts to functional tolerance limits that must be set at the beginning of a tolerance analysis of parts in any assembly."
277,"Although Rapid prototyping, as a material-additive process, is able to create complex geometries that traditional material-removal processes cannot achieve, it still suffers from its limitation due to the layer manufacturing nature. In order to minimize staircase effects, selection of building orientation is an important step in implementing rapid prototyping processes. This paper presents a method to select the optimal direction that leads to minimized volumetric error in building a part from layer manufacturing processes. A unit sphere is discretized first to give potential directions in a 3-D space, and then each facet comprising an STL geometric model is mapped onto the discretized unit sphere as a great circle individually. Both an exhaustive search and a chain searching strategy are presented to identify the globally optimal direction for building a 3-D geometry. At the end of the paper, examples are presented to show the effectiveness of the method."
278,"A new methodology using mechanism to model fixture location error (FLE) is proposed. Typically, there are four elements related to accumulation and propagation of FLE in workpiece-fixture system (WFS), which are machining feature, process datum, location datum and locators, and the relative position variations among them correspond to various FLEs. In this research, the four elements are treated as link mechanism components, the concatenation between location datum and locators is represented by contact pair, which is transformed to kinematic pair in equivalent mechanism, and geometry & dimension tolerances are simulated by link mechanisms. The rules mapping from FLE elements to their equivalent mechanisms are established, and by which the WFS is transformed into an equivalent mechanism model system. The method is studied that the process parameters of WFS are calculated by the structure parameters and kinematic parameters of equivalent mechanism. Thus, the solution of location error can be realized by means of the position analysis methods of mechanism."
279,"For manufacturing companies it is important to develop and produce products that meet requirements from customers and investors. One key factor in meeting these requirements is the efficiency of the product development process. Design automation is a powerful tool to increase efficiency in that process resulting in shortened lead-time, improved product performance, and ultimately decreased cost. Further, automation is beneficial as it increases the ability to adapt products to new product specifications, which is critical to some categories of products."
280,"Using one single trimmed B-Spline surface to fill an n-sided hole is a much desired operation in CAD, but few papers have addressed this issue. The paper presents the method of using trimmed B-Spline surfaces to fill n-sided holes based on energy minimization or variational technique. The method is efficient and robust, and takes less than one second to fill n-sided holes with high quality B-Spline surfaces under complex constraints."
281,"Many natural systems that transport heat, energy or fluid from a distributed volume to a single flow channel exhibit an analogous appearance to trees (examples include bronchial tubes, watersheds, lightening, and blood vessels). Several authors have proceeded with analytical methods to develop fractal or pseudo-fractal designs analogous to these natural instances. This implicates an implicit belief in some designers that there is an optimal attribute to this ‘tree-like’ appearance. A novel explanation for the appearance of these systems is presented in this paper. Natural systems follow the path of least resistance; or in other words, minimize transport effort. Effort is required to overcome all forms of friction (an unavoidable consequence of motion). Therefore effort minimization is analogous to transport distance (path length) minimization. Effort due to friction will be integrated over the total transport distance. Leveraging this observation a simple, geometric explanation for the emergent ‘tree-like’ architecture of many natural systems is now achievable. Note that this ‘tree’ effect occurs when most of the flow volume exhibits diffusion, with a small percentage of interdigitated high flow velocity channels. One notable application of our novel method, "
283,"The paper presents the first optimal conjugation design methodology based on free-form conjugation modeling theory. The methodology is implemented for planar circular gearing and general for any planar gearing. According to the previous research of free-form conjugation, conjugate profiles are modeled by contact path geometries or cutter geometries which are represented by NURBS (non-uniform rational B-splines). The interchangeability between control points and interpolation points of NURBS are introduced in general to offer reasonable constraints of special interpolation conditions in conjugation design. To adapt to the flexibility brought by free-form techniques, the determination of an important conjugate property—contact ratio is carried out through geometric relationship. To make use of NURBS for optimal design, conjugate properties and their differentiations are represented by important parameters and then by control points and interpolation points of NURBS. The interested properties are relative curvature, specific sliding ratio and nominal Hertz contact stress, which are the main factors of gear efficiency and wear. The properties are well-known for their difficulties in optimization. The paper shows that with appropriate manipulations in mathematics and programming, it is feasible for gradient based optimization methods, which are accurate and fast in convergence. The methodology is consistent with the regular optimization of geometry design, and can be integrated into geometry design systems. The examples show the effectiveness of the proposed optimization framework."
284,"Traditionally, deterministic methods have been applied in digital human modeling (DHM). Transforming the deterministic approach of digital human modeling into a probabilistic approach is natural since there is inherent uncertainty and variability associated with DHM problems. Typically, deterministic studies in this field ignore this uncertainty or try to limit the uncertainty by employing optimization procedures. Due to the variability in the inputs, a deterministic study may not be enough to account for the uncertainty in the system. Probabilistic design techniques allow the designer to predict the likelihood of an outcome while also accounting for uncertainty, in contrast to deterministic studies. The purpose of this study is to incorporate probabilistic approaches to a deterministic DHM problem that has already been studied, analyzing human kinematics and dynamics. The problem is transformed into a probabilistic approach where the human kinematic and dynamic reliabilities are determined. The kinematic reliability refers to the probability that the human end-effector position (and/or orientation) falls within a specified distance from the desired position (and/or orientation) in an inverse kinematic problem. The dynamic reliability refers to the probability that the human end-effector position (and/or velocity) falls within a specified distance from the desired position (and/or velocity) along a specified trajectory in the workspace. The dynamic equations of motion for DHM are derived by the Lagrangian backward recursive dynamics formulation."
285,"Mass center of a human body is not a fixed point on the human body because the inertia distribution of the human body changes with body posture. Real-time estimation of the location of human mass center is often required for many biomechanical or biomedical applications. This is not an easy task if the inertia properties of the human’s body segments are unknown. This paper presents a technique for estimating the trajectory of the human mass center based on a recently developed inertia properties identification technology which was derived based on the impulse-momentum principle. The proposed technique assumes a human body as a general treelike multibody system, such that the mass center of the human is predictable with the knowledge of the barycentric parameters of the human. The latter can be identified using inertia identification method. This technique is advantageous because it requires only the 3D motion capture data as its primary input and does not need to know the inertia and geometric parameters of individual body segments of the human. The paper presents a dynamic simulation based study of the proposed estimation technique and also describes an ongoing experimental testing."
286,"The objective of this study is to predict the “Aiming While Standing” and “Aiming While Kneeling” motion tasks for a soldier (human) using a full-body, three dimensional digital human model. The digital human is modeled as a 55 degree of freedom branched mechanism. Six degrees of freedom specify the global position and orientation of the coordinate frame attached to the pelvis of the digital human and 49 degrees of freedom represent the revolute joints which model the human joints and determine the kinematics of the entire digital human. Motion is generated by a multi-objective optimization approach minimizing the mechanical energy and joint discomfort simultaneously. A sequential quadratic programming (SQP) algorithm in SNOPT is used to solve the nonlinear optimization problem. The optimization problem is subject to constraints which represent the limitations of the environment, the digital human model and the motion task. Design variables are the joint angle profiles. All the forces, inertial, gravitational as well as external, are known, except the ground reaction forces. The feasibility of the generation of that arbitrary motion by using the given ground contact areas is ensured by using the well known Zero Moment Point (ZMP) constraint. During the kneeling motion, different parts of the body come in contact and lose contact with the ground which is modeled using a general approach. The ground reaction force on each transient ground contact area is determined using the equations of motion. It is assumed that enough friction exists that allow the human to generate reaction forces as determined by the ZMP constraint. Using these ground reaction forces, the required torques at all joints are calculated by the recursive Lagrangian formulation. Using the given method, we can predict realistic motions for the “Aiming While Standing” and “Aiming While Kneeling” tasks. The optimization approach is able to very well predict the “Natural Point of Aim” which is a well known concept for soldiers. In other words, the approach is able to predict the most comfortable final orientation of the feet on the ground for engaging a specific target. We also predict cases where the orientation of the soldier’s feet are enforced. Many virtual experiments have been conducted by changing the target location in the 3D space, changing the anthropometry of the soldier, adding armor to different joints, changing the variable parameters of the rifle, adding backpack and using different weapons."
287,"Humans act as transducers that transform chemical energy from food, water, and air into mechanical work and the thermal energy of heat loss. Although this energy expenditure can be experimentally measured, methods of predicting energy expenditure have not been broadly studied. This work introduces a new formulation of metabolic energy consumption based on muscle physiology and the equations of motion for the human body. Kinematic and kinetic data from a gait experiment and an over-arm throwing simulation are used to illustrate and validate this new model. The results extend the capabilities of dynamic human modeling to include metabolic energy prediction in general tasks. This novel formulation is useful for the investigation of human performance with applications in physical therapy, rehabilitation, and sports."
288,"The objective of work presented in this paper is to increase the center-of-mass stability of human walking and running in musculo-skeletal simulation. The approach taken is to approximate the whole-body dynamics of the low-dimensional Spring-Loaded Inverted Pendulum (SLIP) model of locomotion in the OpenSim environment using existing OpenSim tools. To more directly relate low-dimensional dynamic models to human simulation, an existing OpenSim human model is first modified to more closely represent bilateral above-knee amputee locomotion with passive prostheses. To increase stability further beyond the energy-conserving SLIP model, an OpenSim model based upon the Clock-Torqued Spring-Loaded-Inverted-Pendulum (CT-SLIP) model of locomotion is also created. The result of this work is that a multi-body musculo-skeletal simulation in Open-Sim can approximate the whole-body sagittal-plane dynamics of the passive SLIP model. By adding a plugin controller to the OpenSim environment, the Clock-Torqued-SLIP dynamics can be approximated in OpenSim. To change between walking and running, only one parameter representing the preferred period of a stride is changed. The result is a robustly stable simulation of the center-of-mass locomotion for both walking and running that could serve as a first step toward increasingly anatomically accurate and robustly stable musculo-skeletal simulations."
289,"Affordance based design (ABD) theory has been presented in several papers and has interested several researchers in the field of design. One criticism of ABD is that the number of affordances identified can be very large, and therefore, the approach may not be amenable for automation. This paper presents a computer based implementation of a process to improve design using affordances."
290,"Metamodel-based design is a well-established method for providing fast and accurate approximations of expensive computer models to enable faster optimization and rapid design space exploration. Traditionally, a metamodel is developed by fitting a surface to a set of training points that are generated with an expensive computer model or simulation. A requirement of this process is that the function being approximated is continuous. However, many engineering problems have variables that are discrete and a function response that is discontinuous in nature. In this paper, a classifier-guided sampling method is presented that can be used for optimization and design space exploration of expensive computer models that have discrete variables and discontinuous responses. The method is tested on a set of example problems. Results show that the method significantly improves the rate of convergence towards known global optima, on average, when compared to random search."
291,"High Dimensional Model Representation (HDMR) is a tool for generating an approximation of an input-output model for a multivariate function. It can be used to model a black-box function for metamodel-based optimization. Recently the authors’ team has developed a radial basis function based HDMR (RBF-HDMR) model that can efficiently model a high dimensional black-box function and, moreover, to uncover inner variable structures of the black-box function. This approach, however, requests a complete new, although optimized, set of sample points, as dictated by the methodology, while in engineering design practice one often has many existing sample data. How to utilize the existing data to efficiently construct a HDMR model is the focus of this paper. We first identify the Random-Sampling HDMR (RS-HDMR), which uses orthonormal basis functions as HDMR component functions and existing sample points can be used to calculate the coefficients of the basis functions. One of the important issues related to the RS-HDMR is that in theory the basis functions are obtained based on the continuous integrations related to the orthonormality conditions. In practice, however, the integrations are approximated by Monte Carlo summation and thus the basis functions may not satisfy the orthonormality conditions. In this paper, we propose new and adaptive orthonormal basis functions with respect to a given set of sample points for RS-HDMR approximation. RS-HDMR models are built for different test functions using the standard and new adaptive basis functions for different number of sample points. The relative errors for both models are calculated and compared. The results show that the models that are built using the new basis functions are more accurate."
292,"Sequential sampling strategies have been developed for managing complexity when using computationally expensive computer simulations in engineering design. However, much of the literature has focused on objective-oriented sequential sampling methods for deterministic optimization. These methods cannot be directly applied to robust design which must account for uncontrollable variations in certain input variables (i.e., noise variables). Obtaining a robust design that is insensitive to variations in the noise variables is more challenging. Even though methods exist for sequential sampling in design under uncertainty, the majority of the existing literature does not systematically take into account the interpolation uncertainty that results from limitations on the number of simulation runs, the effect of which is inherently more severe than in deterministic design. In this paper, we develop a systematic objective-oriented sequential sampling approach to robust design with consideration of both noise variable uncertainty and interpolation uncertainty. The method uses Gaussian processes to model the costly simulator and quantify the interpolation uncertainty within a robust design objective. We examine several criteria, including our own proposed criteria, for sampling the design and noise variables and provide insight into their performance behaviors. We show that for both of the examples considered in this paper the proposed sequential algorithm is more efficient in finding the robust design solution than a one-shot space filling design."
293,"Layout optimization problems deal with the search for an optimal spatial arrangement of components inside a container. Global performances of many engineering products and systems sharply depend on layout design. Because of the great complexity of most real-world layout applications, recent studies focus on the development of efficient optimization algorithms used to solve layout problems. These algorithms, which have to take into consideration multi-constraint and multi-objective optimization problems, aim to be generic to adapt to lots of layout problems. However, the solving of these layout problems is time consuming and designers know if an optimal solution is available for their problem at the end of the optimization process. Designer cannot know a priori if the layout problem can be solved. Then, this paper proposes a new indicator to assess the feasibility of layout problems. This indicator is based on the layout description of the problem and the formulation of designer’s requirements. In particular, it takes into account the non-overlap constraints between layout components. It allows the designer to know if the layout problem can be solved before running the optimization algorithm. After defining the new indicator of feasibility, this paper tests it on a simple layout application."
294,"A primary concern in practical engineering design is ensuring high system reliability throughout a product life-cycle subject to time-variant operating conditions and component deteriorations. Thus, the capability to deal with time-dependent probabilistic constraints in reliability-based design optimization is of vital importance in practical engineering design applications. This paper presents a nested extreme response surface (NERS) approach to efficiently carry out time-dependent reliability analysis and determine the optimal designs. The NERS employs kriging model to build a nested response surface of time corresponding to the extreme value of the limit state function. The efficient global optimization technique is integrated with the NERS to extract the extreme time responses of the limit state function for any given system design. An adaptive response prediction and model maturation mechanism is developed based on mean square error (MSE) to concurrently improve the accuracy and computational efficiency of the proposed approach. With the nested response surface of time, the time-dependent reliability analysis can be converted into the time-independent reliability analysis and existing advanced reliability analysis and design methods can be used. The NERS is integrated with RBDO for the design of engineered systems with time-dependent probabilistic constraints. Two case studies are used to demonstrate the efficacy of the proposed NERS approach."
295,"The ever increasing demands towards improvement in vehicle performance and passenger comfort have led the automotive manufacturers to further enhance the design in the early stages of the vehicle development process. Though, these design changes enhance the overall vehicle performance to an extent, the placement of these components under the car hood also plays a vital role in increasing the vehicle performance. In the past, a study on the automobile underhood packaging or layout problem was conducted and a multi-objective optimization routine with three objectives namely, minimizing center of gravity height, maximizing vehicle components accessibility and maximizing survivability (for army vehicles) has been setup to determine the optimal locations of the underhood components. The previous study did not consider thermal performance as an objective. This study asserts the necessity of including thermal performance as an objective and makes an assessment of the several available thermal analyses that are performed on the automotive underhood to evaluate the thermal objective. A Neural Network approximation of the CFD analysis conducted over the automotive underhood is presented in this paper. The results obtained from the Neural Network are compared with the CFD results, showing good agreement. The Neural Network model is included in the multi-objective optimization routine and new layout results are obtained. A non-deterministic evolutionary multi-objective algorithm (AMGA-2) is used to perform the optimization process."
296,"Approximation Assisted Optimization (AAO) is widely used in engineering design problems to replace computationally intensive simulations with metamodeling. Traditional AAO approaches employ global metamodeling for exploring an entire design space. Recent research works in AAO report on using local metamodeling to focus on promising regions of the design space. However, very limited works have been reported that combine local and global metamodeling within AAO. In this paper, a new approximation assisted multiobjective optimization approach is developed. In the proposed approach, both global and local metamodels for objective and constraint functions are used. The approach starts with global metamodels for objective and constraint functions and using them it selects the most promising points from a large number of randomly generated points. These selected points are then “observed”, which means their actual objective/constraint function values are computed. Based on these values, the “best” points are grouped in multiple clustered regions in the design space and then local metamodels of objective/constraint functions are constructed in each region. All observed points are also used to iteratively update the metamodels. In this way, the predictive capabilities of the metamodels are progressively improved as the optimizer approaches the Pareto optimum frontier. An advantage of the proposed approach is that the most promising points are observed and that there is no need to verify the final solutions separately. Several numerical examples are used to compare the proposed approach with previous approaches in the literature. Additionally, the proposed approach is applied to a CFD-based engineering design example. It is found that the proposed approach is able to estimate Pareto optimum points reasonably well while significantly reducing the number of function evaluations."
297,"The concept of Pareto optimality is the default method for pruning a large set of candidate solutions in a multi-objective problem to a manageable, balanced, and rational set of solutions. While the Pareto optimality approach is simple and sound, it may select too many or too few solutions for the decision-maker’s needs or the needs of optimization process (e.g. the number of survivors selected in a population-based optimization). This inability to achieve a target number of solutions to keep has caused a number of researchers to devise methods to either remove some of the non-dominated solutions via Pareto filtering or to retain some dominated solutions via Pareto relaxation. Both filtering and relaxation methods tend to introduce many new adjustment parameters that a decision-maker (DM) must specify."
298,"It is important for engineers to understand the capabilities and limitations of the technologies they consider for use in their systems. Several researchers have investigated approaches for modeling the capabilities of a technology with the aim of supporting the design process. In these works, the information about the physical form is typically abstracted away. However, the efficient generation of an accurate model of technical capabilities remains a challenge. Pareto frontier based methods are often used but yield results that are of limited use for subsequent decision making and analysis. Models based on parameterized Pareto frontiers—termed Technology Characterization Models (TCMs)—are much more reusable and composable. However, there exists no efficient technique for modeling the parameterized Pareto frontier. The contribution of this paper is a new algorithm for modeling the parameterized Pareto frontier to be used as a model of the characteristics of a technology. The proposed algorithm uses fundamental concepts from multiobjective genetic optimization and machine learning to generate a model of the technology frontier."
299,"Multiple microchannel heat transfer systems have been developed for the urge of rapid and effective cooling of the electronic devices, which have become smaller and more powerful but also produced more heat. Two different types of single-phase liquid cooling, including the straight and U-shaped microchannel heat sinks, have been utilized to reduce the temperature of the electronic chips. The cooling performances however depend on the preferences of different factors such as the thermal resistances, the pressure drops, and the heat flows at the solid-fluid interfaces. Lower thermal resistance represents higher temperature reduction; lower pressure drop means lower usage of the pumping power; and higher heat flows indicates more effective cooling between the heat spreader and the liquid. In this paper, an optimization strategy based on the prioritized performances has been developed to find the optimal design variables for multiple objectives: minimal thermal resistances, minimal pressure drops and maximal heat flows. The fuzzy and correlated preferences are modeled by the Gaussian membership functions with respect to different levels of the objective function values. The overall performances are formulated based on the prioritized preferences and maximized on the Pareto-optimal solution set to find the solutions for various preference conditions. Two case studies have been discussed. The first case considered the prioritized preferences based on uni-objective function values while the second one focused on the preferences of the thermal resistances and the efficiency measures, correlatively evaluated by the flow rates, pressure drops, and heat flows."
300,"In a recent publication, we presented a new strategy for engineering design and optimization, which we termed formulation space exploration. The formulation space for an optimization problem is the union of all variable and design objective spaces identified by the designer as being valid and pragmatic problem formulations. By extending a computational search into this new space, the solution to any optimization problem is no longer predefined by the optimization problem formulation. This method allows a designer to both diverge the design space during conceptual design and converge onto a solution as more information about the design objectives and constraints becomes available. Additionally, we introduced a new way to formulate multiobjective optimization problems, allowing the designer to change and update design objectives, constraints, and variables in a simple, fluid manner that promotes exploration. In this paper, we investigate three use scenarios where formulation space exploration can be utilized in the early stages of design when it is possible to make the greatest contributions to development projects. Specifically, we look at s-Pareto frontier generation in the formulation space, formulation space boundary exploration, and a new way to perform inverse optimization. The benefits of these methods are illustrated with the conceptual design of an impact driver."
301,"The optimization of a statistical process control with a variable sampling interval is studied. A control performance index is the expected loss, caused by delay in detecting process change. It is to be minimized by a proper choice of a sampling interval. The mathematical model of this problem is a nonstandard variational calculus problem with two types of constraints, an isoperimetric constraint and two geometric constraints. The integrands in the cost functional and the isoperimetric constraint are independent of the derivative of the minimizing function. Therefore, the classical Euler-Lagrange equation approach is not applicable when analyzing this extremal problem. The optimization problem depends on the signal-to-noise ratio parameter. The original problem is transformed to an equivalent optimal control problem. Based on the value of the parameter, the latter is decomposed into two simpler problems, solved by application of Pontryagin’s Maximum Principle. The theoretical results are evaluated by numerical simulations."
302,"The complexity of managing multidisciplinary engineering systems offers an unprecedented opportunity to investigate decomposition methods, which separate a system into a number of smaller subsystems that can be designed in multiple physical locations and coordinate the design of the subsystems to collaboratively achieve the original system design. This paper studies a network target coordination model for optimizing subsystems that are distributed as multiple agents in a network. To solve these coupled subsystems concurrently, we consider the “consensus optimization” approach by incorporating subgradient algorithms so that the master problem or auxiliary design variables required by most distributed coordination methods are not needed. The method allows each agent to conduct its optimization by locally solving for coupling variables with the information obtained from other agents in the network in an iteratively improving process. The convergence results of a geometric programming problem that satisfies the convexity assumption is provided. Moreover, two non-convex examples are tested to investigate the convergence characteristics of the proposed methods."
303,"Intricate and complex dependencies between multiple disciplines require iterative intensive optimization processes. To this end, multidisciplinary design optimization (MDO) has been established as a convincing concurrent technique to manage inherited complexities."
304,"Modifying the design of an existing system to meet the needs of a new task is a common activity in mechatronic system development. Often engineers seek to meet requirements for the new task via control design changes alone, but in many cases new requirements are impossible to meet using control design only; physical system design modifications must be considered. Plant-Limited Co-Design (PLCD) is a design methodology for meeting new requirements at minimum cost through limited physical system (plant) design changes in concert with control system redesign. The most influential plant changes are identified to narrow the set of candidate plant changes. PLCD provides quantitative evidence to support strategic plant design modification decisions, including tradeoff analyses of redesign cost and requirement violation. In this article the design of a counterbalanced robotic manipulator is used to illustrate successful PLCD application. A baseline system design is obtained that exploits synergy between manipulator passive dynamics and control to minimize energy consumption for a specific pick-and-place task. The baseline design cannot meet requirements for a second pick-and-place task through control design changes alone. A limited set of plant design changes is identified using sensitivity analysis, and the PLCD result meets the new requirements at a cost significantly less than complete system redesign."
305,"This paper presents an optimum design method for mechanical structures considering harmonic loads using a level set-based topology optimization method and the Finite Element Method (FEM). First, we briefly discuss the level set-based topology optimization method. Second, a topology optimization problem is formulated for a dynamic elastic design problem using level set boundary expressions. The objective functional is set to minimize the displacement at specific boundaries. Based on this formulation, the topological sensitivities of the objective functional are derived. Next, a topology optimization algorithm is proposed that uses the FEM to solve the equilibrium and adjoint equations, and when updating the level set function. Finally, several numerical examples are provided to confirm the validity and utility of the proposed method."
306,"In the discrete topology optimization, material state is either solid or void and there is no topology uncertainty problem caused by intermediate material state. In this paper, the improved quadrilateral discretization model is introduced for the discrete topology optimization of structures. The design domain is discretized into quadrilateral design cells and each quadrilateral design cell is further subdivided into 16 triangular analysis cells. All kinds of dangling quadrilateral design cells and sharp-corner triangular analysis cells are removed in the improved quadrilateral discretization model to promote the material utilization. To make the designed structures safe, the local stress constraint is directly imposed on each triangular analysis cell. To circumvent the geometrical bias against the vertical design cells, the binary bit-array genetic algorithm is used to search for the optimal topology. The effectiveness of the proposed improved quadrilateral discretization model and its related discrete topology optimization method is verified by two topology optimization examples of structures."
307,"Level set topology optimization defines the solution using the level set function values stored at the nodes of a regular finite element grid. These values represent a signed distance function which indicates the distance from each node to the structural boundaries. During optimization, nodal sensitivities are used to update the level set function values, moving the boundaries to create a more optimal structure. This paper presents two applications of the 3D level set topology optimization procedure aiming to minimize structural compliance subject to a volume constraint. The first application is the internal structure of a light subsonic aircraft wing. The results suggest that an alternative arrangement of ribs and sparse may be a more optimal solution for wing structures. The second application is the internal trabecular bone structure of an os-calcis. Comparison of the modeled optimal structure and the real internal structure suggest the internal bone structure is mechanically optimal."
308,"This paper introduces a platform that combines shape grammars with conventional simulation and analysis methods. The premise of this combination is to create an approach to synthesizing optimal shapes considering criteria requiring heat transfer and stress analysis for their evaluation. The necessary mechanisms and issues for integrating shape grammars with standard simulation systems are described. The benefits, challenges and future outlook of this approach with regards to traditional design synthesis systems are explored. Further, possible future research projects to extend the work are presented."
309,"The meta-material design of the shear layer of a non-pneumatic wheel was completed using topology optimization. In order to reduce the hysteretic rolling loss, an elastic material is used and the shear layer microstructure is defined to achieve high compliance comparable to that offered by the elastomeric materials. To simulate the meta-material properties of the shear layer, the volume averaging analysis, instead of more popular homogenization methods, is used as the relative size of the shear layer places realistic manufacturing constraints on the size of unit cells used to generate the meta-material. In this design scenario the properties predicted by the homogenization methods are not accurate since the homogenization scaling assumptions are violated. A number of optimal designs are shown to have meta-material properties similar to those of the linear elastic properties of elastomers, making them good meta-material candidates for the shear layer of the non-pneumatic wheel."
310,"The newly developed element exchange method (EEM) for topology optimization is applied to the problem of blank shape optimization for the sheet-forming process. EEM uses a series of stochastic operations guided by the structural response of the model to switch solid and void elements in a given domain to minimize the objective function while maintaining the specified volume fraction. In application of EEM to blank optimization, a sheet forming simulation model is developed using Abaqus/Explicit. With the goal of minimizing the variability in wall thickness of the formed component, a subset of solid (i.e., high density) elements with the highest increase in thickness is exchanged with a consistent subset of void (i.e., low density) elements having the highest decrease in thickness so that the volume fraction remains constant. The EEM operations coupled with finite element simulations are repeated until the optimum blank geometry (i.e., boundary and initial thickness) is found. The developed numerical framework is applied to blank optimization of a benchmark problem. The results show that EEM is successful in generating the optimum blank geometry efficiently and accurately."
311,"Projection-based algorithms are arising as a powerful tool for continuum topology optimization. They use independent design variables that are projected onto element space to create structure topology. The projection functions are designed so that geometric properties, such as the minimum length scale of features, are naturally achieved. They therefore offer an efficient means for imposing geometry-related design specifications and/or manufacturing constraints. This paper presents recent advances in projection-based algorithms, including topology optimization under manufacturing constraints related to milling and casting processes. The new advancements leverage the logic of recently proposed algorithms for Heaviside projection, including eliminating continuation methods on projection parameters and potential for using multiple design variables to achieve active projection of each phase used in design. The primary advantages of such an approach are that manufacturing restrictions are achieved naturally, without need for additional constraints, and that sensitivity calculations are efficient and straightforward. The primary drawback of the approach is that the so-called neighborhood maps require storage for efficient processing when using unstructured meshing."
312,"For clustering a large Design Structure Matrix (DSM), computerized algorithms are necessary. A common algorithm by Thebeau uses stochastic hill-climbing to avoid local optima. The output of the algorithm is stochastic, and to be certain a very good clustering solution has been obtained, it may be necessary to run the algorithm thousands of times. To make this feasible in practice, the algorithm must be computationally efficient. Two algorithmic improvements are presented. Together they improve the quality of the results obtained and increase speed significantly for normal clustering problems. The proposed new algorithm is applied to a cordless handheld vacuum cleaner."
313,"The generational variety index (GVI) helps identify the components of product variants that are most likely to require redesign in the future. These components can then be embedded with the flexibility required for them to be easily modifiable; the remaining components can be designed into a platform. This paper describes the application of the GVI technique in studying the evolution of the Apple iPhone, which was first released in 2007 and has since undergone multiple redesigns. The analysis includes the five generations of the iPhone (original, 3G, 3GS, 4, and 4S) and focuses primarily on mechanical sub-systems. The results of the analysis and subsequent design recommendations are compared with the actual design evolution of the iPhone product line. For certain subsystems, this comparison reveals a divergence in Apple’s design decision-making from the evolution recommended by the GVI technique. Limitations include its retrospective nature and the use of only publicly available data."
314,"The Bisociative Design framework proposed in this work aims to quantify hidden, previously unknown design synergies/insights across seemingly unrelated product domains. Despite the overabundance of data characterizing the digital age, designers still face tremendous challenges in transforming data into knowledge throughout the design processes. Data driven methodologies play a significant role in the product design process ranging from customer preference modeling to detailed engineering design. Existing data driven methodologies employed in the design community generate mathematical models based on data relating to a specific domain and are therefore constrained in their ability to discover novel design insights beyond the domain itself (I.e., cross domain knowledge)."
315,"Our goal is to select a robust vehicle portfolio mix and optimize its design attributes such that contribution margin is maximized while being regulation compliant under varying fuel prices. Compliance to regulation is measured in terms of the Corporate Average Fuel Economy or CAFE. We formulate this vehicle portfolio optimization problem as a mixed integer non-linear programming problem, both under static and varying fuel price scenarios. We demonstrate our approach using a case study in which an in-house market simulator is employed for incorporating consumer preferences in portfolio decisions. This market simulator uses real-time preferences from tens of thousands of shoppers and captures preference heterogeneity using different Logit coefficients for each shopper and hence is computationally expensive. Also, it does not explicitly model the influence of fuel price in predicting demand. To overcome these issues and to facilitate portfolio optimization we use meta-models of the market simulator. Our results show that while remaining regulation compliant it is also possible to achieve significant improvement in the portfolio’s contribution margin. In some scenarios, the improvements in contribution margin are more than 40% when compared to the traditional approach of using expert judgment to decide the portfolio mix."
316,"Global product family design is the problem in which product variants and supply chain configuration are simultaneously designed. It has become a significant concern of manufacturing industries under globalization. Its context is not only complicated under various factors and their interactions but also vague under strategic decision making. In this paper, first, a multi-objective mixed-integer formulation of simultaneous design of module commonalization and supply chain configuration is developed under the criteria on quality, cost and delivery, and an optimization algorithm for obtaining Pareto optimal solutions is configured by using a neighborhood cultivation genetic algorithm and simplex method. Then, this paper investigates into design concept exploration on the optimality and compromise in global product family design with data-mining techniques, a principal component analysis technique and a self-organizing map technique. This paper demonstrates some numerical case studies for ascertaining the validity and promise of the proposed mathematical model and computational techniques for supporting the designer’s decision making toward the excellence in global product family design."
317,
318,"Product family design strategies based on a common core platform have emerged as an efficient and effective means of providing product variety. The main goal in product platform design is to maximize internal commonality within the family while managing the inherent loss in product performance. Therefore, identification and selection of platform variables is a key aspect when designing a family of products. Based on previous research, the Product Platform Constructal Theory Method (PPCTM) provides a systematic approach for developing customizable products, while allowing for multiple levels of commonality, multiple product specifications, and balancing the tradeoffs between commonality and performance. However, selection of platform variables and the modes for managing product variety are not guided by a systematic process in this method. When developing a platform with more than a few variables, a quantitative method is needed for selecting the optimal platform variable hierarchy. In this paper we present an augmented PPCTM which includes sensitivity analysis of platform variables, such that hierarchical rank is conducted based on the impact of the variables on the product performance. This method is applied to the design of a line of customizable finger pumps."
319,"The formation of modules is an important step in establishing a product’s architecture. This paper proposes a clustering algorithm that creates functionally cohesive and loosely coupled modules in a product’s architecture. The algorithm seeks to group together product elements to form functionally similar and loosely-coupled modules. The algorithm also has the advantage of not requiring any user-defined starting parameters that are necessary in some other clustering algorithms. The proposed algorithm is demonstrated on a laser printer and the results are compared to the results from an existing algorithm."
320,"This paper proposes the PSS Business Case Map as a tool to support designers’ idea generation in PSS design. The map visualizes the similarities among PSS business cases in a two-dimensional diagram. To make the map, PSS business cases are first collected by conducting, for example, a literature survey. The collected business cases are then classified from multiple aspects that characterize each case such as its product type, service type, target customer, and so on. Based on the results of this classification, the similarities among the cases are calculated and visualized by using the Self-Organizing Map (SOM) technique. A SOM is a type of artificial neural network that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional) view from high-dimensional data. The visualization result is offered to designers in a form of a two-dimensional map, which is called the PSS Business Case Map. By using the map, designers can figure out the position of their current business and can acquire ideas for the servitization of their business."
321,"Nowadays, enterprises’ efforts are focused on increasing their product values with additional services and contents to satisfy diverse customer needs in competitive market environments. Trends of integrating services and products lead to the emergence of a product-service system (PSS). To enable designers and manufactures to implement and embody a PSS solution in new product development, there is a need for a comprehensive design framework to facilitate the design factors of PSS in complex business environments. The objective of this research is to propose a product-service system design framework to identify design factors for products and services in the context of a business ecosystem. In this paper, we introduce primary and secondary functions to understand customer purchasing motivations, which can be represented as the design factors. A survey on representative IT products is conducted to identify the design factors in terms of PSS. A business ecosystem is a group of entities associated with PSS. With the emergence of PSS, competitions in homogeneous market segment now become confrontations among different business ecosystems. In the proposed framework, we define a Product-Service platform (PS platform) as interface for customers toward PSS manufacturers, its supplier, and contents providers in the business ecosystem. Further, we discuss PS platform’s roles and advantages with case studies involving electronic consumer products."
322,"The research within the Product-Service Systems (PSS) field aims to support manufacturing industries’ ability to provide value in terms of a service offer to its customers, simultaneously taking a more holistic approach to eco-sustainability. The industrial idea of providing customer benefits in parallel with robust products is not new, yet equipping engineers to conduct innovation and applying a service perspective in the early design stages is noticed as fairly radical. The purpose in this paper is two-fold. First, to describe research efforts within the PSS field seen through our engineering design lenses, second, to explore and discuss plausible directions and by that identify “white spots” on the map, which may be seen as relevant directions for future research."
323,"The value creation paradigm in industry has recently been shifting toward value creation through Product-Service Systems (PSS) where product and service elements are tightly integrated as systems to provide functional fulfillment. A successful PSS should satisfy economical, ecological and experience values. A systematic design method for PSS has been developed where activities of stakeholders are designed to support those values reflecting diverse context elements. Also business model aspects of PSS are considered using the category of the business model canvas together with strategies and protocols obtained from various real world cases. To illustrate the PSS design method with business model strategy integrated, a case of urban umbrella rental service system is presented."
324,"Inverse simulation is an inverse process of a direct simulation. During the process, the simulation input variables are identified for a given set of simulation output variables. Uncertainties such as random parameters may exist in engineering applications of inverse simulation. A reliability method is developed in this work to estimate the probability distributions of unknown simulation input. The First Order Reliability Method is employed and modified so that the inverse simulation is embedded within the reliability analysis algorithm. This treatment avoids the separate executions of reliability analysis and inverse simulation and consequently maintains high efficiency. In addition, the means and standard deviations of unknown input variables can also be obtained. A particle impact problem is presented to demonstrate the proposed method for inverse simulation under uncertainty."
325,"Maintaining high accuracy and efficiency is a challenging issue in time-dependent reliability analysis. In this work, an accurate and efficient method is proposed for limit-state functions with the following features: The limit-state function is implicit with respect to time, and its input contains stochastic processes; the stochastic processes include only general strength and stress variables, or the limit-state function is monotonic to these stochastic processes. The new method employs random sampling approaches to estimate the distributions of the extreme values of the stochastic processes. The extreme values are then used to replace the corresponding stochastic processes, and consequently the time-dependent reliability analysis is converted into its time-invariant counterpart. The commonly used time-invariant reliability method, the First Order Reliability Method, is then applied for the time-variant reliability analysis. The results show that the proposed method significantly improves the accuracy and efficiency of time-dependent reliability analysis."
326,"Uncertainty plays a critical role in engineering design as even a small amount of uncertainty could make an optimal design solution infeasible. The goal of robust optimization is to find a solution that is both optimal and insensitive to uncertainty that may exist in parameters and design variables. In this paper, a novel approach, "
327,"Uncertainty modeling in reliability-based design optimization problems requires a large amount of measurement data that are generally too costly in engineering practice. Instead, engineers are constantly challenged to make timely design decisions with only limited information at hand. In the literature, Bayesian binomial inference techniques have been used to estimate the reliability values of functions of uncertainties with limited samples. However, existing methods assume one sample as the entire set of measurements with one for each uncertain quantity while in reality one sample is one measurement on a specific quantity. As a result, effective yet efficient allocating resources in sample augmentation is needed to reflect the relative contributions of uncertainties on the final optimum. We propose a sample augmentation process that uses the concept of sample combinations. Uncertain quantities are sampled with respect to their relative ‘importance’ while the impacts of bad measurements, which affect the evaluation of reliability inference, are alleviated via a Markov-Chain Monte Carlo filter. The proposed method could minimize the efforts and resources without assuming distributions for uncertainties. Several examples are used to demonstrate the validity of the method in product development."
328,"This paper proposes a novel second-order reliability method (SORM) using non-central or general chi-squared distribution to improve the accuracy of reliability analysis in existing SORM. Conventional SORM contains three types of errors: (1) error due to approximating a general nonlinear limit state function by a quadratic function at most probable point (MPP) in the standard normal U-space, (2) error due to approximating the quadratic function in U-space by a hyperbolic surface, and (3) error due to calculation of the probability of failure after making the previous two approximations. The proposed method contains the first type of error only which is essential to SORM and thus cannot be improved. However, the proposed method avoids the other two errors by describing the quadratic failure surface with the linear combination of non-central chi-square variables and using the linear combination for the probability of failure estimation. Two approaches for the proposed SORM are suggested in the paper. The first approach directly calculates the probability of failure using numerical integration of the joint probability density function (PDF) over the linear failure surface and the second approach uses the cumulative distribution function (CDF) of the linear failure surface for the calculation of the probability of failure. The proposed method is compared with first-order reliability method (FORM), conventional SORM, and Monte Carlo simulation (MCS) results in terms of accuracy. Since it contains fewer approximations, the proposed method shows more accurate reliability analysis results than existing SORM without sacrificing efficiency."
329,"Designing sealing solutions in wind turbines made of elastomeric materials that are robust to various uncertainties from manufacturing errors, material imperfections and wind turbine operation is a challenging task. In this paper, we focus on the simulation-based design of material properties and geometrical dimensions of such sealing solutions. We use a systematic approach to robust concept exploration based on a multi-objective decision formulation, the compromise Decision Support Problem (cDSP). Besides using the rather traditional Archimedean or standard utility function based goal formulations in the cDSP, we leverage the conjoint analysis approach to facilitate the preference elicitation process for the various stakeholders involved in the complex product development processes in industrial practice. By parametrically tailoring geometrical dimensions and material properties, characteristics that are superior to those of more heuristic sealing designs and less sensitive to imperfections in the processing and manufacturing routes as well as operation of the wind turbine are achieved. We compare base-line and various robust solutions from traditional Archimedean and standard or conjoint analysis utility function goal formulations. Thereby, we show that using the conjoint analysis within a systematic approach to robust concept exploration is well-suited for industrial practice. The systematic approach to robust concept exploration not only yields superior solutions validated by sealing prototypes in wind turbine operation, it also fosters product development efficiency by applying design of experiment and meta-modeling techniques instead of focusing on a more heuristic product development process to achieve sealing designs."
330,"Disassembly sequence planning at the early conceptual stage of design leads to enormous benefits including simplification of products, lower assembly and disassembly costs, and design modifications which result in increased potential profitability of end-of-life salvaging operations. However, in the early design stage, determining the best disassembly sequence is challenging. First, the required information is not readily available and very time-consuming to gather. In addition, the best solution is sometimes counterintuitive, even to those with experience and expertise in disassembly procedures. Integrating analytical models with Immersive Computing Technology (ICT) can help designers overcome these issues. A two-stage procedure for doing so is introduced in this paper. In the first stage, a stochastic programming model together with the information obtained through immersive simulation is applied to determine the optimal disassembly sequence, while considering uncertain outcomes, such as time, cost and the probability of causing damage. In the second stage, ICT is applied as a tool to explore alternative disassembly sequence solutions in an intuitive way. The benefit of using this procedure is to determine the best disassembly sequence, not only by solving the analytic model, but also by capturing human expertise. The designer can apply the obtained results from these two stages to analyze and modify the product design. An example of a Burr puzzle is used to illustrate the application of the method."
331,"Modeling uncertainty through probabilistic representation in engineering design is common and important to decision making that considers risk. However, representations of uncertainty often ignore elements of “imprecision” that may limit the robustness of decisions. Further, current approaches that incorporate imprecision suffer from computational expense and relatively high solution error. This work presents the Computationally Efficient Imprecise Uncertainty Propagation (CEIUP) method which draws on existing approaches for propagation of imprecision and integrates sparse grid numerical integration to provide computational efficiency and low solution error for uncertainty propagation. The first part of the paper details the methodology and demonstrates improvements in both computational efficiency and solution accuracy as compared to the Optimized Parameter Sampling (OPS) approach for a set of numerical case studies. The second half of the paper is focused on estimation of non-dominated design parameter spaces using decision policies of Interval Dominance and Maximality Criterion in the context of set-based sequential design-decision making. A gear box design problem is presented and compared with OPS, demonstrating that CEIUP provides improved estimates of the non-dominated parameter range for satisfactory performance with faster solution times. Parameter estimates obtained for different risk attitudes are presented and analyzed from the perspective of Choice Theory leading to questions for future research. The paper concludes with an overview of design problem scenarios in which CEIUP is the preferred method and offers opportunities for extending the method."
332,"The design optimization process relies often on computational models for analysis or simulation. These models must be validated to quantify the expected accuracy of the obtained design solutions. It can be argued that validation of computational models in the entire design space is neither affordable nor required. In previous work, motivated by the fact that most numerical optimization algorithms generate a sequence of candidate designs, we proposed a paradigm where design optimization and calibration-based model validation are performed concurrently in a sequence of variable-size local domains that are relatively small compared to the entire design space. A key element of this approach is how to account for variability in test data and model predictions in order to determine the size of the local domains at each stage of the sequential design optimization process. In this paper, we discuss two alternative techniques for accomplishing this: parametric and nonparametric bootstrapping. The parametric bootstrapping assumes a Gaussian distribution for the error between test and model data and uses maximum likelihood estimation to calibrate the prediction model. The nonparametric bootstrapping does not rely on the Gaussian assumption providing therefore, a more general way to size the local domains for applications where distributional assumptions are difficult to verify, or not met at all. If distribution assumptions are met, parametric methods are preferable over nonparametric methods. We use a validation literature benchmark problem to demonstrate the application of the two techniques, emphasizing that results cannot be compared. Which technique to use depends on whether the Gaussian distribution assumption is appropriate based on available information."
333,"Virtual testing is a new engineering development trend to design, evaluate, and test new engineered products. This research proposes a virtual testing framework for new product development using three successive steps: (i) statistical model calibration, (ii) hypothesis test for validity check and (iii) virtual qualification. Statistical model calibration first improves the predictive capability of a computational model over a calibration domain. Next, the hypothesis test is performed under limited observed data to see if a calibrated model is sufficiently predictive for virtual testing of a new design. A u-pooling metric is employed for the hypothesis test to measure the degree of mismatch between predicted and observed results while considering uncertainty in the u-pooling metric due to the lack of experimental data. The calibrated model can be rejected only when the measured metric of the calibrated model strongly suggest that the null hypothesis—the calibrated model is valid—is false. If the null hypothesis is accepted, the virtual qualification process can be executed with a qualified model for new product developments. The qualification process builds a design decision matrix to aid in rational decision-making on the product developments. A computational model of a tire tread block was used to demonstrate the effectiveness of the proposed framework."
334,"Tradeoff studies help designers better understand how different design considerations relate to one another and to make decisions. Generally a tradeoff study involves a systematic multi-criteria evaluation of various alternatives for a particular system or subsystem. After evaluating these alternatives, designers eliminate those that perform poorly using the Pareto dominance criterion and explore more carefully those that remain."
335,"The Fokker-Planck equation is widely used to describe the time evolution of stochastic systems in drift-diffusion processes. Yet, it does not differentiate two types of uncertainties: aleatory uncertainty that is inherent randomness and epistemic uncertainty due to lack of perfect knowledge. In this paper, a generalized Fokker-Planck equation based on a new generalized interval probability theory is proposed to describe drift-diffusion processes under both uncertainties, where epistemic uncertainty is modeled by the generalized interval while the aleatory one is by the probability measure. A path integral approach is developed to numerically solve the generalized Fokker-Planck equation. The resulted interval-valued probability density functions rigorously bound the real-valued ones computed from the classical path integral method. The new approach is demonstrated by numerical examples."
336,"In this paper, a sampling-based RBDO method using a classification method is presented. The probabilistic sensitivity analysis is used to compute sensitivities of probabilistic constraints with respect to random variables. Since the probabilistic sensitivity analysis requires only the limit state function, and not the response surface or sensitivity of the response, an efficient classification method can be used for a sampling-based RBDO. The proposed virtual support vector machine (VSVM), which is a classification method, is a support vector machine (SVM) with virtual samples. By introducing virtual samples, VSVM overcomes the deficiency in existing SVM that uses only classification information as their input. In this paper, the universal Kriging method is used to obtain locations of virtual samples to improve the accuracy of the limit state function for highly nonlinear problems. A sequential sampling strategy effectively inserts new samples near the limit state function. In sampling-based RBDO, Monte Carlo simulation (MCS) is used for the reliability analysis and probabilistic sensitivity analysis. Since SVM is an explicit classification method, unlike implicit methods, computational cost for evaluating a large number of MCS samples can be significantly reduced. Several efficiency strategies, such as the hyper-spherical local window for generation of the limit state function and the Transformations/Gibbs sampling method to generate uniform samples in the hyper-sphere, are also applied. Examples show that the proposed sampling-based RBDO using VSVM yields better efficiency in terms of the number of required samples and the computational cost for evaluating MCS samples while maintaining accuracy similar to that of sampling-based RBDO using the implicit dynamic Kriging (D-Kriging) method."
337,"In practical engineering problems, often only limited input data are available to generate the input distribution model. The insufficient input data induces uncertainty on the input distribution model, and this uncertainty will cause us to lose confidence in the optimum design obtained using the reliability-based design optimization (RBDO) method. Uncertainty on the input distribution model requires us to consider the reliability analysis output, which is defined as the probability of failure, to follow a probabilistic distribution. This paper proposes a new formulation for the confidence-based RBDO method and design sensitivity analysis of the confidence level. The probability of the reliability analysis output is obtained with consecutive conditional probabilities of input distribution parameters and input distribution types using a Bayesian approach. The approximate conditional probabilities of input distribution parameters and types are suggested under certain assumptions. The Monte Carlo simulation is applied to practically calculate the output distribution, and the copula is used to describe the correlated input distribution types. A confidence-based RBDO problem is formulated using the derived the distribution of output. In this new formulation, the probabilistic constraint is modified to include both the target reliability and the target confidence level. Finally, the sensitivity of the confidence level, which is a new probabilistic constraint, is derived to support an efficient optimization process. Using accurate surrogate models, the proposed method does not require generation of additional surrogate models during the RBDO iteration; it only requires several evaluations of the same surrogate models. Hence, the efficiency of the method is obtained. For the numerical example, the confidence level is calculated and the accuracy of the derived sensitivity is verified when only limited data are available."
338,"In the New Product Development processes, there are usually multiple players interacting through coupled design activities with conflicting design objectives. We use Set-based design approach and Constraint Satisfaction Problem solving techniques to deal with the multiplayer, multi-objective design problem. In order to observe the states of the players during the design process, we develop satisfaction and progress indicators and combine them into a wellbeing indicator. In this paper we simulate different player behaviors on a two-player, multi-objective engineering design problem of a hollow cylindrical cantilever beam. During the simulation cases, at each simulation iteration automatic players are prioritized regarding their wellbeing states for proposing constraints in order to find a set of consistent solutions that improve their satisfaction states. Therefore during the simulation process, while solution space converges to a single solution, epistemic uncertainty is reduced and players’ satisfaction states are improved in wellbeing equilibrium."
339,"Quantification of the accuracy of analytical models (math or computer simulation models) and characterization of the model bias are two essential processes in model validation. Available model validation metrics, whether qualitative or quantitative, do not consider the influence of the number of experimental data for model accuracy check. In addition, quantitative measure from the validation metric does not directly reflect the level of model accuracy, i.e. from 0% to 100%, especially when there is a lack of experimental data. If the original model prediction does not satisfy accuracy criteria compared to the experimental data, instead of revising the model conceptually, characterization of the model bias may be a more practical approach to improve the model accuracy because there is probably no ideal model which can predict the actual physical system with no error. So far, there is a lack of effective approaches that can accurately characterize the model bias for multiple dynamic system responses. To overcome these limitations, the first objective of this study is to develop a model validation metric for model accuracy check considering different number of experimental data. Specifically, a validation metric using the Bhattacharya distance (B-distance) is proposed with three notable benefits. First of all, the metric directly compares the distributions of two set of uncertain system responses from model prediction and experiment rather than the distribution parameters (e.g. mean and variance). Second, the B-distance quantitatively measures the degree of accuracy from 0% to 100% between the distributions of the uncertain system responses. Third, reference accuracy metric with respect to different number of experimental data can be effectively obtained so that hypothesis test can be performed to identify whether the two distributions are identical or not in a probability manner. The second objective of this study is to propose an effective approach to accurately characterize the model bias for dynamic system responses. Specially, the model bias is represented by a generic random process, where realizations of the model bias at each time step could follow arbitrary distributions. Instead of using the traditional Bayesian or Maximum Likelihood Estimation (MLE) approach, we propose a novel and efficient approach to identify the model bias using a generic random process modeling technique. A vehicle safety system with 11 dynamic system responses is used to demonstrate the effectiveness of the proposed approach."
340,"Each year, bone metabolic diseases affect millions of people of all ages, genders, and races. Common diseases such as osteopenia and osteoporosis result from the disruption of the bone remodeling process and can place an individual at a serious fracture risk. Bone remodeling is the complex process by which old bone is replaced with new tissue. This process occurs continuously in the body and is carried out by bone cells that are regulated by numerous metabolic and mechanical factors. The remodeling process provides for various functions such as adaptation to mechanical loading, damage repair, and mineral homeostasis. An improved understanding of this process is necessary to identify patients at risk of bone disease and to assess appropriate treatment protocols."
341,"In this paper a novel approach is proposed to solve the reliability based design optimization (RBDO) problem, which is translated into a single-level problem using Karush-Kuhn-Tucker (KKT) conditions. The transformation of a bi-level RBDO problem into a single-level problem using KKT conditions introduces several equality constraints in the single-level problem definition. Presence of multiple equality constraints poses numerical difficulty to the gradient based optimizers, hence a robust algorithm to solve the single-level RBDO problem is proposed in this paper using an alternative approach. The proposed approach uses an exterior penalty based cross-entropy (CE) method to solve the uni-level RBDO problem. This approach is shown to be robust in handling equality constraints. The three example problems solved in this paper also shows that the algorithm works well with different starting points used for the design variables."
342,"This work presents a novel method for designing crashworthy structures with controlled energy absorption based on the use of compliant mechanisms. This method helps in introducing flexibility at desired locations within the structure, which in turn reduces the peak force at the expense of a reasonable increase in intrusion. For this purpose, the given design domain is divided into two subdomains: flexible (FSD) and stiff (SSD) subdomains. The design in the flexible subdomain is governed by the compliant mechanism synthesis approach for which output ports are defined at the interface between the two subdomains. These output ports aid in defining potential load paths and help the user make better use of a given design space. The design in the stiff subdomain is governed by the principle of a fully-stressed design for which material is distributed to achieve uniform energy distribution within the design space. Together, FSD and SSD provide for a combination of flexibility and stiffness in the structure, which is desirable for most crash applications."
343,"Topography optimization is an innovative technique that can significantly improve the response of certain type of structures. The most challenging aspect of topography optimization is the sensitivity analysis. In this manuscript two methods to approximate the sensitivities for problems in topography optimization are introduced. The gradient is supplanted with either a stochastic approximation, or a physical approximation. Initially, an overview of the state-of-the-art in topography optimization is presented, and some key issues are explored. Subsequently, the technique is outlined, and the proposed methods are introduced. Furthermore, a numerical example in which a structure composed of shell elements is subject to a blast load is provided. This example is solved employing stochastic gradient approximation, and approximate gradient. They are compared to the widely used finite differences approximation. It is possible to observe that the proposed method significantly reduces the computational effort required to solve the problem, while considerably improving the objective function."
344,"Modern green products must be easy to disassemble. Selective disassembly is used to access and remove specific product components for reuse, recycling, or remanufacturing. Early related studies developed various heuristic or graph-based approaches for single-target selective disassembly. More recent research has progressed from single-target to multiple-target disassembly, but disassembly model complexity and multiple constraints, such as fastener constraints and disassembly directions, still have not been considered thoroughly. In this study, a new graph-based method using disassembly sequence structure graphs (DSSGs) was developed for multiple-target selective disassembly sequence planning. The DSSGs are built using expert rules, which eliminate unrealistic solutions and minimize graph size, which reduces searching time. Two or more DSSGs are combined into one DSSG for accessing and removing multiple target components. In addition, a genetic algorithm is used to decode graphical DSSG information into disassembly sequences and optimize the results. Using a GA to optimize results also reduces searching time and improves overall performance, with respect to finding global optimal solutions. Case studies show that the developed method can efficiently find realistic near-optimal multiple-target selective disassembly sequences for complex products."
345,"The Predictive Product Lifecycle Design (PPLD) model that is proposed in this paper enables a company to optimize its product lifecycle design strategy by considering pre-life and end-of-life at the initial design stage. By combining lifecycle design and predictive trend mining technique, the PPLD model can reflect both new and remanufactured product market demands, capture hidden and upcoming trends, and finally provide an optimal lifecycle design strategy in order to maximize profit over the span of the whole lifecycle. The outcomes are lifecycle design strategies such as product design features, the need for buy-backs at the end of its life, and the quantity of products remanufacturing. The developed model is illustrated with an example of a cell phone lifecycle design. The result clearly shows the benefit of the model when compared to a traditional Pre-life design model. The benefit would be increased profitability, while saving more natural resources and reducing wastes for manufacturers own purposes."
346,"Extending the life of a product through remanufacturing or refurbishing is generally regarded as being “greener” than new production, as it avoids the resource consumption and waste generation associated with the new production; however, when considering improved performance of new products, extending the lifetime of less efficient, less productive old products may not always be greener than new production. Shortening the product’s life by early replacement with a newer, more efficient product can be a better option, as “Cash-for-Clunker” programs have claimed. This paper presents a generic model to decide optimal lifetime strategy for a product. Three different lifetime strategies—to maintain, to extend, and to shorten the current lifetime—are compared from an environmental perspective, for a given time horizon. The average environmental impact per unit production is used as the basis for a fair comparison. Applied with an optimization technique, the model can also identify the optimal lifetime length of a product. To illustrate, the developed model is applied to an example of complex heavy-duty, off-road equipment."
347,"In recent years, environmentally conscious design has become a fundamental approach for industries which have to consider the variable environment during the design process. Waste management is one of the most important aspects to be handled, to reduce the disposal in landfills and to encourage the sustainable 3R approach: Reuse, Recycling and Remanufacturing. Product disassembly is an essential phase of the product lifecycle, necessary to evaluate the End-of-Life (EoL) strategies and to reduce environmental impact. In order to minimize the impact on production and costs it is very important to consider EoL scenarios during the embodiment design phase, when designer’s decisions influence product structure. Design for Disassembly (DFD) is a powerful method to reduce disassembly time and costs. However, there are no useful tools which provide guidelines to improve the product disassemblability or promote specific EoL scenarios."
348,"Rolling element bearings operation depends on some variables contributing to the machine element performance. The present work attempts to improve the performance of rolling element bearings through the increase of fatigue life and the reduction of bearing wear. The formulation is based on Elastohydrodynamic to maximize the realistically evaluated minimum film thickness without significant increase in viscous friction torque. The multiobjective problem can then be stated as maximization of minimum film thickness and minimization of total friction torque. Design vectors are reduced in the present study relative to previous studies as some variables are considered as dependent variables. A new important parameter is introduced in this study as a design variable, which is the viscosity of lubricant ("
349,"This paper describes a design automation approach that combines various optimization research and artificial intelligence methods for synthesizing fluid networks. Unlike traditional software tools available today, this approach does not rely on having any predefined network topology to design and optimize its networks. PipeSynth generates its designs by using only desired port locations, and the desired fluid properties at each of those ports. An ideal network is found by optimizing the number and connectivity of pipes and pipe fittings, the size and length of each pipe, and the size and orientation of each fitting. A Uniform-Cost-Search is used for topology optimization along with a combination of non-gradient based optimization methods for parametric optimization. PipeSynth demonstrates how advances in automated design can enable engineers to manage much more complex fluid network problems. PipeSynth uses a unique representation of fluid networks that synthesizes and optimizes networks one pipe at a time, in three-dimensional space. PipeSynth has successfully solved several problems containing multiple interlaced networks concurrently with multiple inputs and outputs. PipeSynth shows the power of automated design and optimization in producing solutions more effectively and efficiently than traditional design approaches."
350,"Energy based topology optimization method has been used in the design of compliant mechanisms for many years. Although many successful examples from the energy based topology optimization have been presented, optimized configurations of these designs are often very similar to their rigid linkage counterparts except using compliant joints in place of rigid links. It is obvious that these complaint joints will endure large deformations under the applied forces in order to perform the specified motions and the large deformation will produce high stress which is very undesirable in compliant mechanism design. In this paper, a strain based topology optimization method is proposed to avoid localized high deformation design which is one of the drawbacks using strain energy formulation. Therefore, instead of minimizing the strain energy for structural rigidity, a global effective strain functional is minimized in order to distribute the deformation within the entire mechanism while maximizing the structural rigidity. Furthermore, the physical programming method is adopted to accommodate both flexibility and rigidity design objectives. Comparisons of design examples from both the strain energy based topology optimization and the strain based method are presented and discussed."
351,"The design requirements of a low rolling loss non-pneumatic wheel are determined through a systematic optimization approach. In order to reduce the rolling resistance, linear elastic materials are considered instead of elastomers. To achieve an adequate compliance level, a metamaterial needs to be designed. The required metamaterial properties are determined as a result of an optimization where the metamaterial tensor components as well as the geometric dimensions are the design variables. This way the metamaterial can be designed such that the overall behavior of the non pneumatic wheel achieves the best performance in terms of compliance and contact patch pressure distribution. The resulting constitutive metamaterial properties of the shear layer can be used as prescribed constitutive properties to tailor the periodic mesostructure of a material by means of topology optimization."
352,"This paper presents a continuum-based approach for multi-objective topology optimization of multi-component structures. Objectives include minimization of compliance, weight and as cost of assembly and manufacturing. Decision variables are partitioned into two main groups: those pertaining to material allocation within a design domain (base topology problem), and those pertaining to decomposition of a monolithic structure into multiple components (joint allocation problem). Generally speaking, the two problems are coupled in the sense that the decomposition of an optimal monolithic structure is not always guaranteed to produce an optimal multi-component structure. However, for spot-welded sheet-metal structures (such as those often found in automotive applications), certain assumptions can be about the performance of a monolithic structure that favor the adoption of a two-stage approach that decouples the base topology and joint allocation problems. A multi-objective genetic algorithm (GA) is used throughout the studies in this paper. While the problem decoupling in two-stage approaches significantly reduces the size of the search space and allows better performance of the GA, the size of the search space can still be quite enormous in the second stage. To further improve the performance, we propose a new mutation operator based on decomposition templates and localized joints morphing. A cantilever-loaded structure is then used as a metric to study and compare various setups of single and two-stage GA approaches."
353,"Considering usage context attributes in choice modeling has been shown to be important when product performance highly depends on the usage context. To build a reliable choice model, it is critical to first understand the relationship between usage context attributes and customer profile attributes, then to identify the market segmentation characterized by both sets of attributes, and finally to construct a choice model by integrating data from multiple sources. This is a complex procedure especially when a large number of customer attributes are potentially influential to the product choice. Using the hybrid electric vehicle (HEV) as an example, this paper presents a systematic procedure and the associated data analysis techniques for implementing each of the above steps. Usage context and customer profile attributes extracted from both National Household Travel Survey (NHTS) and Vehicle Quality Survey (VQS) data are first analyzed to understand the relationship between usage context attributes and customer profile attributes. Next the principal component analysis is utilized to identify the key characteristics of hybrid vehicle drivers, and to determine the market segmentations of HEV and the critical attributes to include in choice models. Before the two sets of data are combined for choice modeling, statistical analysis is used to test the compatibility of the two datasets. A pooled choice model created by incorporating usage context attributes illustrates the benefits of context-based choice modeling using data from multiple sources. Even though NHTS and VQS have been used in the literature to study transportation patterns and vehicle quality ratings, respectively, this work is the first to explore how they may be used together to benefit the study of customer preference for HEVs."
354,"Offering increased variety in a market is one method of capturing greater market share. However, we generally observe diminishing marginal returns in share as the size of the product line is increased. Leveraging commonality is a means of offsetting this constraint as it leads to reductions in manufacturing costs and build complexity. Product platforms strive to capitalize on the naturally occurring phenomena that yield commonality in a product line. The structure of design variable values of individually optimized products create opportunities for commonality in a bottom-up platform, while a top-down platform discovers opportunities for commonality through similarity in customer preferences. This paper explores the effect of changing the number of products, and commonality between those products, on market share. Results from designing a varying number of products independently are leveraged to create a bottom-up product platform. A top-down product platform approach based on a heterogeneous discrete choice model and a multiobjective genetic algorithm is presented that allow commonality decisions and product configuration to occur simultaneously. Using the platforming techniques presented in this paper, it is shown that the top-down platforming approach allows for more well-informed platformed design by providing knowledge of the tradeoff between commonality and market share."
355,"When using conjoint studies for market-based design, two model types can be fit to represent the heterogeneity present in a target market, discrete or continuous. In this paper, data from a choice-based conjoint study with 2275 respondents is analyzed for a 19-attribute combinatorial design problem with over 1 billion possible product configurations. Customer preferences are inferred from the choice task data using both representations of heterogeneity. The hierarchical Bayes mixed logit model exemplifies the continuous representation of heterogeneity, while the latent class multinomial logit model corresponds to the discrete representation. Product line solutions are generated by each of these model forms and are then explored to determine why differences are observed in both product solutions and market share estimates. These results reveal some potential limitations of the Latent Class model in the masking of preference heterogeneity. Finally, the ramifications of these results on the market-based design process are discussed."
356,"The idea of automatic mechanism synthesis is to find an optimal linkage type as well as its geometric dimensions for a given problem without using any pre-determined linkage type. As the first step towards the automatic mechanism synthesis, the authors proposed the use of unified planar linkage consisting of rigid blocks connected by stiffness-varying zero-length springs and formulated the synthesis problem as the iterative design optimization problem. In this investigation, we extend the automatic mechanism synthesis idea to more realistic large-sized problems by resolving several numerical difficulties observed in the earlier formulation such as instable convergence and many local optima. In particular, the objective and constraint functions in the optimization formulation are newly selected. The rationale for choosing such functions is given and the effectiveness of the proposed problem formulation is verified by designing planar mechanisms of complete paths."
357,"The isogeometric method is very effective in shape design optimization due to its effectiveness through the easy design parameterization and accurate sensitivities considering the higher order geometric terms. Due to non-interpolatory property of the NUBRS basis functions, however, the treatment of essential boundary condition is not as straightforward in the isogeometric analysis as in the finite element analysis. Taking advantages of the transformation method developed in meshfree methods, we investigate the isogeometric shape sensitivity analysis with the treatment of essential boundary conditions. Using the property that isogeometric basis functions do not depend on design changes, the transformed shape sensitivity equation is developed and verified for the problem having the essential boundary conditions. Numerical costs to construct the transformed basis function are not as much as the meshfree methods due to the NURBS property that only boundary nodes have their supports on the boundary. Through demonstrative numerical examples having the essential boundary conditions, the effectiveness of proposed design sensitivity analysis is verified."
358,"We have developed a multiscale design sensitivity analysis method for transient dynamics using a bridging scale method by a projection operator for scale decomposition. Employing a mass-weighted projection operator, we can fully decouple the equations of motion into fine and coarse scales using the orthogonal property of complimentary projector to the mass matrix. Thus, independent solvers in response analyses can be utilized for the fine scale analysis of molecular dynamic (MD) and the coarse scale analysis of finite element analysis. To reduce the size of problems and to improve the computational efficiency, a generalized Langevin equation is used for a localized MD analysis. Through demonstrative numerical examples, it turns out that the derived sensitivity analysis method is accurate and efficient compared with finite difference sensitivity."
359,"It is computationally expensive to evaluate the overall system level reliability when several interacting failure modes are present. Therefore, it is even more expensive to optimize considering the system level reliability that accounts for the interactions between failure modes. In this paper, we decompose the system level reliability based optimization problem using surrogates into less expensive problems with fixed risk allocation for each failure mode. In addition, the fixed risk allocation problem is transformed from a purely probabilistic problem to a deterministic one through an iterative process of updating safety factors to limit the number of calls to evaluate the reliability. We found that the number of calls to the simulation to evaluate the system level reliability was reduced by 77% with this methodology."
360,"Random field is a generalization of a stochastic field, of which randomness can be characterized as a function of spatial variables. Examples of the random field can often be found as a geometry, material, and process variation in engineering products and processes. It has been widely acknowledged that consideration of the random field is quite significant to accurately predict variability in system performances. However, current approaches for characterizing the random field can only be applied to the situation with sufficient random field data sets and are not suitable to most engineering problems where the data sets are insufficient. The contribution of this paper is to model the random field based on the insufficient data sets such that sufficient data sets can be simulated or generated according to the random field modeling. Therefore, available random field characterization approaches and probability analysis methods can be used for probability analysis and design of many engineering problems with the lack of random field data sets. The proposed random field modeling is composed of two technical components including: 1) a Bayesian updating approach using the Markov Chain Monte Carlo (MCMC) method for modeling the random field based on available random field data sets; and 2) a Bayesian Copula dependence modeling approach for modeling statistical dependence of random field realizations at different measurement locations. Three examples including a mathematical problem, a heat generation problem of the Lithium-ion battery, and a refrigerator assembly problem are used to demonstrate the effectiveness of the proposed approach."
361,"The purpose of this paper is to show equivalence between continuum and discrete formulations in sensitivity analysis when a linear velocity field is used. Shape sensitivity formulations are presented when the body forces and surface tractions depend on shape design variables. Especially, the continuum-discrete (C-D) and discrete-discrete (D-D) approaches are compared in detail. It is shown that the two methods are theoretically and numerically equivalent when the same discretization, numerical integration, and linear design velocity fields are used. The accuracy of sensitivity calculation is demonstrated using a cantilevered beam under uniform pressure and an arch dam crown cantilever under gravity and hydrostatic loading at the upstream face of the structure. It is shown that the sensitivity results are consistent with finite difference results, but different from the analytical sensitivity due to discretization and approximation errors of numerical analysis."
362,"Reliability analysis plays an essential role in the development of structural systems. However, commonly used reliability analysis methods suffer from either the curse of dimensionality or the lack of accuracy in many structural problems. This paper presents an asymmetric dimension-adaptive tensor-product (ADATP) method to resolve the difficulties of existing reliability analysis methods. The proposed method leverages three ideas: (i) an asymmetric dimension-adaptive scheme to efficiently build the tensor-product interpolation considering both directional and dimensional importance, (ii) a hierarchical interpolation scheme using either piecewise multi-linear basis functions or cubic Lagrange splines, (iii) a hierarchical surplus as an error indicator to automatically detect the highly nonliner regions in a random space and adaptively refine the collocation points in these regions. The proposed method has three distinct features for reliability analysis: (a) automatically detecting and adaptively reproducing tri- and higher-variate interactions, (b) greatly alleviating the curse of dimensionality, and (c) no need of response sensitivities. Several mathematical and engineering problems involving high nonlinearity are used to demonstrate the effectiveness of the ADATP method."
363,"In this paper, comparisons of the design optimization of ball grid array packaging geometry based on the elastic and viscoelastic material properties are made. Six geometric dimensions of the packaging are chosen as input variables. Molding compound and substrate are modeled as elastic and viscoelastic, respectively. Viscoplastic finite element analyses are performed to calculate the strain energy densities (SED) of the eutectic solder balls. Robust design optimizations to minimize SED are carried out, which accounts for the variance of the parameters via Kriging dimension reduction method. Optimum solutions are compared with those by the Taguchi method. It is found that the effects of the packaging geometry on the solder ball reliability are significant, and the optimization results are different depending on the materials modeling."
364,"The objective of this study is to design spacers using fluid topology optimization in 2D crossflow Reverse Osmosis (RO) membrane channel to improve the performance of RO processes. This study is an initial attempt to apply topology optimization to designing spacers in RO membrane channel. The performance was evaluated by the quantity of permeate flux penetrating both upper and lower membrane surfaces. A coupled Navier-Stokes and Convection-Diffusion model was employed to calculate the permeate flux. To get reliable solutions, stabilization methods were employed with standard finite element method. The nine reference models which consist of the combination of circle, rectangular, triangle shape and zigzag, cavity, submerge configuration of spacers were simulated. Such models were compared with new model designed by topology optimization. The permeate flux at both membrane surfaces was determined as an objective function. In addition, permissible pressure drop along the channel and spacer volume were used as constraints. As a result of topology optimization as the permissible pressure drop changes in channel, characteristics of spacer design development was founded. Spacer design based on topology optimization was reconstructed to a simple one considering manufactuability and characteristics of development spacer design. When a simplified design was compared with previous 9 models, new design has a better performance in terms of permeate flux and wall concentration at membrane surface."
365,"In order to face the rapid changing market requirements, companies need methods and tools in order to implement flexibility over the whole product development process, from ideation to manufacturing. The proposed approach targets the development of a method to support decision making in product redesign activities. Design alternatives and product modifications can be rapidly evaluated in terms of feasibility, cost and time. The approach is based on a product structure multilevel representation, where functions, modules, assemblies and components are strictly interrelated. The representation allows criteria and rules in order to efficiently connect the elements within the same level and among levels. Such connections will contain the values to estimate the impact of analyzed product changes. In this way the structure will serve as evaluation tool in the early redesign phases. In order to manage and interact with the structure a software tool has been developed, called Modulor. This system allows modeling the product representation and rapidly evaluating the consequences in terms of change propagation. The tool was tested within the R&D department of a large sized company producing household appliances. Pilot studies have revealed shorter redesign cycles thanks to a broader understanding of implications while deciding among several implementation solutions."
366,"In the New Product Development processes, there are usually interacting multiple actors with multiple design objectives. Design objectives of the actors can be contradictorily related and design modifications related to some objectives may generate negative impacts on the other actors. Therefore conflicts may occur. We present a multi-actor design platform that is capable of preventing potential conflicts with ensuring the satisfaction and progress of the actors at a certain level. An indicator of wellbeing is developed in order to indentify the satisfaction and progress states of the actors. We use Set-based design approach and Constraint Satisfaction Problem solving techniques to deal with the multi-actor, multi-objective design problem. We apply our collaborative design approach to a multi-actor multiobjective engineering design problem of a hollow cylindrical cantilever beam with a load applied at the unsupported end."
367,"In distributed design processes, individual design subsystems have local control over design variables and seek to satisfy their own individual objectives, which may also be influenced by some system level objectives. The resulting network of coupled subsystems will either converge to a stable equilibrium, or diverge in an unstable manner. In this paper, we study the dependence of system stability on the solution process architecture. The solution process architecture describes how the design subsystems are ordered and can be either sequential, parallel, or a hybrid that incorporates both parallel and sequential elements. In this paper we demonstrate that the stability of a distributed design system does indeed depend on the solution process architecture chosen and we create a general process architecture model based on linear systems theory. The model allows the stability of equilibrium solutions to be analyzed for distributed design systems by converting any process architecture into an equivalent parallel representation. Moreover, we show that this approach can accurately predict when the equilibrium is unstable and the system divergent when previous models suggest the system is convergent."
368,"This study offers a new method for understanding the likelihood of acceptable fit for users of adjustable products and environments and is a useful tool for aiding the designer in making decisions about problems involving human variability. Accommodation, which describes the ability of a user to interact with a device or environment in a preferred way, is a key product performance metric. Methods that offer a better understanding of accommodation of broad user populations would allow for the design of products that are more cost-effective, safer, and/or lead to greater levels of customer satisfaction. This work uses parametric studies to explore the characteristics of a target user population and the probability of accommodating individuals of a given body size. Performance regions are identified in both the problem’s design space (the product dimensions under consideration) and the anthropometry space of the target population (the relevant body dimensions of product users). The existence of probability contours is a result of outcome uncertainty due to anthropometry-independent user preference, and the analysis is achieved by assessing binary accommodation of individuals using a “virtual fit” method with many iterations. Two case studies, one univariate and one bivariate in both performance and anthropometry spaces, are presented. An important outcome of the decision making framework described in this work is the ability to intuitively gauge who in the population of target users will be disaccommodated by a design and how to improve overall accommodation."
369,"Preferences are a formal way to represent a designer’s choices when assigning priorities for a set of possible design alternatives within the context of the design process. A design team’s preferences can change over the life of project, and knowledge of this evolution can be useful for understanding a team’s rationale as well as its confidence in a decision. This paper presents a “sliding window” approach (SPPT) to the extraction of preference related information from transcribed design team discussion. The approach suggested in this paper can assess design preferences over time with a finer granularity than a previous approach known as PPT, and removes perturbations that occur when there is little design team discussion. Both SPPT and PPT were applied to a discussion transcript. Results show good consistency among SPPT, PPT and survey results. SPPT is also able to detect more changes in design team preference."
377,"The eco-design of complex industrial systems becomes a major issue for the concerned companies. Life Cycle Assessment (LCA) in particular is more adapted to “classical” products such as consumer goods. Performing LCA of such systems requires some thoughts to ensure significant results, for example concerning data granularity and quality. This article proposes a Life Cycle Assessment of an Alstom Grid AC/DC conversion substation for the aluminium industry. This LCA integrates first answers to the previous limitations. Very interesting results permit to orient the eco-design strategy of the company. Moreover they lead to imagine original ways to configure the system. Thus a first and simple parametric LCA model is proposed: four different electrical sources are used to analyze the sensitivity of the design factors to the environmental impacts of the substation. Based on design of experiments, a more advanced model of such a tool would permit to identify the best configuration in terms of environmental performance, costs and reliability."
378,"The need for reconfigurable manufacturing systems has long been recognized as a key factor to gain the necessary flexibility for economically producing customized products. Automation of the reconfiguration processes is a challenge both on the hardware and the software level. Addressing this issue in the field of fixture design, a new reconfigurable fixture device for a CNC milling machine has been developed. The developed vise contains interchangeable jaws enabling the secure fixture of a wide variety of workpiece geometries. To enable automated reconfiguration, a reasoning system is needed that can determine feasible fixture configurations based on the given workpiece and part as well as the available fixture components. In this paper, an ontology for representing fixture design and reconfiguration knowledge for a specific reconfigurable vise-type fixture is introduced. The creation of the ontology is based on a systematic building methodology to identify necessary concepts, attributes and relations within the domain. Using Description Logic as the representation formalism, core concept definitions and relations are formalized to evaluate the correctness and consistency of the ontology. The ontology is validated both on the informal and on the formal level by its ability to find feasible fixture configurations, i.e. appropriate jaw pairs to fix example workpieces. The paper concludes with a discussion of the results and future work."
379,"Reconfigurable and multifunctional products are breeds of products that cater to the increased diversification of customer needs. Unlike single-state static products which can perform only one primary function, these products cater to different customer needs by performing more than one function with or without changing their configuration. However, there is a lack of systematic methods to support the conceptual task of combining two existing single-state products into an integrated product that provides multiple functions. In this paper, a function based approach is proposed which provides more rigorous support to assess the feasibility of integrating two products. The function structures of the existing products are combined to obtain the overall function structure of the reconfigurable product. Function sharing, based on quantified functional similarity, is proposed and applied to identify functions that can be shared by the same component. The information obtained from the function structure is then mapped to the components of two existing products to analyze their roles in the final reconfigurable product architecture. A case study illustrates the proposed approach by analyzing the integration of a power drill and a dust buster."
381,"Growing concerns for the environment should make every designer more carefully consider product design for the life cycle (DFLC). Although modularity is recognized for its potential to incorporate life cycle considerations into product architecture design, most modular design methods in the literature concentrate on generating highly-modular product architectures but lack the capability for assessing life cycle consequences of these modules in a supply chain. This paper proposes a methodology to find a robust modular architecture with minimal life cycle costs and environmental impacts at the design configuration stage. The objective of the proposed methodology is not to maximize modularity, but to adopt life cycle costing and life cycle assessment of a product in a closed-loop supply chain to identify the most beneficial modular structure. Further, capacity influence of the existing processing facilities in the supply chain on life cycle costs and environmental impacts is evaluated and discussed in this paper."
382,"In this paper a Multi-Level System design (MLS) algorithm is presented and utilized for a wind turbine system analysis. The MLS guides the decision making process for designing a complex system where many alternatives and many mutually competing objectives and disciplines need to be considered and evaluated. Mathematical relationships between the design variables and the multiple discipline performance objectives are developed adaptively as the various design considerations are evaluated and as the design is being evolved. These relationships are employed for rewarding performance improvement during the decision making process by allocating more resources and influence to the disciplines which exhibit the improvement. Simulation tools developed by the National Renewable Energy Laboratory (NREL) are employed in the wind turbine design analysis. The Cost Of Energy (COE) comprises the overall system level objective, while performance improvements at two technical design disciplines are pursued at the same time. The optimal design of the blade geometry for maximum Annual Energy Production (AEP), and the structural design of the blade for minimum bending moment at the root of the blade comprise the two technical design disciplines. Scalar metamodels are developed for linking the design variables with the performance metrics associated with the design of the blade geometry. Main characteristics of the wind turbine, namely, the rotor diameter, the rotational speed, the maximum rated power, the hub height, the structural characteristics of the blade, and the geometric characteristics of the blade (distribution of thickness, twist angle, and chord) are employed as design variables for the overall design analysis. The optimization results and the physical insight which can be gained through a sensitivity analysis for the optimal configuration are presented and discussed."
383,"Current wind farm layout optimization research focuses on advancing optimization methods. The research includes the assumption that a continuous piece of land is readily available. In reality, wind farm development projects rely on the permission of landowners for success. When a viable wind farm site location is identified, local residents are approached for permission to build turbines on their land, typically in exchange for monetary compensation. Landowners play a crucial role on the development of a wind farm and some land parcels are more important to the success of the project than others. In order to advance the research on wind farm optimization, this paper relaxes the assumption that a continuous piece of land is available, developing a novel approach that includes landowners’ decisions on whether or not to participate in the project. The optimization results of this new approach show that, for a specific wind farm layout case, we can identify the most crucial landowners and the optimal positions of turbines prior to the negotiation process with landowners. Using this approach, a site developer can spend more resources on persuading these most-important landowners to take part in the project, or approach them in a personalized manner. This will ultimately increase the efficiency of wind farm projects, saving time and money in the development stages."
384,"A plug-in hybrid electric vehicle (PHEV) relies on relatively larger storage batteries than conventional hybrid electric vehicles. The characteristics of PHEV batteries, as well as hybridization of the PHEV battery with the engine and electric motor, play an important role in the design and potential adoption of PHEVs. To exhaustively evaluate all the possible combinations of available types of batteries, motors and engines, the total computational time is prohibitive. This work proposed an integrated optimal design strategy to address this problem. The recently developed Pareto set pursuing (PSP) multi-objective optimization approach is employed to perform optimal hybridization. Each PHEV with chosen battery, motor and engine is designed for optimal component sizing using the Powertrain System Analysis Toolkit (PSAT) software. The methodology is demonstrated with the Toyota Prius PHEV20: PHEV version sized for 20 miles (32.1 km) of all electric range (AER). Fuel economy, operating cost, and green house gases emissions are simultaneously optimized from 4,480 possible combinations of design parameters: 20 batteries, 14 motors, and 16 engines. The hybridization optimization is performed on two different drive cycles—Urban dynamometer driving schedule (UDDS) and Winnipeg weekday duty cycle (WWDC). It was found that battery, motor, and engine work collectively to define an optimal hybridization scheme and the optimal hybridization scheme varies with each driving cycle. The proposed method and software platform could be applied to optimize other powertrain designs."
385,"Many factors must be addressed when designing infrastructure health monitoring systems. Structures in remote locations or with limited accessibility make the requirements for these systems unique and challenging. For locations where connection to the power grid is difficult or impossible, monitoring system life is severely limited by battery technology. Alternatively, an energy harvesting power supply can make the monitoring system independent of the grid while increasing capabilities and lifetime beyond what is possible with current battery technology. This paper discusses a design and development methodology for developing energy harvesting aspects of a health monitoring system. The system comprises a sensor module that monitors the health of the structure, an on-site processing module that analyzes the data, and a wireless communication module that transmits the data. The method is demonstrated by examples of energy harvesting systems for a bridge monitoring application, using solar, wind, and vibration energy harvesters to provide power to a wireless network, local data processors, and strain gauges. Theoretical feasibility of energy harvesting in these domains has been previously demonstrated. The examples described in this paper validate the feasibility previously calculated as well as illustrate shortcomings in the current technology that inhibit potential implementation. The examples also show areas where innovation is needed to continue to advance the technology of energy harvesting in this application on infrastructure."
386,"Optimal design of an electrical microgrid and sizing of its components seeks to balance capital investment with expected operational cost while meeting performance requirements. Calculating operational cost requires scheduling each microgrid component over some time period (dispatching) for each design evaluated. Heuristic or rule-based dispatch strategies typically consider only single time instances and are computationally efficient but do not include scheduling energy storage for future time periods. In this paper, we propose to optimize microgrid designs using forward-looking optimal dispatch for future energy storage planning. We present a case study of an ‘islanded’ military base microgrid with renewable and non-renewable electricity generation, battery storage, and plug-in vehicles with electrical export power capability. The optimal design and forward-looking dispatch strategy are compared to results obtained using the publicly available rule-based dispatch strategy in HOMER Energy software. Results show that the forward-looking strategy uses storage batteries to plan for future energy shortfalls rather than simply as a buffer for variable renewable energy supply, resulting in a 7.8% reduction in predicted fuel use. For the given cost assumptions, sensitivity analysis of the optimal design with respect to fuel price shows that investment in renewable energy technology is justified at prices greater than $5 per gallon ($1.32/liter) with an attendant reduction in fuel use of 3–30%."
387,"Solar tower with heliostat mirrors is an established technology for utility-scale solar energy harvesting. The setup has several advantages such as the capability to reach high temperature, modularity and ease of maintenance of the heliostats, containment of the high temperature zone, as well as overall low cost per harvested energy. Downscaling to medium and small scale applications is a desirable goal in order to attract more users of the technology. However, the downscaling often does not turn out economically feasible while using flat mirror heliostats, which are the norm in utility-scale systems. This is mainly due to the need to preserve the "
388,"Solar tower with heliostat mirrors is one of the established setups for utility-scale solar energy harvesting. Advantages of the setup include the capability to reach high temperature, modularity and ease of maintenance for the heliostats, containment of the high temperature zone atop the tower, as well as overall low cost per unit energy. However, downscaling to medium or small scale applications often does not turn out economically feasible with flat mirror heliostats that are the norm in utility-scale systems. This is mainly due to the need to preserve the solar concentration ratio, which in turn means the "
389,"The development of large scale wind farms that can produce energy at a cost comparable to that of other conventional energy resources presents significant challenges to today’s wind energy industry. The consideration of the key design and environmental factors that influence the performance of a wind farm is a crucial part of the solution to this challenge. In this paper, we develop a methodology to account for the configuration of the farm land (length-to-breadth ratio and North-South-East-West orientation) within the scope of wind farm optimization. This approach appropriately captures the correlation between the (i) land configuration, (ii) the farm layout, and (iii) the selection of turbines-types. Simultaneous optimization of the farm layout and turbine selection is performed to minimize the Cost of Energy (COE), for a set of sample land configurations. The optimized COE and farm efficiency are then represented as functions of the land aspect ratio and the land orientation. To this end, we apply a recently developed response surface method known as the Reliability-Based Hybrid Functions. The overall wind farm design methodology is applied to design a 25MW farm in North Dakota. This case study provides helpful insights into the influence of the land configuration on the optimum farm performance that can be obtained for a particular site."
390,"Convergence products are multifunctional designs which are changing the way consumers use existing functionalities. Manufacturers’ ventures in developing convergence products abound in the marketplace. Smartphones, tablet computers, internet TV, are just a few examples. The complexity of designing a convergence product can differ significantly from that of single function products which most research in “Design for Market Systems” aims at. In this paper, a new customer-driven approach for designing convergence products is proposed to address the following issues: (i) a design representation scheme that considers information from design solutions used in existing products. The representation facilitates the coupling of and combining multiple functionalities; (ii) a hierarchical Bayes model that evaluates consumers’ heterogeneous choices while revealing how usage of multiple functionalities impacts consumers’ preferences; and (iii) design metrics which help evaluate profitability of design alternatives and account for future market penetration given evolving consumer preferences. An example problem for designing a tablet computer is used to demonstrate the proposed approach. The data for the example is collected by conducting a choice-based conjoint survey which yielded 92 responses. The proposed approach is demonstrated with three scenarios differentiated by the consideration of consumer heterogeneity and future market penetration, while comparing how the resulting optimal design solutions for the convergence product differ."
391,"Success in achieving environmental goals is intrinsically dependent on policy decisions, firm decisions, and consumer decisions. Understanding how consumer product adoption jointly depends on policy incentives and firm design decisions is necessary for both firms and governments to make optimal decisions. This paper demonstrates a methodology for assessing the linkage between policy incentives and firm decisions on the level of consumer adoption of a particular technology. A policy optimization is formulated and technology-adoption indifference curves are constructed to allow firms to identify the most profitable direction for product development given the policy environment, and similarly to allow government organizations to set policies that maximize technology adoption given firm decisions. As an example we use the residential solar electricity industry in New South Wales, Australia. Consumer choice is modeled using a mixed logit choice model estimated with hierarchical Bayes techniques from stated preference experiment data."
392,"Acquisition of the customer data for product design selection using conventional customer survey techniques can be a time-consuming and costly undertaking. The aim of this paper is to overcome this limitation by using web based User-Generated Content (UGC) as an alternative to the conventional customer survey techniques. UGC refers to various public media contents created by web users including contents in online customer reviews, blogs, and social networking interactions. So far, there has not been any systematic effort in using UGC in design selection for a customer durable product. Using UGC in product design selection is not an easy task because UGC can be freely expressed and written by customers with little constraints, structure and bounds. As a result, UGC can contain a lot of noise, variability in content and even bias induced by the customers. In order to make use of UGC, this paper develops a systematic methodology for eliciting product attributes from UGC, constructing customer preference models and using these models in design selection. To demonstrate the proposed method, design selection of a smartphone using UGC is considered as an example. It is shown in the example that the proposed method can provide a reasonable estimation of customer preferences while being useful for product design selection."
393,"In new product design, risk averse firms must consider downside risk in addition to expected profitability, since some designs are associated with greater market uncertainty than others. We propose an approach to robust optimal product design for profit maximization by introducing an α-profit metric to manage expected profitability vs. downside risk due to uncertainty in market share predictions. Our goal is to maximize profit at a firm-specified level of risk tolerance. Specifically, we find the design that maximizes the α-profit: the value that the firm has a (1−α) chance of exceeding, given the distribution of possible outcomes. The parameter α∊[0,1] is set by the firm to reflect sensitivity to downside risk (or upside gain), and parametric study of α reveals the sensitivity of optimal design choices to firm risk preference. We account here only for uncertainty of choice model parameter estimates due to finite data sampling when the choice model is assumed to be correctly specified (no misspecification error). We apply the delta method to estimate the mapping from uncertainty in discrete choice model parameters to uncertainty of profit outcomes and identify the estimated α-profit as a closed form function of design decision variables. This process is described for the multinomial logit model, and a case study demonstrates implementation of the method to find the optimal design characteristics of a midsize consumer automobile."
394,"Collaborative products are created when physical components from two or more products are temporarily recombined to form another product capable of performing additional tasks. In this paper, a method for designing collaborative products is introduced. The method identifies a set of products capable of being recombined into a collaborative product. These products are then designed to allow for this recombination. Collaborative products are particularly useful in reducing the cost, weight, and size of poverty-alleviating products—reductions that are valued in the developing world. A simple example of a cabinet maker’s tool shows that a collaborative block plane created from a chisel and sanding block can account for reductions in cost, weight, and size of 44%, 38%, and 44% respectively, when compared to a typical wooden block plane, chisel, and sanding block. Additionally, an example of a collaborative apple peeler is provided to demonstrate scalability of the method. The authors conclude that the method introduced herein provides a new and useful tool to design collaborative products and to assist in engineering-based poverty alleviation."
395,"Designing appropriate technology is becoming more prevalent as engineers have begun to focus more of their attention on the developing world. However, many efforts have failed or been relatively unsuccessful due to design processes that do not focus on sustainability. One author has provided a solid framework by outlining mechanical design basic principles including design for simplicity, analysis of load paths, and use of prototypes. Yet these principles were not presented in a way that makes them applicable to sustainable projects in the developing world. In this paper, these three principles are investigated through two design case studies. The goal was to analyze how well these principles apply to the developing world and whether several hypothesized changes would also be useful. With the principles in mind, a bicycle trailer and taxi were designed for an impoverished community in rural Africa. Based on these designs, it was determined that the three principles analyzed apply to the developing world but should be refocused and presented differently in order to be utilized effectively for sustainability."
396,"Growing awareness of the unique needs and challenges in the developing world has resulted in the development of products for those in poverty. Successful product design focuses development efforts on design principles that are important to a target market. Consequently, the better these principles are understood, the higher the probability is that resulting products will be successful. Recognizing that the identification of these principles is a major challenge, this paper presents a method for identifying them for any target market, but especially for the developing world. The presented methodology uses characteristics of products within the target market to extract information about the underlying design decisions resulting in these characteristics. This information is then used to identify the design principles. To verify the ability of the method to identify these principles, the method is applied to best selling products in the US and then applied to products created for the developing world. The resulting principles from the two markets are then analyzed and compared to highlight the similarities and differences between the identified principles. The authors conclude that the resulting list of principles will enable designers to better design and develop products for the developing world."
397,"Today the primary challenge confronting engineers is to develop clean, sustainable technologies that can meet the needs of all of the world’s people. Traditionally this effort has focused on meeting the needs of the developed world. It is generally assumed that products needed for the developing world already exist or are relatively simple and hence do not require significant engineering design effort. As a consequence, many of the products intended to meet the needs of the poor miss the mark and do not meet their needs. This is particularly true in the design of products and processes intended to address the energy needs of the rural poor. Too often a set of standard assumptions is used, resulting in poor problem definition. And, because the design problem is not well defined, the resulting products and processes fail. Throughout the developing world it is common to find village water and energy projects that have failed. To design products and processes that meet the energy needs of the rural poor, the critical first step in the design process is a detailed in-village study of energy production and consumption dynamics. Quantifying village energy dynamics provides insight into the unfulfilled or unsatisfied needs of the consumer, establishes the design constraints, aids the engineer and the community members in prioritizing needs, and builds trust with the local community. This paper presents a field methodology developed to understand the energy needs of a rural sub-Saharan village of 700 people and discusses how this field methodology was used to establish the design constraints needed for a comprehensive energy solution."
398,"The motivating question for this article is: ‘How should a system level designer allocate resources for auxiliary simulation model refinement while satisfying system level design objectives and ensuring robust process requirements in multiscale systems? Our approach consists of integrating: (i) a robust design method for multiscale systems (ii) an information economics based approach for quantifying the cost-benefit trade-off for mitigating uncertainty in simulation models. Specifically, the focus is on allocating resources for reducing model parameter uncertainty arising due to insufficient data from simulation models. A comprehensive multiscale design problem, the concurrent design of material and product is used for validation. The multiscale system is simulated with models at multiple length and time scales. The accuracy of the simulated performance is determined by the trade-off between computational cost for model refinement and the benefits of mitigated uncertainty from the refined models. System level designers can efficiently allocate resources for sequential simulation model refinement in multiscale systems using this approach."
399,"Recently a periodic surface model was developed to assist geometric construction in computer-aided nano-design. This implicit surface model helps create super-porous nano structures parametrically and support crystal packing. In this paper, we propose a new approach for pathway search in phase transition simulation of crystal structures. The approach relies on the interpolation of periodic loci surface models. Respective periodic plane models are reconstructed from the positions of individual atoms at the initial and final states, and surface correspondence are found. With geometric constraints imposed based on physical and chemical properties of crystals, two surface interpolation methods are used to approximate the intermediate atom positions on the transition pathway in the full search of the minimum energy path. This hybrid approach integrates geometry information in configuration space and physics information to allow for efficient transition pathway search. The methods are demonstrated by examples of FeTi and VO2 ."
400,"There are two critical components of connecting material and structural design in a multiscale design process: (1) relate material processing parameters to the microstructure that arises after mixing, and (2) stochastically characterize and subsequently reconstruct the microstructure to enable automation of material design that scales upward to the structural domain. This work proposes a data-driven framework to address both above components for two-phase materials and presents the algorithmic backbone to such a framework. In line with the two components above, a set of numerical algorithms is presented for characterization and reconstruction of two-phase materials from microscopic images: these include grayscale image binarization, point-correlation and cluster-correlation characterization, and simulated annealing algorithm for microstructure reconstruction. Another set of algorithms is proposed to connect the material processing parameters with the resulting microstructure by mapping nonlinear, nonphysical regression parameters in microstructure correlation functions to a physically based, simple regression model of key material characteristic parameters. This methodology, that relates material design variables to material structure, is crucial for stochastic multiscale design."
401,"There are countless products that perform the same function but are engineered to suit a different scale. Designers are often faced with the problem of taking a solution at one scale and mapping it to another. This frequently happens with design-by-analogy and bioinspired design. Despite various scaling laws for specific systems, there are no global principles for scaling systems, for example from a biological nano-scale to macro-scale. This is likely one of the reasons bioinspired design is difficult. Very often scaling laws assume the same physical principles are being used, but this study of products indicates that a variety of changes occur as scale changes, including changing the physical principles to meet a particular function. Empirical product research was used to determine a set of principles by observing and understanding numerous products to unearth new generalizations. The function a product performs is examined in various scales to view subtle and blatant differences. Principles are then determined. This study provides an initial step in creating new innovative designs based on existing solutions in nature or other products that occur at very different scales. Much further work is needed by studying additional products and bioinspired examples."
404,"Engineering design problems are most frequently characterized by constraints that make them hard to solve and time-consuming. When evolutionary algorithms are used to solve these problems, constraints are often handled with the generic weighted sum method or with techniques specific to the problem at hand. Most commonly, all constraints are evaluated at each generation, and it is also necessary to fine-tune different parameters in order to receive good results, which requires in-depth knowledge of the algorithm. The sequential constraint-handling techniques seem to be a promising alternative, because they do not require all constraints to be evaluated at each iteration and they are easy to implement. They nevertheless require the user to determine the ordering in which those constraints shall be evaluated. Therefore two heuristics that allow finding a satisfying constraint sequence have been developed. Two sequential constraint-handling techniques using the heuristics have been tested against the weighted sum technique with the ten-bar structure benchmark. They both performed better than the weighted sum technique and can therefore be easy to implement, and powerful alternatives for solving engineering design problems."
405,"To reduce the tremendous computational expense of implementing complex simulation and analysis in engineering design, more and more researchers pay attention to the construction of approximation models. The approximation models, also called surrogate models and metamodels, can be utilized to replace simulation and analysis codes for design and optimization. Commonly used metamodeling techniques include response surface methodology (RSM), kriging and radial basis functions (RBF). In this paper, gene expression programming (GEP) algorithm in evolutionary computing is investigated as an alternative technique for approximation. The performance of GEP is examined by its innovative applications to the approximation of mathematical functions and engineering analyses. Compared to RSM, kriging and RBF, GEP is demonstrated to be more accurate for the small sample size. For large sample sets, GEP also shows good approximation accuracy. Additionally, GEP has the best transparency since it can provide explicit and compact function relationships and clear factor contributions. Overall, as a novel metamodeling technique, GEP exhibits great capabilities to provide the accurate approximation of a design space and will have wide applications in engineering design, especially when only a few sample points are selected for approximation."
406,"The objective of this study is to develop a methodology for use in geothermal pipeline route selection. Special emphasis is placed on finding the shortest route and minimizing the visual affects of the pipeline. Two different approaches are taken to solving the problem. In the first method a distance transform algorithm is used both for visual effects ranking and to obtain the optimal path. Subsequently a genetic algorithm is used to modify the route with regards to necessary expansion units. Included in the tool is site selection for separators and pipeline gathering points based on visual effects, incline, inaccessible areas and total distance to boreholes. The second method uses the Non-dominated sorting genetic algorithm II (NSGA II) to obtain the optimal path with regards visual effects, route length and pipeline gradient. This method uses the distance transform ranking method along with constraints on route length to generate the initial population for the genetic algorithm. The methods are implemented for the Hverahlið geothermal area."
407,"Layout design optimization has a significant impact in the design and use of many engineering products and systems. Real-world layout problems are usually considered as complex problems because of the geometry of components, the problem density and the great number of designer’s requirements. Solving these optimization problems is a hard and time consuming task. This paper proposes an interactive modular optimization strategy which allows the designer to find optimal solutions in a short period of calculation time. This generic strategy is based on a genetic algorithm, combined with specific optimization modules. These modules improve the global performances of the algorithm. This approach is adapted to multi-objective optimization problems and interactivity between the designer and the optimization process is used to make a final choice among design alternatives. This optimization strategy is tested on a real-world application which deals with the search of an optimal spatial arrangement of a shelter."
408,"Modeling to Generate Alternatives (MGA) is a technique used to identify variant designs that maximize design space distance from an initial point while satisfying performance loss constraints. Recent work has explored the application of this technique to nonlinear design problems, where the design space was investigated using an exhaustive sampling procedure. While computational cost concerns were noted, the main focus was determining how scaling and distance metric selection influenced alternative discovery. To increase the viability of MGA for engineering design problems, this work looks to reduce the computational overhead needed to identify design alternatives. This paper investigates and quantifies the effectiveness of using previously sampled designs, i.e. a graveyard, from a multiobjective genetic algorithm as a means of reducing computational expense. Computational savings and the expected error are quantified to assess the effectiveness of this approach. These results are compared to other more common “search” techniques; namely Latin hypercube samplings, grid search, and the Nelder-Mead simplex method. The performance of these “search” techniques are subsequently explored in two case study problems — the design of a two bar truss, and an I-beam — to find the most unique alternative design over a range of different thresholds. Results from this work show the graveyard can be used as a way of inexpensively generating alternatives that are close to ideal, especially nearer to the starting design. Additionally, this paper demonstrates that graveyard information can be used to increase the performance of the Nelder-Mead simplex method when searching for alternative designs."
409,"Today’s manufacturing has become global at all aspects of marketing, design, production, distribution, etc. While product family design has been an essential viewpoint for meeting with the demand for product variety, its meaning is becoming more broad and complicated with linking product design with issues on market systems, supply chain, etc. This paper calls such a design situation ‘global product family design,’ and firstly characterizes its components and complexity. Following them, this paper develops a mathematical model for the simultaneous decision problem of module commonalization strategies under the given product architecture and supply chain configuration through selection of manufacturing sites for module production, assembly and final distribution as an instance of the problems. This paper demonstrates some numerical case studies for ascertaining the validity and promise of the developed mathematical model with an optimization method configured with a genetic algorithm and a simplex method. Finally, it concludes with some discussion on future works."
410,"We seek to elicit individual design preferences through user-computer interaction. During an iteration of the interactive session, the computer presents a set of designs to the user who then picks any preferred designs from the set. The computer learns from this feedback and creates the next set of designs using its accumulated knowledge to minimize a merit function. Under the hypothesis that user responses are deterministic, we show that an effective query scheme is akin to the Efficient Global Optimization (EGO) algorithm. Using simulated interactions, we discuss how the merit function form and user preference sensitivity can affect search efficiency and hence the time to complete an interactive session. We demonstrate the proposed algorithm in the design of vehicle exteriors."
411,"Consumers have different ideas of what makes a design stylish. Some consumers may want a sporty looking car, while others may want a rugged looking or a fuel-efficient looking car. Can computers learn what it means to satisfy those style-based goals and use this knowledge to generate designs that target style-based goals in design? An experiment was conducted where participants were asked to rate computer generated car profiles for sportiness, ruggedness, beauty, and fuel efficiency. This survey data is used as an indicator of consumer stylistic form preferences, and was used to train Artificial Neural Networks (ANN) for each of the four rating categories. The resulting ANNs were then inverted using a Genetic Algorithm (GA) in order to generate new designs that elicit targeted style goals from consumers."
412,"The paradigm of mass customization strives to minimize the tradeoffs between an ‘ideal’ product and products that are currently available. However, the lack of information relation mechanisms that connect the domains of marketing, engineering, and distribution have caused significant challenges when designing products for mass customization. For example, the bridge connecting the marketing and engineering domains is complicated by the lack of proven tools and methodologies that allow customer needs and preferences to be understood at a level discrete enough to support true mass customization. Discrete choice models have recently gained significant attention in engineering design literature as a way of expressing customer preferences. This paper explores how information from choice-based conjoint surveys might be used to assist the development of a mass customizable MP3 player, starting from 140 student surveys. The authors investigate the challenges of fielding discrete choice surveys for the purpose of mass customization, and explore how hierarchical Bayes mixed logit and latent class multinomial logit models might be used to understand the market for customizable attributes. The potential of using discrete choice models as a foundation for mathematically formulating mass customization problems is evaluated through an investigation of strengths and limitations."
413,"The prevailing practice of design for mass customization manifests itself through a configure-to-order paradigm, which means to satisfy explicit customer needs and built upon legacy design. With pervasive connectivity and interactivity of the Internet and sensor networks, personalization has been witnessed in a number of industry sectors as a promising strategy that makes the market of one a reality. This positioning paper envisions an extension to design for mass customization and personalization (DFMCP). By exploiting implicit market demand information and revealing latent customer needs, DFMCP aspires to assist customers in making better informed decisions, and to the largest extent, to anticipate customer satisfaction and adapt to customer delight. Based on a multiverse of user experience, product differentiation and co-creation, the key issues of DFMCP are discussed."
414,"For the purpose of structure vibration reduction, a structural topology optimization for forced vibration problem is proposed based on the level set method. The objective of present study is to minimize the frequency response at the specified points or surfaces on the structure with an excitation frequency or a frequency range, subject to the given amount of the material over the admissible design domain. The sensitivity analysis with respect to the structural boundaries is carried out, while the X-FEM is employed for solving the state equation and the adjoint equation. The optimal structure with smooth boundaries is obtained by the level set evolution with advection velocity, derived from the sensitivity analysis and the optimization algorithm. A number of two-dimensional numerical examples are presented to demonstrate the feasibility and effectiveness of the proposed approach."
415,"The geometric variations in a tolerance-zone can be modeled with hypothetical point-spaces called Tolerance-Maps (T-Maps) for purposes of automating the assignment of tolerances during design. The objective of this paper is to extend this model to represent tolerances on line-profiles. Such tolerances limit geometric manufacturing variations to a specified two-dimensional tolerance-zone, i.e. an area, the boundaries to which are curves parallel to the true profile. The single profile tolerance may be used to control position, orientation, and form of the profile. In this paper, the Tolerance-Map (Patent No. 6963824) is a hypothetical volume of points that captures all the positions for the true profile, and those curves parallel to it, which can reside in the tolerance-zone. The model is compatible with the ASME/ANSI/ISO Standards for geometric tolerances. T-Maps have been generated for other classes of geometric tolerances in which the variation of the feature are represented with a plane, line or circle, and these have been incorporated into testbed software for aiding designers when assigning tolerances for assemblies. In this paper the T-Map for line-profiles is created and, for the first time in this model, features may be either symmetrical or non-symmetrical simple planar curves, typically closed. To economize on length of the paper, and yet to introduce a method whereby T-Maps may be "
416,"An approach to automatically converting a design feature model to an analysis feature model for downstream finite element analysis is proposed. The analysis feature model is a mixed-dimensional model with shell features representing thin regions and solid features representing thick regions. In the approach, the design feature model is first decomposed into a set of remnants of additive features, each of which represents part of an additive feature’s volume that remain in the final volume of the design model. The remnant of each additive feature is then decomposed into swept bodies and non-swept bodies. After that, the thin regions of each swept body are effectively recognized based on its sketch information. These detected thin regions can be wrongly recognized, which, together with potentially missing ones are thus detected and corrected by a synthesization process. Finally, the analysis features and their relative interfaces are generated, which ultimately gives the corresponding analysis feature model to the input design feature model. Experimental results are also shown to demonstrate the proposed method’s effectiveness."
417,"Simulation of a machine is very important before laser metal deposition is performed, as a tool to check collision detection and validate deposition result. There are several kinds of machines that are used for laser deposition and hence there is a need for a generalized concept for visual simulation of various kinds of machines. This paper presents the research conducted on describing each machine configuration in a generic format. A parent–child list and a dependency list obtained from the machine configuration are utilized to form the generic format. Such a format can be used to describe linear and rotational motion of the machines parts. This method has been tested on various examples to demonstrate its robustness and efficiency."
418,"This paper presents a new point set surfacing method that employs neural networks for regression. Our technique takes as input unstructured and possibly noisy point sets representing two-manifolds in R 3  . To facilitate parametrization, the set is first embedded in R 2   using neighborhood preserving locally linear embedding. A neural network is then constructed and trained that learns a mapping between the embedded 2D parametric coordinates and the corresponding 3D space coordinates. The trained network is then used to generate a tessellation that spans the parametric space, thereby producing a surface in the original space. This approach enables the surfacing of noisy and non-uniformly distributed point sets, and can be applied to open or closed surfaces. We show the utility of the proposed method on a number of test models, as well as its application to freeform surface creation in virtual reality environments."
420,"Shape optimization (with topological changes) has become a "
422,"Spot welding is the predominant joining method in car body assembly. Spot welding sequences have a significant influence on the dimensional variation of resulting assemblies and ultimately on overall product quality. It also has a significant influence on welding robot cycle time and thus ultimately on manufacturing cost. In this work we evaluate the performance of Genetic Algorithms, GAs, on multi-criteria optimization of welding sequence with respect to dimensional assembly variation and welding robot cycle time. Reference assemblies are fully modelled in 3D including detailed fixtures, welding robots and weld guns. Dimensional variation is obtained using variation simulation and part measurement data. Cycle time is obtained using automatic robot path planning. GAs are not guaranteed to find the global optimum. Besides exhaustive calculations, there is no way to determine how close to the actual optimum a GA trial has reached. Furthermore, sequence fitness evaluations constitute the absolute majority of optimization computation running time and do thus need to be kept to a minimum. Therefore, for two industrial reference assemblies we investigate the number of fitness evaluations that is required to find a sequence that is optimal or a near-optimal with respect to the fitness function. The fitness function in this work is a single criterion based on a weighted and normalized combination of dimensional variation and cycle time. Both reference assemblies involves 7 spot welds which entails 7!=5040 possible welding sequences. For both reference assemblies, dimensional variation and cycle time is exhaustively calculated for all 5040 possible sequences, determining the optimal sequence, with respect to the fitness function, for a fact. Then a GA that utilizes Random Key Encoding is applied on both cases and the performance is recorded. It is found that in searching through about 1% of the possible sequences, optimum is reached in about half of the trials and 80–90% of the trials reach the ten best sequences. Furthermore the optimum of the single criterion fitness function entails dimensional variation and cycle time fairly close to their respective optimum. In conclusion, this work indicates that genetic algorithms are highly effective in optimizing welding sequence with respect to dimensional variation and cycle time."
423,"In product design, designers often create a multitude of concept sketches as part of the ideation process. Transforming such sketches to 3D digital models usually require special expertise and effort due to a lack of suitable Computer Aided Design (CAD) tools. Although recent advances in sketch-based user interfaces and immersive environments (such as augmented/virtual reality) have introduced novel curve design tools, rapid surfacing of such data remains an open challenge. To this end, we propose a new method that enables a quick construction of approximate surfaces from a cloud of 3D curves that need not be connected to one another. Our method first calculates a vector field by discretizing the space in which the curve cloud appears into a voxel image. This vector field drives a deformable surface onto the 3D curve cloud thus producing a closed surface. The surface smoothness is achieved through a set of surface smoothing and subdivision operations. Our studies show that the proposed technique can be particularly useful for early visualization and assessment of design ideas."
424,"Product development is seeing a paradigm shift in the form of a simulation-driven approach. Recently, companies and designers have started to realize that simulation has the biggest impact when used as a concept verification tool in early stages of design. Early stage simulation tools like ANSYS™ Design Space and SIMULIA™ DesignSight Structure help to overcome the limitations in traditional product development processes where analyses are carried out by a separate group and not the designers. Most of these commercial tools still require well defined solid models as input and do not support freehand sketches, an integral part of the early design stage of product development. To this extent, we present APIX (acronym for A nalysis from Pix ellated Inputs), a tool for quick analysis of two dimensional mechanical sketches and parts from their static images using a pen-based interface. The input to the system can be offline (paper) sketches and diagrams, which include scanned legacy drawings and freehand sketches. In addition, images of two-dimensional projections of three dimensional mechanical parts can also be input. We have developed an approach to extract a set of boundary contours to represent a pixellated image using known image processing algorithms. The idea is to convert the input images to online sketches and use existing stroke-based recognition techniques for further processing. The converted sketch can now be edited, segmented, recognized, merged, solved for geometric constraints, beautified and used as input for finite element analysis. Finally, we demonstrate the effectiveness of our approach in the early design process with examples."
425,"Computer models and simulations are essential system design tools that allow for improved decision making and cost reductions during all phases of the design process. However, the most accurate models tend to be computationally expensive and can therefore only be used sporadically. Consequently, designers are often forced to choose between exploring many design alternatives with less accurate, inexpensive models and evaluating fewer alternatives with the most accurate models. To achieve both broad exploration of the design space and accurate determination of the best alternatives, surrogate modeling and variable accuracy modeling are gaining in popularity. A surrogate model is a mathematically tractable approximation of a more expensive model based on a limited sampling of that model. Variable accuracy modeling involves a collection of different models of the same system with different accuracies and computational costs. We hypothesize that designers can determine the best solutions more efficiently using surrogate and variable accuracy models. This hypothesis is based on the observation that very poor solutions can be eliminated inexpensively by using only less accurate models. The most accurate models are then reserved for discerning the best solution from the set of good solutions. In this paper, a new approach for global optimization is introduced, which uses variable accuracy models in conjuction with a kriging surrogate model and a sequential sampling strategy based on a Value of Information (VOI) metric. There are two main contributions. The first is a novel surrogate modeling method that accommodates data from any number of different models of varying accuracy and cost. The proposed surrogate model is Gaussian process-based, much like classic kriging modeling approaches. However, in this new approach, the error between the model output and the unknown truth (the real world process) is explicitly accounted for. When variable accuracy data is used, the resulting response surface does not interpolate the data points but provides an approximate fit giving the most weight to the most accurate data. The second contribution is a new method for sequential sampling. Information from the current surrogate model is combined with the underlying variable accuracy models’ cost and accuracy to determine where best to sample next using the VOI metric. This metric is used to mathematically determine where next to sample and with which model. In this manner, the cost of further analysis is explicitly taken into account during the optimization process."
426,"Predictive modeling is an important tool in engineering design and optimization. Designers can develop a predictive model to replace a computationally-intensive physics-based model (a practice referred to as meta-modeling or response-surface modeling) or to model systems based on empirically-obtained data. However, such models typically have a limited domain of validity—that is, only certain combinations of model inputs yield predictions that are trustable. Consequently, designers must take care to bound the search space of optimization algorithms that otherwise would be unable to distinguish between valid and invalid predictions. Prior research has found that the valid input domain of a model can be shaped irregularly and difficult to model using simple bounds on input variables. The Support Vector Domain Description (SVDD) method was shown to be an effective approach for modeling such boundaries. However, the method used previously for generating the domain description is slow and scales poorly as the size of the training data set grows. This paper describes a new incremental method for generating a SVDD using a point-by-point comparison in place of considering all data points at once. This method is observed to be over 1000 times faster than the original method. This makes the overall approach attractive on problems of practical scale. We describe the new method, explore its characteristics, and demonstrate it on a design example for the selection of component concepts for a commercial power generation plant."
427,"This paper presents a framework for identification of the global optimum of Kriging models. The framework is based on a branch and bound scheme for sub-division of the search space into hypercubes while constructing convex under-estimators of the Kriging models. The convex under-estimators, which are a key development in this paper, provide a relaxation of the original problem. The relaxed problem has two key features: i) convex optimization algorithms such as sequential quadratic programming (SQP) are guaranteed to find the global optimum of the relaxed problem, and ii) objective value of the relaxed problem is a lower bound on the best attainable solution within a hypercube for the original (Kriging model) problem. The convex under-estimators improve in accuracy as the size of a hypercube gets smaller via the branching search. Termination of a hypercube branch is done when either: i) solution of the relaxed problem within the hypercube is no better than current best solution of the original problem, or ii) best solution of the original problem and that of the relaxed problem are within tolerance limits. To assess the significance of the proposed framework, comparison studies against genetic algorithm (GA) are conducted using Kriging models that approximate standard nonlinear test functions, as well as application problems of water desalination and vehicle crashworthiness. Results of the studies show the proposed framework deterministically providing a solution within tolerance limits from the global optimum, while GA is observed to not reliably discover the best solutions in problems with larger number of design variables."
428,"The design of most modern systems requires the tight integration of multiple disciplines. In practice, these multiple disciplines are often optimized independently, given only fixed values or targets for their interactions with other disciplines. The result is a system that may not represent the optimal system-level design. It may also not be a robust design in the sense that small changes in each subsystem’s performance may have a large impact on the system-level performance. The use of kriging models to represent the response surfaces of subsystems that are then combined to estimate system-level performance can be used as a method to provide collaboration between design teams. The difficulty with this method is the creation of the models given potentially large number of dimensions or observations. This paper presents a method to reduce the dimensionality of the input space for kriging models used for designing of complex systems. The input dimensionality of the kriging model is reduced to only includes the most important factors needed for the prediction of the observed output. A result of using these reduced dimensionality models is the need to no longer force interpolation of all of the observations used to create the models."
429,"This paper explores the effectiveness of the recently developed surrogate modeling method, the Adaptive Hybrid Functions (AHF), through its application to complex engineered systems design. The AHF is a hybrid surrogate modeling method that seeks to exploit the advantages of each component surrogate. In this paper, the AHF integrates three component surrogate models: (i) the Radial Basis Functions (RBF), (ii) the Extended Radial Basis Functions (E-RBF), and (iii) the Kriging model, by characterizing and evaluating the local measure of accuracy of each model. The AHF is applied to model complex engineering systems and an economic system, namely: (i) wind farm design; (ii) product family design (for universal electric motors); (iii) three-pane window design; and (iv) onshore wind farm cost estimation. We use three differing sampling techniques to investigate their influence on the quality of the resulting surrogates. These sampling techniques are (i) Latin Hypercube Sampling (LHS), (ii) Sobol’s quasirandom sequence, and (iii) Hammersley Sequence Sampling (HSS). Cross-validation is used to evaluate the accuracy of the resulting surrogate models. As expected, the accuracy of the surrogate model was found to improve with increase in the sample size. We also observed that, the Sobol’s and the LHS sampling techniques performed better in the case of high-dimensional problems, whereas the HSS sampling technique performed better in the case of low-dimensional problems. Overall, the AHF method was observed to provide acceptable-to-high accuracy in representing complex design systems."
430,"This paper proposes fundamental concepts for goal-defined product designs, and practical methodologies for achieving optimal designs based these concepts. Also emphasized are the functions and significance of Pareto optimum solution sets in multi-objective optimizations during the execution of the proposed methodologies. Three main concepts for product design optimization are presented. First, the goal of the product design optimization is specified to obtain the best harmony of related (and often conflicting) characteristics, where Pareto optimum solution sets represent this harmony and more preferable degrees of harmony cause an increase in social profit. Second, to obtain design solutions that maximize the desired harmony, deeper level characteristics in the design optimization problem are derived based on simplification or decomposition of the usual surface level characteristics, and optimizations are initiated from these deeper levels where the most important and influential aspects of the design problems are easiest to recognize. The third concept entails the use of collaboration with specialist experts concerning the product characteristics, focusing on Pareto optimum solution sets obtained in deeper level optimizations, so that these experts can facilitate the development of more preferable results based on their own ideas and knowledge. The interrelationships between the second and third concepts are described and used to obtain globally optimal design solutions that have the highest degree of harmony for the required product design objectives. The proposed concepts and methodologies for product design optimizations are demonstrated using certain designs for articulated robots."
431,"In multi-objective problems, one is often interested in generating the envelope of the objective-space, where the envelope is, in general, a superset of pareto-optimal solutions. In this paper, we propose a method for tracing the envelope of multi-objective "
434,"Simultaneous development of an industrial robot family, consisting typically of 2–10 robots, has been an engineering practice in robotics industry. In this process, significant scenario studies on defining product requirement specifications and associated design change are conducted. This implies that understanding the relation between product requirements and design of the robot family is of critical importance. However, in the current engineering practice, any change in requirement specification results in tremendous efforts in the re-design of the robot family. This discloses the need for efficient methodology and tools for simultaneously optimizing product requirements and design of an industrial robot family. In this work, methodology and tools have been successfully developed for simultaneously optimizing product requirements and design of an industrial robot family in a fully automated way. This problem is formulated to a multi-objective optimization problem and solved using multi-objective genetic algorithm (MOGA). Results of this work have demonstrated clearly the efficiency of this approach and the insight obtained on the relation between product requirement and product design. The developed methodology and results of simultaneous requirement specification and design optimization will be detailed in this paper. In addition, research experience and future work will also be discussed. To our best knowledge, the simultaneous optimization of product requirement and product design has not been widely investigated and explored in academia. The trade-off information explored by such approach is crucial in product development in industrial practice. Such approach will further increase the complexity of traditional design optimization approach where product requirement is normally pre-defined and used as constraint. It is certain that discussions of the addressed problem and developed methodology will contribute to promoting the significance of efforts in the research society of multi-objective design optimization, multi-objective design optimization of product families, and design automation."
435,"During the process of trade space exploration, information overload has become a notable problem. To find the best design, designers need more efficient tools to analyze the data, explore possible hidden patterns, and identify preferable solutions. When dealing with large-scale, multi-dimensional, continuous data sets (e.g., design alternatives and potential solutions), designers can be easily overwhelmed by the volume and complexity of the data. Traditional information visualization tools have some limits to support the analysis and knowledge exploration of such data, largely because they usually emphasize the visual presentation of and user interaction with data sets, and lack the capacity to identify hidden data patterns that are critical to in-depth analysis. There is a need for the integration of user-centered visualization designs and data-oriented data analysis algorithms in support of complex data analysis. In this paper, we present a work-centered approach to support visual analytics of multi-dimensional engineering design data by combining visualization, user interaction, and computational algorithms. We describe a system, Learning-based Interactive Visualization for Engineering design (LIVE), that allows designer to interactively examine large design input data and performance output data analysis simultaneously through visualization. We expect that our approach can help designers analyze complex design data more efficiently and effectively. We report our preliminary evaluation on the use of our system in analyzing a design problem related to aircraft wing sizing."
436,"Multidisciplinary design optimization (MDO) has evolved remarkably since its inception 25 years ago. Despite these advances, the design of complex engineered systems remains a challenge, and many large-scale engineering projects are routinely plagued by exorbitant cost overruns and delays. To gain insight into these challenges, 48 people gathered from industry, academia, and government agencies to examine MDO’s current and future role in designing complex engineered systems. This paper summarizes the views of five distinguished speakers on the “state of the research” along with the discussions from an industry panel of representatives from Boeing, Caterpillar, Ford, NASA Glenn Research Center, and United Technologies Research Center on the “state of the practice”. This paper also summarizes the future research topics identified by breakout groups in five key areas: (1) modeling and the design space; (2) metrics, objectives, and requirements; (3) coupling in complex engineered systems; (4) dealing with uncertainty; and (5) people and workflow. Finally, five over-arching themes are offered to advance MDO. First, we need to engage more disciplines outside of engineering and look for opportunities to use MDO outside of its traditional areas. Second, MDO problem formulations must evolve to encompass a wider range of design criteria. Third, we need effective strategies for putting designers “back in the loop” during MDO. Fourth, we need to do a better job of publicizing the successful examples of MDO so that we can improve the “buy in” that is needed to advance MDO in academia, industry, and government agencies. Fifth, we need to better educate our students and practitioners on systems design, optimization, and MDO along with their benefits and drawbacks."
437,"Multidisciplinary optimization is a highly iterative process that requires a large number of function evaluations to evaluate objective functions and constraints. Metamodels for computationally expensive functions or simulations can be employed in the multidisciplinary optimization instead of the actual solvers resulting in significant computational savings. In this paper, metamodeling is applied to the multidisciplinary design optimization of a ship hull with resistance, seakeeping, and maneuvering performance analyses. At the top system level, a simple cost metric is defined to drive the overall design optimization process. Changes to the hull shape are reflected in the numerical model for resistance computations and in the simulations associated with the seakeeping and maneuvering disciplines. An automated process has been developed for propagating changes to the numerical (CFD) model for the resistance computations; this expedites the computations at the sample points used for developing the metamodels. The validity of employing metamodels instead of the actual solvers during the optimization is demonstrated by comparing the values of the objective functions and constraints at the optimum point when using the actual solvers and when using the metamodels."
438,"Understanding relationships amongst n-dimensional design spaces has long been a problem in the engineering community. Many visual methods previously developed, although useful, are limited to comparing three design variables at a time. Work described in this paper builds off the idea of a self-organizing map in order to visualize n-dimensional data on a two dimensional map. By using the contextual self-organizing map, current work shows that more design space information can be gleaned from map nodes themselves. By breaking the final visualization up into three maps containing separate contextual information, an investigator can quickly obtain information about the overall behavior of a design space. Tests run on well-known optimization functions show that information such as modality and curvature may be quickly suggested by these maps, and that they may provide enough information for a designer to choose a function to proceed with formal optimization of a given data set."
439,"This paper presents a multidisciplinary design optimization framework for modular industrial robots. An automated design framework, containing physics based high fidelity models for dynamic simulation and structural strength analyses are utilized and seamlessly integrated with a geometry model. The proposed framework utilizes well-established methods such as metamodeling and multi-level optimization in order to speed up the design optimization process. The contribution of the paper is to show that by applying a merger of well-established methods, the computational cost can be cut significantly, enabling search for truly novel concepts."
440,"Complex design problems are typically decomposed into smaller design problems that are solved by domain-specific experts who must then coordinate their solutions into a satisfactory system-wide solution. In set-based collaborative design, collaborating engineers coordinate themselves by communicating multiple design alternatives at each step of the design process. Previous research has demonstrated that classifiers can be a communication medium for facilitating set-based collaborative design because of their ability to divide a design space into satisfactory and unsatisfactory regions. The proposed kernel-based Bayesian network (KBN) classifier uses a set of example designs of known acceptability, called the training set, to create a map of the satisfactory region of the design space. However, previous implementations used deterministic space-filling sampling sequences to choose the training set of designs. The shortcoming of deterministic space-filling sampling schemes is that they do not adapt to focus the samples on regions of interest to the design team (exploitation) or, alternatively, on regions in which little information is known (exploration). In this paper, we introduce the use of KBN classifiers as the basis for sequential sampling strategies that can be exploitive, exploratory, or any combination thereof."
441,"Time is an asset of critical importance in the design process and it is desirable to reduce the amount of time spent developing products and systems. Design is an iterative activity and a significant portion of time spent in the product development process is consumed by design engineers iterating towards a mutually acceptable solution. Therefore, the amount of time necessary to complete a design can be shortened by reducing the time required for design iterations or by reducing the number of iterations. The focus of this paper is on reducing the number of iterations required to converge to a mutually acceptable solution in distributed design processes. In distributed design, large systems are decomposed into smaller, coupled design problems where individual designers have control over local design decisions and seek to satisfy their own individual objectives. The number of iterations required to reach equilibrium solutions in distributed design processes can vary depending on the starting location and the chosen process architecture. We investigate the influence of process architecture on the convergence behavior of distributed design systems. This investigation leverages concepts from game theory, classical controls and discrete systems theory to develop a transient response model. As a result, we are able to evaluate process architectures without carrying out any solution iterations."
442,"Design of physical systems and associated control systems are coupled tasks; design methods that manage this interaction explicitly can produce system-optimal designs, whereas conventional sequential processes may not. Here we explore a new technique for combined physical system and control design (co-design) based on a simultaneous dynamic optimization approach known as direct transcription, which transforms infinite-dimensional control design problems into finite dimensional nonlinear programming problems. While direct transcription problem dimension is often large, sparse problem structures and fine-grained parallelism (among other advantageous properties) can be exploited to yield computationally efficient implementations. Extension of direct transcription to co-design gives rise to a new problem structures and new challenges. Here we illustrate direct transcription for co-design using a new automotive active suspension design example developed specifically for testing co-design methods. This example builds on prior active suspension problems by incorporating a more realistic physical design component that includes independent design variables and a broad set of physical design constraints, while maintaining linearity of the associated differential equations."
443,"Analytical Target Cascading method has been widely developed to solve hierarchical design optimization problems. In the Analytical Target Cascading method, a weighted-sum formulation has been commonly used to coordinate the inconsistency between design points and assigned targets in each level while minimizing the cost function. However, the choice of the weighting coefficients is very problem dependent and improper selections of the weights will lead to incorrect solutions. To avoid the problems associated with the weights, single objective functions in the hierarchical design optimization are formulated by a new Bounded Target Cascading method. Instead of point targets assigned for design variables in the Analytical Target Cascading method, bounded targets are introduced in the new method. The target bounds are obtained from the optimal solutions in each level while the response bounds are updated back to the system level. If the common variables exist, they are coordinated based on their sensitivities with respect to design variables. Finally, comparisons of the results from the proposed method and the weighted-sum Analytical Target Cascading are presented and discussed."
444,"Lattice materials are characterized at the microscopic level by a regular pattern of voids confined by walls. Recent rapid prototyping techniques allow their manufacturing from a wide range of solid materials, ensuring high degrees of accuracy and limited costs. The microstructure of lattice material permits to obtain macroscopic properties and structural performance, such as very high stiffness to weight ratios, highly anisotropy, high specific energy dissipation capability and an extended elastic range, which cannot be attained by uniform materials. Among several applications, lattice materials are of special interest for the design of morphing structures, energy absorbing components and hard tissue scaffold for biomedical prostheses. Their macroscopic mechanical properties can be finely tuned by properly selecting the lattice topology and the material of the walls. Nevertheless, since the number of the design parameters involved is very high, and their correlation to the final macroscopic properties of the material is quite complex, reliable and robust multiscale mechanics analysis and design optimization tools are a necessary aid for their practical application. In this paper, the optimization of lattice materials parameters is illustrated with reference to the design of a bracket subjected to a point load. Given the geometric shape and the boundary conditions of the component, the parameters of four selected topologies have been optimized to concurrently maximize the component stiffness and minimize its mass."
445,"A multiscale design and multiobjective optimization procedure is developed to design a new type of graded cellular hip implant. We assume that the prosthesis design domain is occupied by a unit cell representing the building block of the implant. An optimization strategy seeks the best geometric parameters of the unit cell to minimize bone resorption and interface failure, two conflicting objective functions. Using the asymptotic homogenization method, the microstructure of the implant is replaced by a homogeneous medium with an effective constitutive tensor. This tensor is used to construct the stiffness matrix for the finite element modeling (FEM) solver that calculates the value of each objective function at each iteration. As an example, a 2D finite element model of a left implanted femur is developed. The relative density of the lattice material is the variable of the multiobjective optimization, which is solved through the non-dominated sorting genetic algorithm II (NSGA-II). The set of optimum relative density distributions is determined to minimize concurrently interface stress distribution and bone loss mass. The results show that the amount of bone resorption and the maximum value of interface stress can be reduced by over 70% and 50%, respectively, when compared to current fully dense titanium stem."
446,"This paper presents a method to improve the fatigue strength of 2D periodic cellular materials under a fully-reversed loading condition. For a given cell topology, the shape of the unit cell is synthesized to minimize any stress concentration caused by discontinuities in the cell geometry. We propose to reduce abrupt geometric changes emerging in the periodic microstructure through the synthesis of a cell shape defined by curved boundaries with continuous curvature, i.e. G2 -continous curves. The bending moments caused by curved cell elements are reduced by minimizing the curvature of G2 -continuous cell elements so as to make them as straight as possible. The asymptotic homogenization technique is used to obtain the homogenized stiffness matrix and the fatigue strength of the synthesized cellular material. The proposed methodology is applied to synthesize a unit cell topology described by smooth boundary curves. Numeric simulations are performed to compare the performance of the synthesized cellular solid with that of common two dimensional lattice materials having hexagonal, circular, square, and Kagome shape of the unit cell. The results show that the methodology enables to obtain a cellular material with improved fatigue strength. Finally, a parametric study is performed to examine the effect of different geometric parameters on the performance of the proposed cellular geometries."
447,"This paper presents the energy absorption properties of hexagonal honeycomb structures of varying cellular geometries to high speed in-plane impact. While the impact responses in terms of energy absorption and densification strains have been extensively researched and reported, a gap is identified in the generalization of honeycombs with controlled and varying geometric parameters. This paper attempts to address this gap through a series of finite element (FE) simulations where cell angle and angled wall thickness are varied while maintaining a constant mass of the honeycomb structure. A randomly filled, non-repeating Design of Experiments (DOE) is generated to determine the effects of these geometric parameters on the output of energy absorbed, and a statistical sensitivity analysis is used to determine the parameters significant for optimization. A high degree of variation in the impact response of varying cellular geometries has shown the potential for the forward design into lightweight crushing regions in many applications, particularly the automotive and aerospace industries. It is found that while an increase in angled wall thickness enhances the energy absorption of the structure, increases in either the cell angle or ratio of cell angle to angled wall thickness have adverse effects on the output. Finally, optimization results present that a slightly auxetic cellular geometry with maximum angled wall thickness provides for maximum energy absorption, which is verified with an 8% error when compared to a final FE simulation."
448,"In this study, hexagonal honeycombs with a shape memory alloy (SMA) are explored for super-compliant meso-structural design. A nitianol (NiTi) SMA based shear compliant hexagonal cellular materials are introduced and their elastic properties in shear are investigated. The constitutive relation of SMA and Cellular Materials Theory (CMT) are used to develop analytical constitutive equations of SMA honeycombs under isothermal shear loading. A fixed volume based SMA honeycombs are designed with a target shear modulus, ("
449,"Reverse engineering (extracting information about a product from the product itself) is a competitive strategy for many firms and is often costly to innovators. Recent research has proven metrics for estimating the reverse engineering time and barrier and has shown that products can strategically be made more difficult to reverse engineer, thus protecting the innovator. Reverse engineering, however, is only the first phase of attempting to duplicate a product. Imitating — the process of discovering how to physically reproduce the performance of the reverse engineered product in one or more of its performance areas — is the second and final phase. This paper presents metrics for the time and barrier to imitating and shows how they can be joined with reverse engineering metrics to estimate a total time and total barrier to duplicate a product. As there is a cost associated with the design of barriers to reverse engineering and imitating it is important that a return on investment analysis be performed to ensure a profitable endeavor. Details of such an analysis are presented here."
450,"Automated concept generation is non-trivial task. The complexity of this problem is mainly due to lack of formal representation frameworks that lend themselves easily to a computational approach. Generative grammar has emerged as a potential solution to this problem and presents a number of different possibilities for conceptual design automation. This paper presents a novel search method that has been developed specifically for search trees defined by a special class of generative grammar in which rules of the grammar have parameters associated with them. A novel feature of the proposed search is ‘Human in the loop’ approach in which learning about the search space is achieved by querying the user. The user fatigue restricts the maximum number of comparisons of candidate solutions (30–50). From the data gathered from the comparisons, a stochastic decision making process proposed in this paper quickly converges to a region of design space which best meet the user’s preference. The method is implemented and applied to a grammar for shampoo bottle concept generation. It is shown through multiple user-guided and automated experiments that the method has ability to learn and adopt through human computer interaction process. The implications of the proposed search method for automated conceptual design are expounded on in the conclusions."
451,"The development of safe, energy efficient mechatronic systems is currently changing standard paradigms in the design and control of industrial manipulators. In particular, most optimization strategies require the improvement or the substitution of different system components. On the other hand, from an industry point of view, it would be desirable to develop energy saving methods applicable also to established manufacturing systems being liable of small possibilities for adjustments. Within this scenario, an engineering method is reported for optimizing the energy consumption of serial manipulators for a given operation. An object-oriented modeling technique, based on bond graph, is used to derive the robot electromechanical dynamics. The system power flow is then highlighted and parameterized as a function of the total execution times. Finally, a case study is reported showing the possibility to reduce the operation energy consumption when allowed by scheduling or manufacturing constraints."
452,"In most cases the deleterious effects associated with the occurrence of leaks may present serious problems and therefore, leaks must be quickly detected, located and repaired. The problem of leakage becomes even more serious when it is concerned with the vital supply of fresh water to the community. In addition to waste of resources, contaminants may infiltrate into the water supply. The possibility of environmental health disasters due to delay in detection of water pipeline leaks has spurred research into the development of methods for pipeline leak and contamination detection. Leaking in water networks has been a very significant problem worldwide, especially in developing countries, where water is sparse. Many different techniques have been developed to detect leaks, either from the inside or from the outside of the pipe; each one of them with their advantages, complexities but also limitations. To overcome those limitations we focus our work on the development of an in-pipe-floating sensor. The present paper discusses the design considerations of a novel autonomous system for in-pipe water leak detection. The system is carefully designed to be minimally invasive to the flow within the pipe and thus not to affect the delicate leak signal. One of its characteristics is the controllable motion inside the pipe. The system is capable of pinpointing leaks in pipes while operating in real network conditions, i.e. pressurized pipes and high water flow rates, which are major challenges."
453,"As technology advances, there is an increasing need to reliably output mechanical work at smaller scales. At the nanoscale, one of the most promising routes is utilizing biomolecular motors such as myosin proteins commonly found in cells. Myosins convert chemical energy into mechanical energy and are strong candidates for use as components of artificial nanodevices and multi-scale systems. Isoforms of the myosin superfamily of proteins are fine-tuned for specific cellular tasks such as intracellular transport, cell division, and muscle contraction. The modular structure that all myosins share makes it possible to genetically engineer them for fine-tuned performance in specific applications. In this study, a parametric analysis is conducted in order to explore the design space of Myosin II isoforms. The crossbridge model for myosin mechanics is used as a basis for a parametric study. The study sweeps commonly manipulated myosin performance variables and explores novel ways of tuning their performance. The analysis demonstrates the extent that myosin designs are alterable. Additionally, the study informs the biological community of gaps in experimentally tabulated myosin design parameters. The study lays the foundation for further progressing the design and optimization of individual myosins, a pivotal step in the eventual utilization of custom-built biomotors for a broad range of innovative nanotechnological devices."
454,"In product family design the goal is to generate a set of lowest cost products that target specific market niches. Sharing components, called platforms, between different products can minimize duplication of effort, thereby lowering family costs. However, if the products’ requirements are too dissimilar, sharing components may compromise the end product; such variance will lead to lower end products being overdesigned and/or higher end products being underdesigned. It is important to identify which components are similar enough, so that sharing does not compromise the individual products’ performances. Most existing product family design methods make decisions a priori about platforms; constraining platforms to be used by every product in the family, or not at all. Methods that simultaneously optimize component sharing and design variable settings have the potential to find better families because product subsets may be more similar to each other than to other subsets of products. Allowing components to be shared between any subset of family members leads to a very large combinatorial problem, and considering large product families can be computationally prohibitive. This paper proposes a method to identify possible sets of product family platforms by using the pattern recognition technique of fuzzy c-means clustering on component subspaces. Component subspaces are taken from a database of generated design points for the whole family. If components from different products are similar enough to be grouped into the same cluster, then those components could possibly become the same platform. Fuzzy equivalence relations can be extracted from the cluster membership functions that show the binary relationship from one products’ component to a different products’ component. Ultimately, this method can be used as a platform identification heuristic in a larger product family design methodology. This method is demonstrated by applying it to find possible common components in a family of universal electric motors."
455,"Increase of demand on product variety has pushed companies to think about offering more and more product variants in order to take more market shares. However, product variation can lead to cost increase for design and production, as well as the lead time for new variants. As a result, a proper tradeoff is required between cost-effectiveness of manufacturing and satisfying diverse demands. Such tradeoff has been shown to be manageable effectively by exploiting product family design (PFD) and platform-based product development. These strategies have been widely studied during the past decades, and a large number of approaches have been proposed for covering different issues and steps related to design and development of product families and platforms. Verification and performance of such approaches have also been traced through practical case studies applied to several industries. This paper focuses on a review of the research in this field and efforts to classify the recent advancements relevant to product family design and platform development issues. A comprehensive review on the state-of-the-art research in this field was done by Jiao et al. in 2007; therefore the main focus of this paper is on the research activities from 2006 to present. Mainly, the effort of this paper is to identify new achievements in regard with different aspects of product family design such as customer involvement in design, market driven studies, new indices and metrics for assessing families and developing the desired platforms, issues relevant to product family optimization (i.e., new algorithms and optimization approaches applied to different PFD problems along with their benefits and limitations in comparison to previously developed approaches), issues relevant to development of platforms (i.e., platform configuration approaches, joint platform design and optimization, and factors effective on forming proper platform types), and issues relevant to knowledge management and modeling of families and platforms for facilitating and supporting future design efforts. Through a comparison with previous research, new achievements are discussed and the remaining challenges and potential new research areas in this field are addressed."
456,"Product family design allows innovative companies to create customized product roadmaps, to manage designers and component partners, and to develop the next generation of products based on platform strategies. In product family design, problems for determining a design strategy or the degree of commonality for a platform can be considered as a multidisciplinary optimization problem with respect to design variables, production cost, company’s revenue, and customers’ satisfaction. In this paper, we investigate strategic module-based platform design to identify an optimal platform strategy in a product family. The objective of this paper is to introduce a multi-objective particle swarm optimization (MOPSO) approach to select the best platform design strategy from a set of Pareto-optimal solutions based on commonality and design variation within the product family. We describe modifications to apply the proposed MOPSO to the multi-objective problem of product family design and allow designers to evaluate varying levels of platform strategies. To demonstrate the effectiveness of the proposed approach, we use a case study involving a family of General Aviation Aircraft. The limitations of the approach and future work are also discussed."
457,"Product family design is probably the most widely adopted strategy for product realization in mass customization paradigm. With the ever-increasing product offerings in consumer market, current product representation schemes are restricted by their limited capability in handling multiple conceptual relationships amongst product components and rich semantic annotations associated with different design concepts. Previously, we have studied and proposed an ontology-based information representation scheme for product family design, which offers a promising solution to address the aforementioned challenges. In this study, we suggest a new commonality metric and a faceted platform selection approach, which are both created for ontology-based product family representation models. Utilizing this metric and faceted search, we discuss the advantages of our approach compared to existing modeling possibilities. We also exemplify the applications of our proposal towards an optimal configuration of product variants using a case study of four laptop computer families. Finally, we conclude this paper with some indications for future work."
458,"Reliability is an important engineering requirement for consistently delivering acceptable product performance through time. The reliability usually degrades with time increasing the lifecycle cost due to potential warranty costs, repairs and loss of market share. Reliability is the probability that the system will perform its intended function successfully for a specified time. In this article, we consider the first-passage reliability which accounts for the first time failure of non-repairable systems. Methods are available which provide an upper bound to the true reliability which may overestimate the true value considerably. The traditional Monte-Carlo simulation is accurate but computationally expensive. A computationally efficient importance sampling technique is presented to calculate the cumulative probability of failure for random dynamic systems excited by a stationary input random process. Time series modeling is used to characterize the input random process. A detailed example demonstrates the accuracy and efficiency of the proposed importance sampling method over the traditional Monte Carlo simulation."
459,"A common approach to the validation of simulation models focuses on validation throughout the entire design space. In a more recent methodology, we proposed to validate designs as they are generated during a simulation-based optimization process, relying on validating the simulation model through calibration in a sequence of local domains. In that work, the size of the local domains was held fixed and not linked to uncertainty, and the confidence in designs was quantified using Bayesian hypothesis testing. In this article, we present an improved methodology where the size and shape of the local domain at each stage of a sequential design optimization process, are determined from a parametric bootstrap methodology involving maximum likelihood estimators of unknown model parameters. Validation through calibration is carried out in the local domain at each stage. The sequential process continues until the local domain does not change from stage to stage during the design optimization process, ensuring convergence to an optimal design. The proposed methodology is illustrated with the design of a thermal insulator using one-dimensional, linear heat conduction in a solid slab with heat flux boundary conditions."
460,"Engineering design reconciles design constraints with decision maker preferences. The task of eliciting and encoding decision maker preferences is, however, extremely difficult. A Pareto front representing the locus of the non-dominated designs is therefore, often generated to help a decision maker select the best design. In this paper, we show that this method has a shortcoming. We show that when there is uncertainty in both the decision problem variables and in the decision maker’s preferences, this methodology is inconsistent with multi-attribute utility theory, unless the decision maker trades off attributes or some functions of them linearly. This is a strong restriction. To account for this, we propose a methodology that enables a decision maker to select the best design on a modified Pareto front which is acquired using envelopes of a set of certainty equivalent surfaces. This methodology does not require separability of the multi-attribute utility function into single attribute utilities, nor does it require the decision maker to trade the attributes (or any function of them) linearly. We demonstrate this methodology on a simple optimization problem and in design of a reduction gear."
461,"During casting residual stresses are developed due to the solidification and cooling. In this work the robustness of residual stresses in casted brake discs with respect to variations in four parameters is evaluated. The parameters are Young’s modulus, yield strength and hardening, time of breaking the mould and the thickness of the brake disc. The robustness analysis is performed by Monte Carlo simulations of metamodels which are surrogates to a finite element model. Quadratic response surfaces and Kriging approximations are considered. Those are based on finite element analyses defined by a Latin hypercube sampled design of experiments. In the finite element analyses an un-coupled approach is utilized where a thermal analysis generates a temperature history of the solidification and cooling. Then follows a structural analysis which is driven by the temperature history. After casting the machining of the brake disc is analyzed by gradually removing elements in the finite element model. The results show that the variation in the studied parameters yield large variation in residual stresses. The thickness of the brake disc is the parameter that has largest influence to the variation in residual stresses. Furthermore, the level of the residual stresses are in general high and might influence the fatigue life of the brake disc."
462,"This study presents a methodology to convert an RBDO problem requiring very high reliability to an RBDO problem requiring relatively low reliability by increasing input standard deviations for efficient computation in sampling-based RBDO. First, for linear performance functions with independent normal random inputs, an exact probability of failure is derived in terms of the ratio of the input standard deviation, which is denoted by "
463,"In this study, an efficient classification methodology is developed for reliability analysis while maintaining the accuracy level similar to or better than existing response surface methods. The sampling-based reliability analysis requires only the classification information — a success or a failure – but the response surface methods provide real function values as their output, which requires more computational effort. The problem is even more challenging to deal with high-dimensional problems due to the curse of dimensionality. In the newly proposed virtual support vector machine (VSVM), virtual samples are generated near the limit state function by using linear or Kriging-based approximations. The exact function values are used for approximations of virtual samples to improve accuracy of the resulting VSVM decision function. By introducing the virtual samples, VSVM can overcome the deficiency in existing classification methods where only classified function values are used as their input. The universal Kriging method is used to obtain virtual samples to improve the accuracy of the decision function for highly nonlinear problems. A sequential sampling strategy that chooses a new sample near the true limit state function is integrated with VSVM to maximize the accuracy. Examples show the proposed adaptive VSVM yields better efficiency in terms of the modeling time and the number of required samples while maintaining similar level or better accuracy especially for high-dimensional problems."
464,"Quality characteristics (QC’s) are often treated static in robust design optimization while many of them are time dependent in reality. It is therefore desirable to define new robustness metrics for time-dependent QC’s. This work shows that using the robustness metrics of static QC’s for those of time-dependent QC’s may lead to erroneous design results. To this end, we propose the criteria of establishing new robustness metrics for time-dependent QC’s and then define new robustness metrics. Instead of using a point expected quality loss over the time period of interest, we use the expectation of the maximal quality loss over the time period to quantify the robustness for time-dependent QC’s. Through a four-bar function generator mechanism analysis, we demonstrate that the new robustness metrics can capture the full information of robustness of a time-dependent QC over a time interval. The new robustness metrics can then be used as objective functions for time-dependent robust design optimization."
465,"There is always a deviation between a model prediction and the reality that the model intends to represent. The deviation is largely caused by the model uncertainty due to ignorance, assumptions, simplification, and other sources of lack of knowledge. Quantifying model uncertainty is a vital task and requires the comparison between model prediction and observation. This exercise is generally computationally intensive on the prediction side and costly on the experimentation side. In this work, a new methodology is proposed to provide an alternative implementation of model uncertainty quantification. With the new methodology, the experimental results are reported with expanded uncertainty terms around the experimental results for both model input and output. In other words, the experimental results are expressed as intervals. Then the model takes the experimental results of the input intervals and produces an interval prediction. The model uncertainty is then quantified by the difference between the model prediction and experimental observation, represented by an interval as well. By employing the standards for measurement uncertainty, the new methodology is easy to implement and could serve as a common framework for both model builders and experimenters."
466,"Designing complex systems for mission-critical applications requires a design process with focus upon maximizing the probability of meeting design requirements. Typically the design process for these systems consists of filtering and refining an initial set of conceptual designs to produce a final set of detailed designs. This final set of designs is presented to the system contracting agency or management team for design selection. In this work, a framework is presented for a multi-stage design process in which the Probability of Correctness (PoC) is utilized as a metric to sequentially filter designs from the abstract conceptual phase through the detailed design phase. This framework utilizes methods for uncertainty propagation (UP) from reliability engineering, which are organized within the framework to match the UP method with the model fidelity and data type available at each stage of the process. A case study using the Advanced Diagnostic and Prognostic Testbed (ADAPT) Electric Power System (EPS) is presented to illustrate both the verification process utilizing multiple UP methods, and also the use of the OpenModelica environment for system design. A discussion presents a generalization of the framework and the future work needed to realize the comprehensive framework for system design."
467,"Most engineered systems are designed with a passive and fixed design capacity and, therefore, may become unreliable in the presence of adverse events. Currently, most engineered systems are designed with system redundancies to ensure required system reliability under adverse events. However, a high level of system redundancy increases a system’s life-cycle cost (LCC). Recently, proactive maintenance decisions have been enabled through the development of prognostics and health management (PHM) methods that detect, diagnose, and predict the effects of adverse events. Capitalizing on PHM technology at an early design stage can transform passively reliable (or vulnerable) systems into adaptively reliable (or resilient) systems while considerably reducing their LCC. In this paper, we propose a resilience-driven system design (RDSD) framework with the goal of designing complex engineered systems with resilience characteristics. This design framework is composed of three hierarchical tasks: (i) the resilience allocation problem (RAP) as a top-level design problem to define a resilience measure as a function of reliability and PHM efficiency in an engineering context, (ii) the system reliability-based design optimization (RBDO) as the first bottom-level design problem for the detailed design of components, and (iii) the system PHM design as the second bottom-level design problem for the detailed design of PHM units. The proposed RDSD framework is demonstrated using a simplified aircraft control actuator design problem resulting in a highly resilient actuator with optimized reliability, PHM efficiency and redundancy for the given parameter settings."
468,"To simulate the dynamic behaviors of large molecular systems, approaches that solve ordinary differential equations such as molecular dynamics (MD) simulation may become inefficient. The kinetic Monte Carlo (KMC) method as the alternative has been widely used in simulating rare events such as chemical reactions or phase transitions. Yet lack of complete knowledge of transitions and the associated rates is one major challenge for accurate KMC predictions. In this paper, a reliable KMC (R-KMC) mechanism is proposed to improve the robustness of KMC results, where propensities are interval estimates instead of precise numbers and sampling is based on random sets instead of random numbers. A multi-event algorithm is developed and implemented. The weak convergence of the multi-event algorithm towards traditional KMC is demonstrated with a proposed generalized Chapman-Kolmogorov Equation."
469,"The optimal design of hybrid power generation systems (HPGS) can significantly improve the economic and technical performance of power supply. Due to the intermittent nature of renewable energy sources, as well as the application of energy storage techniques, the efficacy and efficiency of reliability assessment have become vital for successful HPGS design optimization. This paper proposes a sizing optimization method for HPGS based on a Markovian approach for long term reliability assessment. A multi-scenario formulation is considered to minimize the system cost while guaranteeing acceptable reliability across all the representative scenarios. The presented reliability analysis approach employs a Markov chain to model the state of charge of the energy storage based on probabilistic resource and load models. With this treatment, the loss of load probability of the HPGS can be tracked with relatively low computation, making it suitable for optimization applications. The effectiveness of the reliability analysis approach is tested through a comparison with Monte Carlo simulation; then the optimization approach is demonstrated with a numerical case study."
470,"The use of complex computer simulations to design, improve, optimize, or simply to better understand complex systems in many fields of science and engineering is now ubiquitous. However, simulation models are never a perfect representation of physical reality. Two general sources of uncertainty that account for the differences between simulations and experiments are parameter uncertainty and model uncertainty. The former derives from unknown model parameters, while the latter is caused by underlying missing physics, numerical approximations, and other inaccuracies of the computer simulation that exist even if all of the parameters are known. To obtain knowledge of these two sources of uncertainty, data from computer simulations (usually abundant) and data from physical experiments (typically more limited) are often combined using statistical methods. Statistical adjustment of the computer simulation model to account for the two sources of uncertainty is referred to as calibration. We argue that calibration as it is typically implemented, using only a single response variable, is challenging in that it is often extremely difficult to distinguish between the effects of parameter and model uncertainty. However, many different responses (distinct responses and/or the same response measured at different spatial and temporal locations) are automatically calculated in simulations. As multiple responses generally share a mutual dependence on the unknown parameters, they provide valuable information that can improve identifiability of parameter and model uncertainty in calibration, if they are also measured experimentally. In this paper, we explore the use of multiple responses for calibration."
471,"Robust structural design optimization with non-probabilistic uncertainties is often formulated as a two-level optimization problem. The top level optimization problem is simply to minimize a specified objective function while the optimized solution at the second level solution is within bounds. The second level optimization problem is to find the worst case design under non-probabilistic uncertainty. Although the second level optimization problem is a non-convex problem, the global optimal solution must be assured in order to guarantee the solution robustness at the first level. In this paper, a new approach is proposed to solve the robust structural optimization problems with non-probabilistic uncertainties. The WCDO problems at the second level are solved directly by the monotonocity analysis and the global optimality is assured. Then, the robust structural optimization problem is reduced to a single level problem and can be easily solved by any gradient based method. To illustrate the proposed approach, truss examples with non-probabilistic uncertainties on stiffness and loading are presented."
472,"Butt welds with orthotropic behavior are widely applied in mechanical and structural designs. Since welds cannot always be perfect in practice, it is important to understand the weld’s stress behavior under different imperfect geometries. In this paper research has been performed to investigate the relationship between stress intensity factors and change of geometry of orthotropic butt welds. Finite element methods were applied to simulate weld geometries. The simulation was performed using ANSYS software assuming two beams are welded together with a discontinuity at the bottom of the weld. The combined beams and the butt weld are then considered to be one piece of glued structure. The discontinuity in the structure is used to model a crack and lack of weld penetration. By changing three important factors of the weld geometry under uniform axial static loads, the trend of stress intensity factor behavior versus change of geometry has been investigated. Both single and double sided butt welds were considered in this paper. The results of this investigation will be a helpful tool for design engineers in deciding the best weld geometry in applications."
473,"In this paper, the effective mechanical properties of the 34 .6 lattice material are characterized theoretically and experimentally. The characterized properties include the stiffness and the strength of the material. A detailed description of the design procedure of the test specimens is presented. Three quasi-static tests are performed, namely, uniaxial tension, uniaxial compression and pure shear. The comparison of the experimental data to the theoretical results shows that the former are in good agreement with the latter. The maximum error obtained of 13% is acceptable according to the literature on experimental studies of cellular solids."
474,"In this paper, dimensional distortion during the compression molding of thermoplastic matrix composites, typically described as spring-in or spring forward, is investigated through a finite element model. Spring-in is the reduction of the enclosed angle of two surfaces on the final component shape with respect to the original mold shape. Spring-in of thermoplastic matrix composites has typically been attributed to the difference in the thermal expansion of in-plane and through thickness directions of the composite. However, using this mechanism alone during modeling has not shown complete agreement with the experimental data. A new meso-level mechanism based on the viscoelasticity effect of the thermoplastic matrix is proposed. With this mechanism, the predicted spring-in angle can be in good agreement with experiments."
475,"In this study, the natural frequencies and mode shapes of carbon nanotube (CNT) reinforced polymer composite microcantilever beams are investigated by means of a micromechanical model and the three-dimensional finite element analysis. Microcantilever beams are made of Poly vinyl chloride (PVC) and reinforced with multi-wall carbon nanotubes (MWCNTs). MWCNTs can be distributed along the length/width/thickness of the nanocomposite beam. To validate the accuracy and effectiveness of the model, a direct comparison of results is made with an analytical solution for a test case. Next, various material types of the nanocomposite microcantilever beam are introduced and the effect of different distribution patterns and the weight-percents (wt%) of MWCNTs on the first six natural frequencies and mode shapes is found."
476,"Experimental study on the damage of hexagonal honeycombs under in–plane shear loading does not appear to be available in the literature. In this paper, shear damage behaviors of five different hexagonal mesostructures are investigated with rapid prototyped polycarbonate (PC) honeycomb coupon samples and proper design of a fixture for shear loading. Effective shear stress-strain curves of PC honeycomb coupons are generated for each shear test and the corresponding local cell wall failure is investigated. Two different failure modes of PC honeycombs were observed primarily depending on the cell wall thickness: The PC honeycombs having a lower cell wall thickness induce the plastic post buckling, resulting in preventing propagation of initial cracks through the cell wall end up with higher plastic load bearing. On the other hand, the failure mode of the honeycombs having a high cell wall thickness is the cell wall fracture by crack propagation through wall without severe buckling."
477,"Chiral honeycombs are auxetic cellular structures that exhibits negative Poison’s ratio. Chiral honeycombs are structures arranged in an array of cylinders connected by ligaments. Four different configurations of these geometries with 4- and 6- ligaments attached are investigated for its use in shear layer of non-pneumatic wheel. The objective of the study is to find an ideal geometry for the shear layer while meeting its requirements of shear properties (about 6.5 MPa effective shear modulus and 0.15 maximum effective shear strain) with polycarbonate as base material. Finite Element (FE) based numerical tests are carried out and optimum chiral meso-structures are found for the target shear properties. Parametric studies on geometries are also conducted to find the effect of geometries on the target properties. The effect of cell wall thickness is studied and the optimum thickness is suggested to meet the target requirements. Effect of direction of shear loading has been studied on each different configuration in order to minimize the effect of direction of loading."
478,"The paper examines the impact of varying two geometric cross-section parameters of an advance composite D-spar on its structural stiffness. For a given blade topology, the orientation of the D-spar web with respect to the beam axis and the distance of the D-spar web from the leading edge of the blade have been selected here as the variables of study, as they govern the elastic properties of the composite cross-section. A code has been developed to calculate the matrix terms of the Euler-Bernoulli cross-sectional stiffness utilizing the closed form expressions of the structural properties formulated by assuming both Thin-Walled composite Beam theory (TWB) and Classical Laminate Theory. The code has been validated through the Variational Asymptotic Beam Sectional analysis (VABS) for the cross-sectional stiffness matrix. Two cases have been studied for a quasi-isotropic laminate D-spar. The first is for a symmetric airfoil, whereas the second is for an unsymmetrical airfoil. The variation of the stiffness parameters for the quasi-isotropic D-spar including the coupling parameters has been visualized into parametric maps. The paper also examines the impact that these geometric variables have on the stiffness-to-mass ratio to show that along with the ply orientations they play a major role in the aeroelastic tailoring and structural optimization of a composite blade."
479,"Considering many potential applications of fiber reinforced metal laminates (FMLs) in sensitive structures, it is necessary to understand their mechanical behavior under impact loads. In this study, low velocity impact tests based on ASTM D7136 have been conducted on FMLs made of 1050 aluminum sheets and various types of fiber reinforced polymer (FRP) layers; namely E-Glass, Kevlar 49, and carbon T300 plain woven in the epoxy resin. Projectile energy, fiber type and the number of successive impacts are selected among important parameters that can affect the performance of FMLs. In particular, the effects of these parameters on the absorbed energy, contact force, front and rear face damage areas, central deflection and permanent deformation of FMLs have been investigated. For determining the damage area and central deflection of the specimens, an image processing method is adapted."
480,"The current practice of gear design is based on the Lewis bending and Hertzian contact models. The former provides the maximum stress on the gear base, while the latter calculates the contact pressure at the contact point between the gear and pinion. Both calculations are obtained at the reference configuration with ideal conditions; i.e., no tolerances and clearances. The first purpose of this paper is to compare these two analytical models with the numerical results, in particular, using finite element analysis. It turns out that the estimations from the two analytical equations are closely matched with those of the numerical analysis. The numerical analysis also yields the variation of contact pressures and bending stresses according to the change in the relative position between gear and pinion. It has been shown that both the maximum bending stress and contact pressure occur at non-reference configurations, which should be considered in the calculation of a safety factor. In reality, the pinion-gear assembly is under the tolerance of each part and clearance between the parts. The second purpose of this report is to estimate the effect of these uncertain parameters on the maximum bending stress and contact pressure. For the case of the selected gear-pinion assembly, it turns out that due to a 0.57% increase of clearance, the maximum bending stress is increased by 4.4%. Due to a 0.57% increase of clearance, the maximum contact pressure is increased by 17.9%."
481,"In this paper a proof is presented that shows that the relation between technical functions and their subfunctions in functional descriptions of products can formally not be taken as a relation of parthood. Technical functions of two specific classes are modelled as well as their composition. In this modelling functions are taken as transformations of tokens of flows of energy, material and signals, which makes them proper instances of functions on many engineering accounts of functions. Then it is proved that the relations between the considered functions and their subfunctions do not in general meet the basic postulates of mereology, the theory of parthood relations. The ramification of this proof is that in engineering ontologies the relation between subfunctions and functions should not be described as a formal parthood relation."
482,"This research proposes a patent-based design process by systematically integrating patent information, the rules of patent infringement judgment, strategies of designing around patents, and innovation design methodologies. The purpose of the process is to systematically generate new design concepts that are local variations of one of the concerned patents but does not infringe with existing patents. The basic idea is to consider patent infringement before engineering design concepts are actually generated. In this process, first the designer conducts standard patent analysis to identify the related patents to be designed around. Each patent is then symbolized by a “design matrix” converted from the technology/function matrix of the patent. A design-around algorithm is developed to generate a new design matrix that does not infringe with design matrices of existing patents. Then the new design matrix is transformed back into a real engineering design using the “contradiction matrix” in TRIZ. A computerized design-around tool based on the innovative patent-based design process is also developed."
483,"As the description of design requirements at the earlier design stage is inaccurate and vague, it is difficult to figure out functional structure of a product and make sense product configuration. Therefore, it plays an important role to formally represent the process of design for product development in the conceptual design stage. Furthermore, port, as the location of intended interaction, is crucial to capture component concept and realize conceptual design for multi-solution generation. Agent is considered as an effective approach to collaboratively implementing design problem solving and reasoning. Combining both port and agent may be employed to generate new concepts of the product in order to customize product scheme varieties. In this paper, the product module attributes are firstly described. The objective is to implement modeling of design process for obtaining system new concepts to guide multi-solution generation. Secondly, an effective approach to decomposing design process is presented to describe the process of structure generations and product decomposition by formal representation. According to properties of modularity for product development and component connections, we can calculate the number of component connections and density of components. In addition, product module division and coupling degree analysis are conducted, and coupling degrees are calculated by considering the correspondence ratio and the cluster independence. A port-based knowledge building process is described for functional modeling. A port-agent collaborative design framework is given and describes different agent functions to help designers to obtain new design schemes. Finally, a case study is presented to describe the modeling process of conceptual design."
484,"This paper presents an industry case study investigating change propagation due to requirement changes. This paper makes use of a change propagation prediction tool, ΔDSM, to identify if the propagated changes could have been identified and predicted. The study used an automation firm’s client project as the study subject. The project entailed 160 requirements, changing over the span of 15 month. Engineering change notifications were developed for each change and documented under the firm’s data management system. This study makes use of the change notifications to identify if any of the change were as a result of a previous change. The findings of this paper indicated the changes that occurred could have been predicted as the ΔDSM was able to predict affected requirements. This was identified by finding subsequent requirements in the engineering change notification documentation that the ΔDSM indicated might change."
485,"The primary objective in design is to achieve the target value of the design’s functional requirement. In design with multiple functional requirements, one way a design fails is the inability to converge to the multiple target values in spite of iterative adjustment of the design parameters. This is symptom of a design that fails to perform in the presence of functional coupling. Functional coupling occurs when two or more functional requirements are affected by a common set of design parameters. It is particularly difficult to identify and break when it involves inter-relation loops created among large number of functional requirements, typical of a large complex system. This paper presents a structured method based on the graph theory to effectively identify and eliminate functional couplings in a design. Use of the graph theory in this context is natural by the fact that inter-relations among functional requirements and design parameters can be represented by a digraph. Each inter-relation corresponds to an arc of the digraph, and functional coupling is equivalent to a cycle in it. The proposed method consists of: 1) represent interactions among functional requirements and design parameters as a digraph, 2) construct the cycle matrix for the digraph, 3) identify those candidate sets of arcs that, if removed, will destroy all cycles in the digraph, and 4) examine engineering feasibility of the candidate solutions. Once target interactions, i.e. arcs, are determined, the design parameters responsible for those interactions are modified to implement the solution. To demonstrate the effectiveness of the proposed method, we apply it to a large complex system, the car door to body, involving 28 functional requirements and design parameters."
486,"Introduced nearly 25 years ago, the paradigm of mass customization (MC) has largely not lived up to its promise. Despite great strides in information technology, engineering design practice, and manufacturing production, the necessary process innovations that can produce products and systems with sufficient customization and economic efficiency have yet to be found in wide application. In this paper, the state-of-the-art in MC is explored in order to answer the question of “why not?” and to highlight areas for specific research in the MC paradigm. To establish perspective for this work, we consider MC to be a product development approach which allows for the production of goods — after a customer places an order — which minimize the tradeoff between the ideal product and the available product by fulfilling the needs and preferences of individuals functionally, emotionally and anthropologically. Results of this research were generated by reviewing 88 papers from various journals that span three domains of interest (marketing, engineering, and distribution) and explore proposed methodologies, specific information inputs and outputs, proposed metrics, and barriers toward the implementation of MC. Qualitatively, we show that the lack of MC in application is due to two factors: 1) a lack of marketing tools capable of capturing individual needs that can be mapped to the technical space; and 2) a lack of information relation mechanisms that connect the domains of marketing, engineering, and distribution. In the end it is our belief that MC is realizable and that eventually it will emerge as a dominant paradigm in the design and delivery of products and systems. However, pursuing the opportunities for research presented in this work will hopefully speed this emergence."
487,"This paper discusses a new concept generation technique that improves upon a previous automated concept generation theory and algorithm developed by Bryant, et al. at the University of Missouri – Rolla. The previous automated concept generation algorithm utilizes the design knowledge present in a repository to produce an array of partial concept solutions. While the previous algorithm is capable of handling branched functional models, it does not efficiently remove all of the infeasible partial solutions to leave only whole concepts in the final results. A matrix-based algorithm is presented in this paper that utilizes the result from the previous concept generation algorithm and solves for complete solutions of branched concepts. The presented algorithm eliminates incomplete and infeasible concepts or components from the results and generates a set of full solutions for further analysis by a designer. The details of the algorithm are described in this paper, and a peanut-sheller example is used to illustrate the effective use of the algorithm for producing branched concept variants."
488,"The effectiveness of the use of game theory in addressing multi-objective design problems has been illustrated. For the most part, researchers have focused on design problems at single level. In this paper, we illustrate the efficacy of using game theoretic protocols to model the relationship between multidisciplinary engineering teams and facilitate decision making at multiple levels. We will illustrate the protocols in the context of an underwater vehicle with three levels that span material and geometric modeling associated with microstructure mediated design of the material and vehicle."
489,"During conceptual design it is desirable to produce many potential solutions. Recently, computational tools have emerged to help designers more fully explore possible solutions. These automated concept generators use knowledge from existing products and the desired functionality of the new design to suggest solutions. While research has shown these tools can increase the variety of solutions developed, they often provide unmanageably large sets of poorly differentiated results. This work proceeds from the hypothesis that automated concept generator output includes many permutations of a relatively few principal solution variants. A method to discover these underlying solution types from the initial concept generator output is proposed. The proposed method employs principal component analysis for variable reduction followed by cluster analysis for classification. The method is applied to the automatically generated solutions of three sample design problems. Preliminary evidence of the utility and efficiency of the proposed method is presented based upon those sample problems. Finally, a method for extending the proposed technique to much larger solution sets is discussed."
490,"A transforming product is a system that has different functionality when physically changed or reconfigured into a different state. This increased functionality allows diverse customer needs to be met in a single product. Transforming devices have become more prevalent in recent years, as customers desire both increased capabilities and reduced complexity to reduce waste in our society. When designing a multifunctional product that transforms from one state to another, it can be difficult to conceptualize a design that does not reduce effectiveness or provide a compromise in either state. Transformational Design Theory has been developed and shows basic principles and facilitators that enable transformation to occur within a product space. An illustrative example is a chair designed to flip over to be used as a table. Flip is one of the 19 facilitators that are found in transformation design. This is also an example of expose/cover, a transformation design principle. Certain principles and facilitators are more prevalent than others in different design domains (such as tools, storage, organisms etc.). If we know the states that exist within the transformer, concept opportunity diagrams can be used to determine the opportunities for transformation within each state. When the diagrams are paired with a constituent relationship chart specific to each domain, new design concepts may be facilitated. This technique creates a cognitive process for designers where they process a series of questions when creating the concept opportunity diagram. The diagram will help them understand the unanticipated additional design space of each state. The Constituent Relationship Chart is a tool that allows them to apply their knowledge of these states to the facilitator hierarchy so that prospective facilitators can directly contribute to originally unforeseen design concepts. This paper presents this twofold process known as the Transformer Diagram Matching Method and shows the results on a fully functioning prototype of an office supply transformer. Although the proposed process is detailed, it allows the designer to find a large number of quality concepts they would not have foreseen otherwise. Our original concept generation processes produced thirty eight ideas, but this process added another thirty two ideas to the design space. The paper indicates specifically how this method can be integrated in with the standard transformational design process as well as suggests strategies for implementation within other design techniques."
491,"The offer of tailored products is a key factor to satisfy specific customer needs in the current competitive market. Modular products can easily support customization in a short time. Design process, in this case, can be regarded as a configuration task where solution is achieved through the combination of modules in overall product architecture. In this scenario efficient configuration design tools are evermore important. Although many tools have been already proposed in literature, they need further investigation to be applicable in real industrial practice, because of the high efforts required to implement system and the lack of flexibility in products updating. This work describes an approach to overcome drawbacks and to introduce a product independent configuration system which can be useful in designing recurrent product modules. To manage configuration from the designer perspective, the approach is based on Configurable Virtual Prototypes (CVP). In particular, the definition of geometrical models is analyzed providing a tool for eliciting and reusing knowledge introduced by parametric template CAD models. Semantic rules are used to recognize parts parameterization and assembly mating constraints. The approach is exemplified through a case study."
492,"Reconfigurable systems are able to meet the increasingly diverse needs of consumers. A reconfigurable system is able to change its configuration repeatedly and reversibly to match the customer’s needs or the surrounding environment, allowing the system to meet multiple requirements. In this paper, a sample of reconfigurable products was studied to better understand the methods used to achieve different configurations. Four methods of reconfiguration were discovered. This expands work previously done in a parallel field with products that transform where only three methods were identified. In order to support the findings of this paper, the variations were identified and example products were presented that clearly show the need for at least one additional method of reconfiguring. A case study is also provided to illustrate the benefits of incorporating four principles, as apposed to three, into the concept generation phase of new reconfigurable product development."
493,"Adaptable design is a new design approach to create an adaptable product to replace multiple products for satisfying the different requirements in the product life-cycle. In this research, a method to identify the optimal product considering changes of requirements, configurations and parameters in the whole product life-cycle is introduced. The requirements, configurations and parameters of the adaptable product are modeled as functions of the life-cycle time parameter. The adaptable product is changed to different configurations and parameters to satisfy the different requirements in different life-cycle time periods. The evaluation measures, which are achieved from configurations and parameters, are also changed in different life-cycle time periods. The optimal product, modeled by its configurations and parameters, considering the whole product life-cycle is identified through optimization. A case study is provided to demonstrate how the introduced method can be employed for solving engineering problems."
495,"Reconfigurable systems that maintain a high level of performance under changing operational conditions and requirements are an ongoing research challenge. Many existing systems are able to work under narrow operational conditions only. They should perform the configuration transitions from the current state to a desired one smoothly. The question is: how to find the optimal state transition trajectory that configures the system from an initial state to a desired state? We present a method to determine steps (list of actions to be taken) of state transitions for reconfigurable systems. This method makes use of graph search algorithms that solve shortest path problems and that are commonly used in routing: Dijkstra’s algorithm and the A* algorithm. The method is applied to the design of a reconfigurable printer, which has to change its configuration to achieve maximum performance (e.g. high quality of print) when operational conditions are changing (e.g. speed of printing)."
496,"Reconfigurable products can adapt to new and changing customer needs. One potential, high-impact, area for product reconfiguration is in the design of income-generating products for poverty alleviation. Non-reconfigurable income-generating products such as manual irrigation pumps have helped millions of people sustainably escape poverty. However, millions of other impoverished people are unwilling to invest in these relatively costly products because of the high perceived and actual financial risk involved. As a result, these individuals do not benefit from such technologies. Alternatively, when income-generating products are designed to be reconfigurable, the window of affordability can be expanded to attract more individuals, while simultaneously making the product adaptable to the changing customer needs that accompany an increased income. The method provided in this paper significantly reduces the risks associated with purchasing income-generating products while simultaneously allowing the initial purchase to serve as a foundation for future increases in income. The method presented builds on principles of multiobjective optimization and Pareto optimality, by allowing the product to move from one location on the Pareto frontier to another through the addition of modules and reconfiguration. Elements of product family design are applied as each instantiation of the reconfigurable product is considered in the overall design optimization of the product. The design of a modular irrigation pump for developing nations demonstrates the methodology."
497,"Usage Context-Based Design (UCBD) is an area of growing interest within the design community. A framework and a step-by-step procedure for implementing consumer choice modeling in UCBD are presented in this work. To implement the proposed approach, methods for common usage identification, data collection, linking performance with usage context, and choice model estimation are developed. For data collection, a method of try-it-out choice experiments is presented. This method is necessary to account for the different choices respondents make conditional on the given usage context, which allows us to examine the influence of product design, customer profile, usage context attributes, and their interactions, on the choice process. Methods of data analysis are used to understand the collected choice data, as well as to understand clusters of similar customers and similar usage contexts. The choice modeling framework, which considers the influence of usage context on both the product performance, choice set and the consumer preferences, is presented as the key element of a quantitative usage context-based design process. In this framework, product performance is modeled as a function of both the product design and the usage context. Additionally, usage context enters into an individual customer’s utility function directly to capture its influence on product preferences. The entire process is illustrated with a case study of the design of a jigsaw."
498,"A series of products, i.e. a product family is deployed for effectively and flexibly meeting with a variety of customer’s needs under a given product platform. Since such a deployment consumes various engineering resources and simultaneously brings profits gradually over the time sequence, when and how respective modules are designed and respective products are launched to the market must be rationally planed. Further, as a nature of product families, module commonalization accelerates the deployment but infuses some overheads on features and production cost. This paper investigates such a product family deployment problem under the optimal design viewpoint. After some general discussions, a mathematical model of dynamic design decisions is conditionally developed by integrating a combinatorial optimization technique for decision of module selection on commonalization and a market system model with discrete choice analysis and for describing the compromise among sequence of product rollout, arrangement of product lineup, required engineering resource, expected profit, etc. Then, the compromise among those factors is illustrated through the case study on a simplified deployment problem of circuit boards for digital television sets. Finally, an optimal planning approach for product family deployment and accompanied resource allocation is envisioned based on the developed model and findings from the case studies."
499,"Market players, such as competing manufacturing firms and retail channels, can significantly influence the demand and profit of a new product. Existing methods in design for market systems use game theoretic models that can maximize a focal manufacturing firm’s profit with respect to product design and price variables given the Nash equilibrium of the market system. However, in the design for uncertain market systems, there is seldom equilibrium with players having fixed strategies in a given time period. In this paper, we propose an agent based approach for design for market systems that accounts for learning behaviors of the market players under uncertainty. By learning behaviors we mean that market players gradually, over a time period, learn to play with better strategies based on action-reaction behaviors of other players. We model a market system with agents representing competing manufacturers and retailers who possess learning capabilities and are able to automatically react and make decisions on the product design and pricing. The proposed approach provides strategic design and pricing decisions for a focal manufacturer in response to anticipated reactions from market players in the short and long term horizons. Our example results show that the proposed agent based approach can produce competitive strategies for a focal firm over a time period when market players react only by setting prices compared to a game theoretic approach. Furthermore, it can yield profitable product design decisions and competitive strategies when competing firms react by changing design attributes in the short term — a case for which no previous method in design for market systems has been reported."
500,"Recently, it has become increasingly important to provide customers with highly creative services to attain differentiation from competing firms. Fulfilling customer requirements with such creative service contents is an effective way to differentiate a firm’s services from those of its competitors. However, there have been few studies that directly support the creation of new service contents, e.g., service design support using information-processing technology for knowledge by the computer. It is valid for service providers to acquire creative service design solutions that fulfill customer requirements. In this paper, we propose a support method to fulfill customer requirements for a target service by using the functions of another type of service. This method supports the acquisition of new service design solutions on the basis of the similarity with the customer requirements or functions of different services. The proposed method is verified by applying it to an existing service case."
501,"This paper presents a massively parallel Biogeography-based Optimization – Pattern Search (BBO-PS) algorithm with graphics hardware acceleration on bound constrained optimization problems. The objective of this study was to determine the effectiveness of using Graphics Processing Units (GPU) as a hardware platform for BBO-PS. GPU, the common graphics hardware found in modern personal computers (PC), can be used for data-parallel computing in a desktop setting. In this research, the BBO was adapted in the data-parallel GPU computing platform featuring ‘Single Instruction – Multiple Thread’ (SIMT). The global optimal search of the BBO was enhanced by the classical local Pattern Search (PS) method. The hybrid BBO-PS method was implemented in the GPU environment, and compared to a similar implementation in the common computing environment with a Central Processing Unit (CPU). Computational results indicated that GPU-accelerated SIMT-BBO-PS method was orders of magnitude faster than the corresponding CPU implementation. The main contribution of this paper was the parallelization analysis and performance analysis of the hybrid BBO-PS with GPU acceleration. The research result was significant in that it demonstrated a very promising direction for high speed optimization with desktop parallel computing on a personal computer (PC)."
502,"Many engineering design problems deal with global optimization of constrained black-box problems which is usually computation-intensive. Ref. [1] proposed a Mode-Pursuing Sampling (MPS) method for global optimization based on a sampling technique which systematically generates more sample points in the neighborhood of the function mode while statistically covering the entire problem domain. In this paper, we propose a novel and more efficient sampling technique which greatly enhances the performance of the MPS method, especially in the presence of "
503,"In design preference elicitation, we seek to find individuals’ design preferences, usually through an interactive process that would need only a very small number of interactions. Such a process is akin to an optimization algorithm that operates with point values of an unknown function and converges in a small number of iterations. In this paper, we assume the existence of individual preference functions and show that the elicitation task can be translated into a derivative-free optimization (DFO) problem. Different from commonly-studied DFO formulations, we restrict the outputs to binary classes discriminating sample points with higher function values from those with lower values, to capture people’s natural way of expressing preferences through comparisons. To this end, we propose a heuristic search algorithm using support vector machines (SVM) that can locate near-optimal solutions with a limited number of iterations and a small sampling size. Early experiments with test functions show reliable performance when the function is not noisy. Further, SVM search appears promising in design preference elicitation when the dimensionality of the design variable domain is relatively high."
504,"Many heuristic optimization approaches have been developed to combat the ever-increasing complexity of engineering problems. In general, these approaches can be classified based on the diversity of the search strategies used, the amount of change to those search strategies during the optimization process, and the level of cooperation between the strategies. A review of the literature indicates that approaches which are simultaneously very diverse, highly dynamic, and cooperative are rare but have immense potential for finding high quality final solutions. In this work, a taxonomy of heuristic optimization approaches is introduced and used to motivate a new approach, entitled Protocol-based Multi-Agent Systems. This approach is found to produce final solutions of much higher quality when its implementation includes the use of multiple search protocols, the adaptation of those protocols during the optimization, and the cooperation between the protocols than when these characteristics are absent."
505,"This paper presents a hybrid genetic algorithm that expands upon the previously successful approach of twinkling genetic algorithm (TGA) by incorporating a highly efficient local fuzzy-simplex search within the algorithm. The TGA was in principle a bio-mimetic algorithm that introduced a controlled deviation from a typical GA method, by not requiring that every genevariable of an offspring be the result of a crossover. Instead, twinkling allowed the genetic information of the randomly chosen gene locations to be directly passed on from one parent, which was shown to increase the likelihood of survival of a successful gene value within the offspring, rather than requiring it to be blended. The twinkling genetic algorithms proved highly effective at locating exact global optimum with a competitive rate of convergence for a wide variety of benchmark problems. In this work, it is proposed to couple the TGA with a fuzzy simplex local search to increase the rate of convergence of the algorithm. The proposed algorithm is tested using common mathematical and engineering design benchmark problems. Comparison of the results of this algorithm with earlier algorithms is presented."
506,"In the context of the Usage Context Based Design (UCBD) of a product-service, a taxonomy of variables is suggested to setup the link between the design parameters of a product-service and the part of a set of expected usages that may be covered. This paper implements a physics-based model to provide a performance prediction for each usage context that also depends on the user skill. The physics describing the behavior and consequently the performances of a jigsaw are established. Simulating numerically the usage coverage is non trivial for two reasons: the presence of circular references in physical relations and the need to efficiently propagate value sets or domains instead of accurate values. For these two reasons, we modeled the usage coverage issue as a Constraint Satisfaction Problem and we result in the expected service performances and a value of a covered usage indicator."
507,"This study presents a novel, quantitative tool for design decision-making for products designed for human variability. Accommodation, which describes the ability of a user to interact with a device or environment in a preferred way, is a key product performance metric. Methods that offer a better understanding of accommodation of broad user populations would allow for the design of products that are more cost-effective, safer, and/or lead to greater levels of customer satisfaction. Target user populations are often characterized by measures of anthropometry, or body dimensions. A methodology is proposed that uses a visual analysis method for understanding and exploring accommodation across the variability in anthropometry of a target user population. This is achieved by assessing binary accommodation of individuals using a “virtual fit” method and examining trends in binary accommodation across the range of anthropometric variability, referred to as the “anthropometry space”. Various factors influencing accommodation, such as user preference independent of anthropometry and the quality of a design, are also discussed and are an important contribution of the work. Two demonstration studies are presented that illustrate the methodology and provide opportunity for discussion of its impact. The first study investigates the simple univariate problem of dimensionally optimizing the seat height and range of adjustability of an exercise cycle. The second study investigates the more complex problem of optimally configuring the driver package of a commercial truck."
508,"In products designed for human variability, the anthropometry (body measurements) of the target user population constitutes a primary source of variability that must be considered in the optimization of the spatial dimensions of the product. Accommodation, which describes the ability of a user to interact with a device or environment in their preferred manner, is a key measure of its performance. Other studies have considered various methods for accounting for the variability in anthropometry in a target user population to calculate estimated accommodation, but few have explicitly considered the effects of secular trends and demographic changes over time. This paper considers these changes in the context of a case study involving truck drivers and cab geometry. The truck driver populations are used to illustrate changes in body size and shape over a 30-year period and show how they affect user acceptability of designs. Changes in the gender split of the driver population are also considered, and are shown to have a significant effect on accommodation. The work demonstrates that secular trends and demographic changes over time significantly affect accommodation, but a well designed product will be more robust to these changes."
509,"Virtual prototyping allows us to reduce the expensive production of real prototypes to a minimum and shorten vehicle development phases. Augmented Reality (AR) visualization is a demonstrative and intuitive tool in order to overlay physical prototypes with virtual content and thus comprehend complex relationships quickly. Further, AR technologies can intensify the collaboration between specialists with different expert knowledge and support common decision making during reviews meetings. However, the existing work processes, software tools and predefined regulations do not permit the use of AR tools in all automotive areas. In this work, a prototypical AR application, based on optical tracking tools, was set up in a real automotive environment and evaluated in terms of its applicability for CFD simulation data in the passenger compartment. The examination provides valuable information about environmental conditions, requirements from end users as well as the integration in existing work processes. The results are a basis for future improvements in order to offer a seamless and automated workflow in an early state of the development process and for maintenance."
510,"The paper elucidates how to connect forming process simulation with innovative measurement- and analysis equipment thereby taking into account the machine influences. Reverse Engineering use 3D-Scanning data of sheet metal forming dies. Following this paradigm, the models simulation relies on are refined, and spotting of forming dies is subjected to a scientific analysis. That means, that with Reverse Engineering, “extended process engineering” is verified at the real spotting procedure, the comparison of simulation- and measuring results is used to evaluate how close the investigated models are to reality, extending the optimisation algorithms used for springback compensation to die spotting, the modification of the die topology will be carried out automatically thanks to new software functions."
511,"The zigzag and offset path have been the two most popular path patterns for tool movement in machining process. Different from the traditional machining processes, the quality of parts produced by the metal deposition process is much more dependent upon the choice of deposition paths. Due to the nature of the metal deposition processes, various tool path patterns not only change the efficiency but also affect the deposition height, a critical quality for metal deposition process. This paper presents the research conducted on calculating zigzag pattern to improve efficiency by minimizing the idle path. The deposition height is highly dependent on the laser scanning speed. The paper also discussed the deposition offset pattern calculation to reduce the height variation by adjusting the tool-path to achieve a constant scanning speed. The results show the improvement on both efficiency and height."
512,"With multi-axis capability, direct laser deposition process can produce a metal part without the usage of support structures. In order to fully utilize such a capability, the paper discusses a slicing method for multi-axis metal deposition process. Using the geometry information of adjacent layers, the slicing direction and layer thickness can be changed as needed. A hierarchy structure is designed to manage the topological information which is used to determine the slicing sequence. Its usage is studied to build overhang type structure. With such a character, some overhang features such as holes, can be deposited directly to save the required machining operation and material cost, which improves the efficiency of the metal deposition process. Combined with direct 3D layer deposition technique, the multi-axis slicing method is implemented."
513,"Recently, the use of bioresorbable materials (e.g., β-tricalcium phosphate (β-TCP)) has enabled the development of autologous-bone-replaceable artificial bones that are degraded and resorbed, i.e., replaced with autologous bone, when placed inside the human body for a sufficiently long duration. Although such autologous-bone replaceability requires high porosity of the artificial bone to promote the ingression of blood vessels and cells, the high porosity reduces the mechanical strength, which leads to disadvantages such as possible fracture after bone substitution surgery. One solution to this problem is to optimally arrange low-porosity portions for mechanical strength and high-porosity portions for autologous-bone replaceability in solid artificial bones. Commercially available artificial bones typically have fixed shapes such as a rectangular parallelepiped or cylinder. The use of recent solid freeform fabrication technologies, however, has enabled solid artificial bones with various shapes to be customized for individual medical cases. In this paper, the authors propose a solid freeform fabrication method for autologous-bone-replaceable artificial bones with a porosity distribution. A β-TCP porous artificial bone can be fabricated by placing a slurry consisting of β-TCP powder, water, a peptization reagent and a frother in a mold, drying it to form a solid shape and then sintering it. This β-TCP slurry contains ammonium polyacrylate as the peptization reagent, which is an electrolyte, and ammonia, hydrogen and oxygen gases are produced from its electrolysis. The authors conceived the idea of controlling the foaming of the β-TCP slurry by electrolysis, and of designing and implementing a fabrication system consisting of a fine nozzle with a microscrew for extruding β-TCP slurry as a filament and electrodes for controlling the electrolysis of the slurry. Using this system, we can fabricate a solid shape by drawing two-dimensional sections with the slurry filament and stacking each section, and at the same time vary the porosity by controlling the electric current applied for the electrolysis of the slurry. Using the experimental system, three β-TCP porous samples (approximately 18mm × 18mm × 9mm) of high (71.8%), medium (59.5%) and low (54.6%) porosity are successfully fabricated by applying electric currents of 20mA, 10mA and 0mA, respectively. Then a β-TCP porous sample (approximately 40mm × 10mm × 10mm) with a gradient porosity distribution (from 72.3% to 56.1%) is successfully fabricated by varying the electric current from 0mA to 20mA in a continuous fabrication process. From these results, the authors confirm the efficacy and potential of the proposed approach."
514,"Additive manufacturing (AM) processes based on mask image projection such as "
515,"This research carries out a systematic investigation on the automation of fixture location planning. A novel classification and representation method of constrained degrees of freedom (DOF) is proposed. Constrained DOFs are classified into four types: linear translation DOF, linear rotation DOF, planar translation DOF and planar rotation DOF, and they are represented in the form of vector. On the basis of the classification and representation, constrained DOFs are automatically obtained from process requirement. By analyzing datum constraint DOF capability and datum location ability, the operation rules of constrained DOF: union, decomposition and coordinate transformation, are established. The approach to the evaluation of datum location ability and the judgment of datum conflict is explored, and the solution of datum conflict and datum location problem is presented. Eventually, the rules of location point layout are formalized."
519,"We demonstrate a technique to evaluate the aerodynamic robustness of a given blade profile which it is exposed to stochastic geometrical variation. The technique is based on random fields, with geometrical deviations continuously defined over the entire structure, with a prescribed statistical distribution function and a given correlation between these deviations. Control points are defined on the blade surface to model the blade geometry disturbances. At each control point a stochastic deviation is defined, which acts in the normal direction of the blade. By modeling disturbances in the normal direction instead of in the separate Cartesian directions, we automatically reduce the number of stochastic variables by a factor two. The perturbation variables are transformed via Karhunen-Loève eigenvalue decomposition, giving stochastically independent variables. The robustness is finally estimated by a Monte Carlo simulation, where computational fluid dynamic simulations are performed to evaluate the resulting change in blade performance for given geometrical perturbations."
521,"Symmetry properties of components have many applications during a product development process, including shape transformations for modification purposes, Finite Element Analysis (FEA), model retrieval, etc. This paper presents an algorithm to generate 3D model symmetry planes using the B-Rep model of CAD volumes. In the framework of CAD software, 3D models are described as B-Rep volume models. Design processes of volume models strongly rely on extrusion and revolution primitives from sketches containing essentially straight line segments and circular arcs. Hence, the boundary surfaces considered are planes, cylinders, cones, tori and spheres. The object boundary is effectively processed as an infinite set of points. Global symmetry properties of faces are derived to initiate the global symmetry planes of the object. To this end, the intersection curves between two adjacent faces are used to characterize possible global symmetry planes of the object. Then, the algorithm starts analyzing the symmetry properties of couple of faces. Subsequently, the candidate symmetry planes set up contains all the possible global symmetry planes. Finally, the symmetry properties of neighboring faces help determining robustly the global symmetry planes, whether there is a finite number or an infinite number."
522,"Locating cranes is critical toward safely lifting in construction industry. In this paper, based on overlapping work envelopes, we present optimized solutions for collision-free locating of mobile cranes. Our solution first determines the feasible location areas of the cranes, which are discretized with regular grids. We then incorporate a collision detection method, through which the locations with potential collision are eliminated. With the objective of minimizing the weight sum about safety, a smart search is further performed to identify the most suitable locations. Our solution has been implemented in the intelligent computation module for 3D lifting system developed by our group. Its feasibility and effectiveness has also been demonstrated through a concrete case study."
523,"A mesh movement algorithm suitable for aerodynamic design optimization problems is presented. It involves B-spline surface construction, projection and evaluation on B-spline faces for the surface mesh movement, as well as inverse-distance and 2D/3D TFI interpolations for the volume mesh deformation. The algorithm is fast and exhibits an excellent parallel efficiency. It is used to deform the surface and volume mesh of an ONERA-M6 wing undergoing several planform changes. The quality of the deformed mesh is preserved as long as the difference between the initial surface mesh and the B-spline surface model is small. A good agreement reported between the flow simulation results on the deformed mesh and those obtained on initial fixed mesh."
524,"Fundamental concepts for obtaining optimum product designs from higher view points are presented. These fundamental concepts are: (1) concepts that aim to achieve designs that are in harmony with natural and human environments; (2) collaborations based on mutuality, where the collaborating individuals are free from inhibiting hierarchies; and (3) methodologies that enable discovery of effective solutions by examining the deepest, most fundamental levels of design problems. Product designs that minimize stress on natural environments and maximize benefit to people are increasingly important, given limited natural resources and an increasing world population. The achievement of such product designs generally requires collaborative scenarios based on mutuality and equality of the participants, and the implementation of characteristics-based hierarchical optimization methodologies. Practical methodologies based on these fundamental concepts are discussed here and examples are provided."
525,"Design tools which appear to manage complexity through their inherent behavior do not appear to have been developed specifically for complexity management. This research explores how complexity is managed within the design process through: the generation of complexity within the design process (sources), the techniques which were used to manage complexity (approaches), and the examination of design tools with respect to complexity. Mappings are developed between the sources, the approaches, and the tools with respect to phases of design. The mappings are propagated through these distinct, yet adjacent domains in order to study how the tools might be able to be used to manage complexity sources found in different stages of the design process. As expected, the highest value for each design tool is found in the stage of design in which the tool is traditionally been used. However, there are secondary ratings which suggest that design tools can be used in other stages of the design process to manage specific aspects of complexity."
526,"Complex design problems are typically decomposed into smaller design problems that are solved by domain-specific experts who must then coordinate their solutions into a satisfactory system-wide solution. In set-based collaborative design, collaborating engineers coordinate themselves by communicating multiple design alternatives at each step of the design process. The goal in set-based collaborative design is to spend additional resources exploring multiple options in the early stages of the design process, in exchange for less iteration in the latter stages, when iterative rework tends to be most expensive. Several methods have been proposed for representing sets of designs, including intervals, surrogate models, fuzzy membership functions, and probability distributions. In this paper, we introduce the use of Bayesian networks for capturing sets of promising designs, thereby classifying the design space into satisfactory and unsatisfactory regions. The method is compared to intervals in terms of its capacity to accurately classify satisfactory design regions as a function of the number of available data points. A simplified, multilevel design problem for an unmanned aerial vehicle is presented as the motivating example."
527,"In engineering design, matrices have been commonly used to capture dependency relationships for structure-related problems (e.g., product architecture, process workflow, and team organization). In this context, structuring is considered a group formation process that clusters the design entities and identifies the interactions among the formed groups. To support matrix-based design structuring, this paper proposes a clustering approach that has three phases in the working procedure. Firstly, the coupling analysis is used to assess the coupling strength of any two entities according to the application context. Secondly, the sorting analysis is used to organize the matrix’s rows and columns by bringing the highly coupled entities close to each other, thus yielding a sorted matrix. Thirdly, the partitioning analysis is applied to form a structured matrix that identifies the groups of entities and their interactions based on some structural criteria (e.g., number of groups, limits on group sizes, etc). The proposed clustering method has been applied to four engineering examples to demonstrate its flexibility and adaptability in tackling different design structuring problems."
528,"When designing complex systems, it is often the case that a design process is subjected to a variety of unexpected inputs, interruptions, and changes. These disturbances can create unintended consequences including changes to the design process architecture, the planned design responsibilities, or the design objectives and requirements. In this paper a specific type of design disturbance, mistakes, is investigated. The impact of mistakes on the convergence time of a distributed multi-subsystem optimization problem is studied for several solution process architectures. A five subsystem case study is used to help understand the ability of certain architectures to handle the impact of the mistakes. These observations have led to the hypothesis that selecting distributed design architectures that minimize the number of iterations to propagate mistakes can significantly reduce their impact. It is also observed that design architectures that converge quickly tend to have these same error damping properties. Considering these observations when selecting distributed design architectures can passively reduce the impact of mistakes."
529,"This paper discusses the application of Support Vector Regression (SVR) for modeling the non-linear and hysteretic behavior exhibited by mechanical snubbing systems. Though the discussion in this paper is limited to the application of SVR to snubbing in elastomeric isolators, the approach is generic and can be applied to other dynamic systems or to systems exhibiting hysteretic behavior. A theoretical model that represents the coupled dynamics of an isolation system with the corresponding snubbing system for a single degree-of-freedom system is proposed. The theoretical model is experimentally validated and is subsequently used to build a metamodel using SVR. The results of the metamodel are compared to the theoretical model for a simulation example and are found to be comparable, thereby reducing the computational time for the design and analysis of the snubbing system by orders of magnitude. The SVR based metamodel can, therefore, be used to substitute the computationally intense theoretical model for performing design iterations and design optimization of the snubbing system, significantly reducing model complexity as well as computational time during the design cycle."
530,"In early stages of the engineering design process it is necessary to explore the design space to find a feasible range or point that satisfies the design requirements. When robustness of the system is among the requirements, the Robust Concept Exploration Method (RCEM) can be used. In RCEM a metamodel such as a global response surface of the entire design space is used. Based on this surrogate model the robustness of the system is evaluated. In nonlinear or multimodal design spaces a very detailed metamodel such as a very high order response surface might be required to reflect accurately the characteristics of the model. For large design spaces this is computationally very expensive. In this paper, using the Probabilistic Collocation Method (PCM) for generating local response models at the points of interest, a Simulation-Based RCEM is proposed as a very efficient and flexible robust concept exploration method. We believe that using the PCM with other design exploration methods would be equally effective."
531,"Design analysis and optimization based on high-fidelity computer experiments is commonly expensive. Surrogate modeling is often the tool of choice for reducing the computational burden. However, even after years of intensive research, surrogate modeling still involves a struggle to achieve maximum accuracy within limited resources. This work summarizes advanced and yet simple statistical tools that help. We focus on four techniques with increasing popularity in the design automation community: (i) screening and variable reduction in both the input and the output spaces, (ii) simultaneous use of multiple surrogates, (iii) sequential sampling and optimization, and (iv) conservative estimators."
532,"Modeling of h igh dimensional e xpensive b lack-box (HEB) functions is challenging. A recently developed method, radial basis function-based high dimensional model representation (RBF-HDMR), has been found promising. This work extends RBF-HDMR to enhance its modeling capability beyond the current second order form and “uncover” black-box functions so that not only a more accurate metamodel is obtained, but also key information of the function can be gained and thus the black-box function can be turned “white.” The key information that can be gained includes 1) functional form, 2) (non)linearity with respect to each variable, 3) variable correlations. The resultant model can be used for applications such as sensitivity analysis, visualization, and optimization. The RBF-HDMR exploration is based on identifying the existence of certain variable correlations through derived theorems. The adaptive process of exploration and modeling reveals the black-box functions till all significant variable correlations are found. The black-box functional form is then represented by a structure matrix that can manifest all orders of correlated behavior of variables. The proposed approach is tested with theoretical and practical examples. The test result demonstrates the effectiveness and efficiency of the proposed approach."
533,"Metamodels have been proposed in the literature to reduce the time and resources devoted to design space exploration, to learn about design trade-offs, and to find the best solution to the design problem in the context of simulation-based design and optimization. In previous work in engineering design based on multiple performance criteria, we have proposed the use of Multi-response Bayesian Surrogate Models (MR-BSM) to model several response variables simultaneously, instead of modeling them independently. By doing so, it is expected that the correlation among the response variables can be used to achieve better models with smaller data sets. In this work, we extend the capabilities of MR-BSM by developing a multistage formulation with non-stationary covariance functions. This formulation for multi-response metamodeling in successive stages of experimental design, data acquisition and model fitting, enables the integration of different sources of information about system responses, with different levels of accuracy, into a single, global model of the system. The feasibility of the proposed formulation is demonstrated with an example in which two test functions are jointly approximated in two stages. In addition, we demonstrate the potential of the methodology to take advantage of a priori information, expressed as upper and lower bounds on the responses, to improve the accuracy of the metamodels. Results show that the use of bound information can result in order-of-magnitude improvements in metamodel accuracy."
534,"Plug-in hybrid electric vehicles (PHEVs) have potential to reduce greenhouse gas (GHG) emissions in the U.S. light-duty vehicle fleet. GHG emissions from PHEVs and other vehicles depend on both vehicle design and driver behavior. We pose a twice-differentiable, factorable mixed-integer nonlinear programming model utilizing vehicle physics simulation, battery degradation data, and U.S. driving data to determine optimal vehicle design and allocation for minimizing lifecycle greenhouse gas (GHG) emissions. The resulting nonconvex optimization problem is solved using a convexification-based branch-and-reduce algorithm, which achieves global solutions. In contrast, a randomized multistart approach with local search algorithms finds global solutions in 59% of trials for the two-vehicle case and 18% of trials for the three-vehicle case. Results indicate that minimum GHG emissions is achieved with a mix of PHEVs sized for around 35 miles of electric travel. Larger battery packs allow longer travel on electric power, but additional battery production and weight result in higher GHG emissions, unless significant grid decarbonization is achieved. PHEVs offer a nearly 50% reduction in life cycle GHG emissions relative to equivalent conventional vehicles and about 5% improvement over ordinary hybrid electric vehicles. Optimal allocation of different vehicles to different drivers turns out to be of second order importance for minimizing net life cycle GHGs."
535,"The optimal design of hybrid power generation systems (HPGS) can significantly improve the economical and technical performance of power supply. However, the discrete-time simulation with logical disjunctions involved in HPGS design usually leads to a nonsmooth optimization model, to which well established techniques for smooth nonlinear optimization could not be directly applied. This paper proposes a multistage design optimization problem with complementarity constraints approach for HPGS design, which introduces a complementarity formulation of the nonsmooth logical disjunction, as well as a multistage decomposition framework, to ensure a fast local solution. A numerical study of a stand-alone hybrid photovoltaic (PV)/wind power generation system is presented to demonstrate the effectiveness of the proposed approach."
536,"The Morgantown Personal Rapid Transit (M-PRT) system is a comfortable conveyance for travel in Morgantown, WV. One of its operating concerns is the increasing cost of heat to the guideway during winter. As the vehicles cannot run safely during snow, the system includes a guideway heating system to melt the ice from the guideway. To reduce the use of expensive natural gas, an interest has been expressed to define a hybrid heating system using an alternate fuel supply. Solid Oxide Fuel Cell (SOFC) was incorporated in the hybrid heating system. This hybrid heating system was designed, and then a detailed analysis was performed to ascertain the performance parameters like heat produced, thermal efficiency, cost of the system and the emissions involved. This high temperature fuel cell releases large amounts of usable heat in the form of exhaust gases. The exhaust gases are deprived of any undesired emissions that pollute the atmosphere. A USDOE EPSCoR WV State Implementation Award conducted by Advance Power Electricity Research Center (APERC) at West Virginia University provided support for conducting this research."
537,"This paper explores the application of genetic algorithms (GA) for optimal design of reverse osmosis (RO) water desalination systems. While RO desalination is among the most cost and energy efficient methods for water desalination, optimal design of such systems is rarely an easy task. In these systems, salty water is made to flow at high pressure through vessels that contain semi-permeable membrane modules. The membranes can allow water to flow through, but prohibit the passage of salt ions. When the pressure is sufficiently high, water molecules will flow through the membranes leaving the salt ions behind and are collected in a fresh water stream. Typical system design variables for RO systems include the number and layout of the vessels and membrane modules, as well as the operating pressure and flow rate. This paper explores models for single and two-stage RO pressure vessel configurations. The number and layout of the vessels and membrane modules are regarded as discrete variables, while the operating pressures and flow rate are regarded as continuous variables. GA is applied to optimize the models for minimum overall cost of unit produced fresh water. Case studies are considered for four different water salinity concentration levels. In each of the studies, three different types of crossover are explored in the GA. While all the studied crossover types yielded satisfactory results, the crossover types that attempt to exploit design variable continuity performed slightly better, even for the discrete variables of this problem."
538,"This paper explores optimal design of reverse osmosis (RO) systems for water desalination. In these systems, salty water flows at high pressure through vessels containing semi-permeable membrane modules. The membranes can allow water to flow through, but prohibit the passage of salt ions. When the pressure is sufficiently high, water molecules will flow through the membranes leaving the salt ions behind, and are collected in a fresh water stream. Typical system design variables include the number and layout of the vessels and membrane modules, as well as the operating pressure and flow rate. This paper presents models for single and two-stage pressure vessel configurations. The models are used to explore the various design scenarios in order to minimize the cost and energy required per unit volume of produced fresh water. Multi-objective genetic algorithm (GA) is used to generate the Pareto-optimal design scenarios for the systems. Case studies are considered for four different water salinity concentration levels. Results of the studies indicate that even though the energy required to drive the RO system is a major contributor to the cost of fresh water production, there exists a tradeoff between minimum energy and minimum cost. An additional parametric study on the unit cost of energy is performed in order to explore future trends. The parametric study demonstrates how an increase in the unit cost of energy may shift the minimum cost designs to shift to more energy-efficient design scenarios."
539,"An extended pattern search approach is presented for optimizing the placement of wind turbines on a wind farm. The algorithm will develop a two-dimensional layout for a given number of turbines, employing an objective function that minimizes costs while maximizing the total power production of the farm. The farm cost is developed using an established simplified model that is a function of the number of turbines. The power development of the farm is estimated using an established simplified wake model, which accounts for the aerodynamic effects of turbine blades on downstream wind speed, to which the power output is directly proportional. The interaction of the turbulent wakes developed by turbines in close proximity largely determines the power capability of the farm. As pattern search algorithms are deterministic, multiple extensions are presented to aid escaping local optima by infusing stochastic characteristics into the algorithm. This stochasticity improves the algorithm’s performance, yielding better results than purely deterministic search methods. Three test cases are presented: a) constant, unidirectional wind, b) constant, multidirectional wind, and c) varying, multidirectional wind. Resulting layouts developed by this extended pattern search algorithm develop more power than previously explored algorithms with the same evaluation models and objective functions. In addition, the algorithm’s layouts motivate a heuristic that yields the best layouts found to date."
540,"The EcoCAR Challenge team at The Ohio State University has designed a range-extending electric vehicle capable of 40 miles all-electric range via a 22 kWh lithium-ion battery pack, with range extension and limited parallel operation supplied by a 1.8 L dedicated E85 engine. This vehicle is designed to drastically reduce fuel consumption, with an estimated fuel economy of 89 miles per gallon gasoline equivalent (mpgge), while meeting Tier II Bin 5 emissions standards. This paper documents the team’s control system development effort, starting with the vehicle architecture selection and specifying the powertrain configuration, explaining a detailed control system development process, summarizing the selected control hardware architecture at vehicle and component level, describing supervisory control algorithm design and implementation for fuel economy optimization and performance improvement, and concluding with the use of MIL and HIL techniques for system development and validation."
541,"Plug-in hybrid electric vehicles (PHEVs) have the potential to reduce green house gases emissions and provide a promising alternative to conventional internal combustion engine vehicles. However, PHEVs have not been widely adopted in comparison to the conventional vehicles due to their high costs and short charging intervals. Since PHEVs rely on large storage batteries relative to the conventional vehicles, the characteristics and design issues associated with PHEV batteries play an important role in the potential adoption of PHEVs. Consumer acceptance and adoption of PHEVs mainly depends on fuel economy, operating cost, operation green house gas (GHG) emissions, power and performance, and safety among other characteristics. We compare the operational performance of PHEV20 (PHEV version sized for 20 miles of all electric range) based on fuel economy, operating cost, and greenhouse gas (GHG) emissions through Pareto set point identification approach for 15 different types of batteries, including lithium-ion, nickel metal hydride (NiMH), nickel zinc (NiZn), and lead acid batteries. It is found that two from 15 batteries dominate the rest. Among the two, a NiMH (type ess_nimh_90_72_ovonic) gives the highest fuel economy, and a lithium-ion (type ess_li_7_303) yields the lowest operating cost and GHG emissions. From comparing nine batteries that are either on or close to the Pareto frontier, one can see that lithium-ion and NiMH batteries offer better fuel economy than lead-acid batteries. Though lithium-ion batteries bear clear advantage on operating costs and GHG emissions, NiMH and lead-acid batteries show similar performances from these two aspects."
542,"Customer preferences for sustainable products are dependent upon the context in which the customer makes a purchase decision. This paper investigates a case study in which fifty-five percent of survey customers say they prefer recycled paper towels, but do not purchase them. These customers represent a profit opportunity for a firm. This paper explores the impact of investing capital in activating pro-environmental preferences on a firm’s profitability and greenhouse gas (GHG) emissions through a multi-objective optimization study. A product optimization is designed to include models of carbon dioxide emissions, manufacturing costs, customer preference, and technical performance. Because the optimization includes a tradeoff between recycled paper and performance, a model of customer preferences, and a market of competing products, the maximum GHG reduction occurs at less than 100% recycled paper. Also, the tradeoff between GHG reductions and profit is not dictated by the configuration of the product, but instead by its price. These results demonstrate the importance of including customer preferences with engineering performance in design optimization. Investment in the activation of pro-environmental preferences is high at all points on the Pareto optimal frontier, suggesting that further engineering design research into the activation of pro-environmental product preferences is warranted."
543,"This paper develops a cost model for onshore wind farms in the U.S.. This model is then used to analyze the influence of different designs and economic parameters on the cost of a wind farm. A response surface based cost model is developed using Extended Radial Basis Functions (E-RBF). The E-RBF approach, a combination of radial and non-radial basis functions, can provide the designer with significant flexibility and freedom in the metamodeling process. The E-RBF based cost model is composed of three parts that can estimate (i) the installation cost, (ii) the annual Operation and Maintenance (O&M) cost, and (iii) the total annual cost of a wind farm. The input parameters for the E-RBF based cost model include the rotor diameter of a wind turbine, the number of wind turbines in a wind farm, the construction labor cost, the management labor cost and the technician labor cost. The accuracy of the model is favorably explored through comparison with pertinent real world data. It is found that the cost of a wind farm is appreciably sensitive to the rotor diameter and the number of wind turbines for a given desirable total power output."
544,"Automated Optical Inspection (AOI) systems are rapidly replacing slow and tedious manual inspections of Printed Circuit Boards (PCBs). In an AOI system, a minicamera traverses the PCB in a pre-defined travel path, snapping shots of all the PCB components or nodes, at pre-defined locations. The images are then processed and information about the different nodes is extracted and compared against ideal standards stored in the AOI system. This way, a flawed board is detected. Minimizing both the number of images required to scan all the PCB nodes, and the path through which the camera must travel to achieve this, will minimize the image acquisition time and the traveling time, and thus the overall time of inspection. This consequently both reduces costs and increases production rate. This work breaks down this problem into two sub-problems: The first is a clustering problem; the second a travelling salesman sequencing problem. In the clustering problem, it is required to divide all the nodes of a PCB into the minimum number of clusters. The cluster size is constrained by the given dimensions of the camera’s scope or Field of Vision (FOV). These dimensions determine the dimension of the inspection windows. It is thus required to find the minimum number of inspection windows that will scan all the nodes of a PCB, and their locations. Genetic algorithms are applied in a two-step approach with special operators suited for the problem. A continuous Genetic Algorithm (GA) is applied to find the optimum inspection window locations that cover one node and as many other nodes as possible. A discrete GA is then applied to eliminate redundant inspection windows leaving the minimum number of windows that cover all nodes throughout the PCB. In the second sub-problem, an Ant Colony Optimization (ACO) method is used to find the optimum path between the selected inspection windows. The method proposed in this paper is compared against relevant published work, and it is shown to yield better results."
545,"In decomposition-based design optimization strategies, such as Analytical Target Cascading (ATC), it is sometimes necessary to use reduced dimensionality representations to approximate functions of large dimensionality whose values need to be exchanged among subproblems. The reduced representation variables may not be physically meaningful, and it can become challenging to constrain them properly and define the model validity region. For example, in coordination strategies like ATC, representing vector-valued coupling variables with improperly constrained reduced representation variables can lead to poor performance or convergence failure. This paper examines two approaches for constraining effectively the model validity region of reduced representation variables based on proper orthogonal decomposition: a penalty value-based heuristic and a support vector domain description. An ATC application on electric vehicle design helps to illustrate the concepts discussed."
546,"An often cited motivation for using decomposition-based optimization methods to solve engineering system design problems is the ability to apply discipline-specific optimization techniques. For example, structural optimization methods have been employed within a more general system design optimization framework. We propose an extension of this principle to a new domain: control design. The simultaneous design of a physical system and its controller is addressed here using a decomposition-based approach. An optimization subproblem is defined for both the physical system (i.e., plant) design and the control system design. The plant subproblem is solved using a general optimization algorithm, while the controls subproblem is solved using a new approach based on optimal control theory. The optimal control solution, which is derived using the the Minimum Principle of Pontryagin (PMP), accounts for coupling between plant and controller design by managing additional variables and penalty terms required for system coordination. Augmented Lagrangian Coordination is used to solve the system design problem, and is demonstrated using a circuit design problem."
547,"This paper presents a new method (the Unrestricted Wind Farm Layout Optimization (UWFLO)) of arranging turbines in a wind farm to achieve maximum farm efficiency. The powers generated by individual turbines in a wind farm are dependent on each other, due to velocity deficits created by the wake effect. A standard analytical wake model has been used to account for the mutual influences of the turbines in a wind farm. A variable induction factor, dependent on the approaching wind velocity, estimates the velocity deficit across each turbine. Optimization is performed using a constrained Particle Swarm Optimization (PSO) algorithm. The model is validated against experimental data from a wind tunnel experiment on a scaled down wind farm. Reasonable agreement between the model and experimental results is obtained. A preliminary wind farm cost analysis is also performed to explore the effect of using turbines with different rotor diameters on the total power generation. The use of differing rotor diameters is observed to play an important role in improving the overall efficiency of a wind farm."
548,"Various decomposition and coordination methodologies for solving large-scale system design problems have been developed and studied during the past few decades. However, there is generally no guarantee that they will converge to the expected optimum design under general assumptions. Those with proven convergence often have restricted hypotheses or a prohibitive cost related to the required computational effort. Therefore there is still a need for improved, mathematically grounded, decomposition and coordination techniques that will achieve convergence while remaining robust, flexible and easy to implement. In recent years, classical Lagrangian and augmented Lagrangian methods have received renewed interest when applied to decomposed design problems. Some methods are implemented using a subgradient optimization algorithm whose performance is highly dependent on the type of dual update of the iterative process. This paper reports on the implementation of a cutting plane approach in conjunction with Lagrangian coordination and the comparison of its performance with other subgradient update methods. The method is demonstrated on design problems that are decomposable according to the analytic target cascading (ATC) scheme."
549,"Optimization under uncertainty can be a difficult and computationally expensive problem driven by the need to consider the degrading effects of system variations. Sources of uncertainty that may be reducible in some fashion present a particular challenge because designers must determine how much uncertainty to accept in the final design. Many of the existing approaches for design under input uncertainty require potentially unavailable or unknown information about the uncertainty in a system’s input parameters; such as probability distributions, nominal values or uncertain intervals. These requirements may force designers into arbitrary or even erroneous assumptions about a system’s input uncertainty when attempting to estimate nominal values and/or uncertain intervals for example. These types of assumptions can be especially degrading during the early stages in a design process when limited system information is available. In an effort to address these challenges a new design approach is presented that can produce optimal solutions in the form of upper and lower bounds (which specify uncertain intervals) for all input parameters to a system that possess reducible uncertainty. These solutions provide minimal variation in system objectives for a maximum allowed level of input uncertainty in a multi-objective sense and furthermore guarantee as close to deterministic Pareto optimal performance as possible with respect to the uncertain parameters. The function calls required by this approach are dramatically reduced through the use of a kriging meta-model assisted multi-objective optimization technique performed in two stages. The capabilities of the approach are demonstrated through three example problems of varying complexity."
550,"It has become a common practice to conduct simulation-based design of industrial robotic cells, where Mechatronic system model of an industrial robot is used to accurately predict robot performance characteristics like cycle time, critical component lifetime, and energy efficiency. However, current robot programming systems do not usually provide functionality for finding the optimal design of robotic cells. Robot cell designers therefore still face significant challenge to manually search in design space for achieving optimal robot cell design in consideration of productivity measured by the cycle time, lifetime, and energy efficiency. In addition, robot cell designers experience even more challenge to consider the trade-offs between cycle time and lifetime as well as cycle time and energy efficiency. In this work, utilization of multi-objective optimization to optimal design of the work cell of an industrial robot is investigated. Solution space and Pareto front are obtained and used to demonstrate the trade-offs between cycle-time and critical component lifetime as well as cycle-time and energy efficiency of an industrial robot. Two types of multi-objective optimization have been investigated and benchmarked using optimal design problem of robotic work cells: 1) single-objective optimization constructed using Weighted Compromise Programming (WCP) of multiple objectives and 2) Pareto front optimization using multi-objective generic algorithm (MOGA-II). Of the industrial robotics significance, a combined design optimization problem is investigated, where design space consisting of design variables defining robot task placement and robot drive-train are simultaneously searched. Optimization efficiency and interesting trade-offs have been explored and successful results demonstrated."
551,"Although Genetic Algorithms (GAs) and Multi-Objective Genetic Algorithms (MOGAs) have been widely used in engineering design optimization, the important challenge still faced by researchers in using these methods is their high computational cost due to the population-based nature of these methods. For these problems it is important to devise MOGAs that can significantly reduce the number of simulation calls compared to a conventional MOGA. We present an improved kriging assisted MOGA, called Circled Kriging MOGA (CK-MOGA), in which kriging metamodels are embedded within the computation procedure of a traditional MOGA. In the proposed approach, the decision as to whether the original simulation or its kriging metamodel should be used for evaluating an individual is based on a new objective switch criterion and an adaptive metamodeling technique. The effect of the possible estimated error from the metamodel is mitigated by applying the new switch criterion. Three numerical and engineering examples with different degrees of difficulty are used to illustrate applicability of the proposed approach. The results show that, on the average, CK-MOGA outperforms both a conventional MOGA and our developed Kriging MOGA in terms of the number of simulation calls."
552,"Significant research has focused on multiobjective design optimization and negotiating trade-offs between conflicting objectives. Many times, this research has referred to the possibility of attaining similar performance from multiple, unique design combinations. While such occurrences may allow for greater design freedom, their significance has yet to be quantified for trade-off decisions made in the design space (DS). In this paper, we computationally explore which regions of the performance space (PS) exhibit “one-to-many” mappings back to the DS, and examine the behavior and validity of the corresponding region associated with this mapping. Regions of interest in the PS and DS are identified and generated using indifference thresholds to effectively “discretize” both spaces. The properties analyzed in this work are a mapped region’s location in the PS and DS and the total hypervolume of the mappings. Our proposed approach is demonstrated on two different multiobjective engineering problems. The results indicate that one-to-many mappings occur in engineering design problems, and that while these mappings can result in significant design space freedom, they often result in notable performance sacrifice."
553,"The present work attempts to improve the performance of rolling element bearings through the increase of fatigue life and the reduction of bearing wear. The formulation is based on Elastohydrodynamic to maximize the realistically evaluated minimum film thickness without significant increase in viscous friction torque. Design vectors are reduced in the present study relative to previous studies as some variables are considered as dependent variables. Other design vectors are not considered as variables in the study due to machining accuracy. This paper presents a more viable method to solve the multi-objective optimization problem using genetic algorithms (GAs) to reduce the chances of getting trapped in local maximum or minimum. Using a utility function, optimal Pareto points are obtained by changing the weight coefficients. Specific weights can be used depending on the designer decision whether to increase fatigue and wear life, or decrease the power consumption."
554,"As global markets saturate and competition intensifies, many manufacturers are focusing on benchmarking families of products alongside individual products to gain valuable insight and strategic advantage over their competitors. Unfortunately, the advantages of benchmarking families of products are often undermined by the limited capability of current benchmarking tools to assist in this process. While methods have been proposed for product family analysis and benchmarking, a major problem is the way in which the component details are collected, and few of the methods have been integrated together. Benchmarking and product family analysis is also time-consuming and subject to human variability since the process is typically done manually without the aid of software. To address these problems, we introduce the Product Family Analysis Toolkit (PFA Toolkit), which combines several popular benchmarking tools to streamline and standardize the process of product family benchmarking. We describe the toolkit’s features and capabilities and then discuss its functionality and usability. The advantages of automating the product family benchmarking process are discussed along with future work."
555,"Product family design (PFD) is one of the commonly adopted strategies of product realization in mass customization paradigm. Among the current product family modeling approaches, ontology based modeling has been identified as a promising approach. Previously, we have studied the feasibility of using a semantically annotated multi-facet product family ontology in performing product analysis and variant derivation in the PFD domain. However, the visualization aspects of the ontology are important to assist product designers and engineers to gain insights and benefit from the ever-increasing information from the ontology, e.g. dimension, assembly or configuration wise. From the previous literature, we observe that there are limited usage of visualization and interaction in PFD for tasks such as product analysis and variant derivation. The current hierarchy based representations are limited in displaying ontological relationships and tasks such as commonality analysis seldom make use of visualization to foster better understanding of component similarity. In this study, we report our efforts in assisting product family analysis and variant derivation through visualization and user interface (UI) which enables interactive PFD. Design considerations for our visualization and user interaction design are discussed. By using a multi-touch UI, we discuss on how our UI is able to enable users to better perform product analysis and variant derivation based on the aforementioned ontology in an interactive, intuitive and intelligent manner. We finally conclude this paper with some indications for future works."
556,"Effective product platforms must strike an optimal balance between commonality and variety. Increasing commonality can reduce costs by improving economies of scale while increasing variety can improve market performance, or in our robot family example, satisfy various robot missions. Two metrics that have been developed to help resolve this tradeoff are the Generational Variety Index (GVI) and the Product Family Penalty Function (PFPF). GVI provides a metric to measure the amount of product redesign that is required for subsequent product offerings, whereas PFPF measures the dissimilarity or lack of commonality between design (input) parameters during product family optimization. GVI is examined because it is the most widely used metric applicable during conceptual development to determine platform components. PFPF is used to validate GVI because of its ease of implement for parametric variety, as used in this case. This paper describes a product family trade study that has been performed using GVI for a robot product family and compares the results to those obtained by optimizing the same family using PFPF. This work provides a first attempt to validate the output of GVI by using a complementary set of results obtained from optimization. The results of this study indicate that while there are sometimes similarities between the results of GVI and optimization using PFPF, there is not necessarily a direct correlation between these two metrics. Moreover, the platform recommended by GVI is not necessarily the most performance-optimized platform, but it can help improve commonality. In the same regard, PFPF may miss certain opportunities for commonality. The benefits of integrating the two approaches are also discussed."
557,"Strategic adaptability is essential in capitalizing on future investment opportunities and responding properly to market trends in an uncertain environment. Customized products or services are an important source of revenue for many companies, particularly those working with in a mass customization environment where customer satisfaction is of paramount important. In this paper, we extend methods from mass customization and product family design to create specific methods for universal product family design. The objective of this research is to propose a valuation financial model to facilitate universal design strategies that will maximize the expected profit under uncertain constrains. Real options analysis is applied to estimate the valuation of options related to introducing new modules as a platform in a universal product family. We use customers’ preferences based on performance utilities for universal design to reflect demand and demographic trends. To demonstrate implementation of the proposed model, we use a case study involving a family of light-duty trucks. We perform sensitivity analysis to investigate the behavior of the estimated option value against chaining system parameters."
558,"An essential part of designing a successful product family is establishing a recognizable, familiar, product family identity. It is very often the case that consumers first identify products based on their physical embodiment. The Apple iPod, DeWalt power tools, and KitchenAid appliances are all examples of product families that have successfully branded themselves based on physical principles. While physical branding is often the first trait apparent to designers, there are some products that cannot be differentiated based on physical appearance. This is especially common for consumable products. For example, it is impossible to differentiate between diet Coke, Classic Coke, and Pepsi when each is poured into separate glasses. When differentiation is difficult to achieve from a product’s physical characteristics, the product’s package becomes a vital part of establishing branding and communicating membership to a product family while maintaining individual product identity. In this paper, product packaging is investigated with a focus on the graphic packaging components that identify product families. These components include: color, shape, typography, and imagery. Through the application of tools used in facilities layout planning, graph theory, social network theory, and display design theory an approach to determine an optimal arrangement of graphic components is achieved. This approach is validated using a web based survey that tracks user-package interactions across a range of commonly used cereal boxes."
559,"A simulation-based, system reliability-based design optimization (RBDO) method is presented that can handle problems with multiple failure regions and correlated random variables. Copulas are used to represent dependence between random variables. The method uses a Probabilistic Re-Analysis (PRRA) approach in conjunction with a sequential trust-region optimization approach and local metamodels covering each trust region. PRRA calculates very efficiently the system reliability of a design by performing a single Monte Carlo (MC) simulation per trust region. Although PRRA is based on MC simulation, it calculates “smooth” sensitivity derivatives, allowing the use of a gradient-based optimizer. The PRRA method is based on importance sampling. One requirement for providing accurate results is that the support of the sampling PDF must contain the support of the joint PDF of the input random variables. The trust-region optimization approach satisfies this requirement. Local metamodels are constructed sequentially for each trust region taking advantage of the potential overlap of the trust regions. The metamodels are used to determine the value of the indicator function in MC simulation. An example with correlated input random variables demonstrates the accuracy and efficiency of the proposed RBDO method."
560,"Reliability is an important engineering requirement for consistently delivering acceptable product performance through time. As time progresses, a product may fail due to time-dependent operating conditions and material properties, and component degradation. The reliability degradation with time may significantly increase the lifecycle cost due to potential warranty costs, repairs and loss of market share. In this work, we consider the first-passage reliability, which accounts for the first time failure of non-repairable systems. Methods are available that provide an upper bound to the true reliability, but they may overestimate the true value considerably. This paper proposes a methodology to calculate the cumulative probability of failure (probability of first passage or upcrossing) of a dynamic system with random properties, driven by an ergodic input random process. Time series modeling is used to characterize the input random process based on data from a “short” time period (e.g. seconds) from only one sample function of the random process. Sample functions of the output random process are calculated for the same “short” time because it is usually impractical to perform the calculation for a “long” duration (e.g. hours). The proposed methodology calculates the time-dependent reliability, at a “long” time using an accurate “extrapolation” procedure of the failure rate. A representative example of a quarter car model subjected to a stochastic road excitation demonstrates the improved accuracy of the proposed method compared with available methods."
561,"In this work we develop a method to perform simultaneous design and tolerance allocation for engineering problems with multiple objectives. Most studies in existing literature focus on either optimal design with constant tolerances or the optimal tolerance allocation for a given design setup. Simultaneously performing both design and tolerance allocation with multiple objectives for hierarchical systems increases problem dimensions and raises additional computational challenges. A design framework is proposed to obtain optimal design alternatives and to rank their performances when variations are present. An optimality influence range is developed to aid design alternatives selections with an influence signal-to-noise ratio that indicates the accordance of objective variations to the Pareto set and an influence area that quantifies the variations of a design. An additional tolerance design scheme is implemented to ensure that design alternatives meet the target tolerance regions. The proposed method is also extended to decomposed multi-level systems by integrating traditional sensitivity analysis for uncertainty propagation with analytical target cascading. This work enables decision-makers to select their best design alternatives on the Pareto set using three measures with different purposes. Examples demonstrate the effectiveness of the method on both single- and multi-level systems."
562,"In this work, the presence of equality constraints in reliability-based design optimization (RBDO) problems is studied. Relaxation of soft equality constraints in RBDO and its challenges are briefly discussed while the main focus is on hard equalities that can not be violated even under uncertainty. Direct elimination of hard equalities to reduce problem dimensions is usually suggested; however, for nonlinear or black-box functions, variable elimination requires expensive root-finding processes or inverse functions that are generally unavailable. We extend the reduced gradient methods in deterministic optimization to handle hard equalities in RBDO. The efficiency and accuracy of the first and the second order predictions in reduced gradient methods are compared. Results show the first order prediction being more efficient when realizations of random variables are available. A gradient-weighted sorting with these random samples is proposed to further improve the solution efficiency of the reduced gradient method. Feasible design realizations subject to hard equality constraints are then available to be implemented with the state-of-the-art sampling techniques for RBDO problems. Numerical and engineering examples show the strength and simplicity of the proposed method."
563,"In this paper, a probabilistic design optimization method based on finite element method is proposed to calculate the variability of design parameters subject to a specified dispersion of natural frequencies of rotating blades. The element stiffness and mass matrices are derived using a two-stage finite element method and numerical integration. Based on the perturbation technology, the sensitivity of the frequencies, as well as relationship between the frequency dispersion and the coefficient of variability (CV) of the design parameters can be obtained. Such sensitivity information is then used to convert the probabilistic design optimization problem into a deterministic optimization problem. Two case studies are given to illustrate the proposed method. From the results, it is concluded that rotation of blade changes the sensitivity of CV to the design parameters considered, and using the proposed method can transform the probabilistic constraints to deterministic constraints."
564,"The widely used First Order Reliability Method (FORM) is efficient, but may not be accurate for nonlinear limit-state functions. The Second Order Reliability Method (SORM) is more accurate but less efficient. To maintain both high accuracy and efficiency, we propose a new second order reliability analysis method with first order efficiency. The method first performs the FORM and identifies the Most Probable Point (MPP). Then the associated limit-state function is decomposed into additive univariate functions at the MPP. Each univariate function is further approximated as a quadratic function, which is created with the gradient information at the MPP and one more point near the MPP. The cumulant generating function of the approximated limit-state function is then available so that saddlepoint approximation can be easily applied for computing the probability of failure. The accuracy of the new method is comparable to that of the SORM, and its efficiency is in the same order of magnitude as the FORM."
565,"In this paper, we present a new approach to solve optimization problems with multiple objectives under uncertainty. Optimality is considered in terms of the risk that the overall system performance, as defined by all of the multiple objectives exceeding their desired thresholds, remains acceptable. Unlike the existing state-of-the-art, where first-order moments of the system level objectives are used to ensure optimality, we employ a joint probability formulation in our research. The Pareto optimality criterion under uncertainty is defined in terms of joint probability, i.e., probability that all  system objectives are less than the desired thresholds. These thresholds can be viewed as the desired upper/lower bounds on the individual system objectives. The higher the joint probability, the more reliably the thresholds bound the system performance, hence the lower the overall system performance risk. However, a desirable high joint probability may necessitate undesirably high/low thresholds, and hence the tradeoff. In this context, the proposed method provides two decision-making capabilities: (1) Maximum probability design:  given a set of threshold values for system objectives, find the design that yields the maximum joint probability (2) Optimum threshold design:  Given a desired joint probability, find the set of thresholds that yield this probability. In this paper, optimization formulations are presented to solve the above two decision-making problems. A two-bar truss example and the conceptual design of a two-stage-to-orbit launch vehicle are presented to illustrate the proposed methods. The numerical results show that optimizing the mean values of the objectives individually does not necessarily guarantee the desired performance of all objectives jointly under uncertainty, which is of ultimate interest in multiobjective optimization."
566,"Sensitivity analysis and computer model calibration are generally treated as two separate topics. In sensitivity analysis one quantifies the effect of each input factor on outputs, whereas in calibration one finds the values of input factors that provide the best match to a set of field data. In this paper we show a connection between these two seemingly separate concepts, and illustrate it with an automotive industry application involving a Road Load Acquisition Data (RLDA) computer model. We use global sensitivity analysis for computer models with transient responses to screen out inactive input parameters and make the calibration algorithm numerically more stable. Because the computer model can be computationally intensive, we construct a fast statistical surrogate for the computer model with transient responses. This fast surrogate is used for both sensitivity analysis and RLDA computer model calibration."
567,"The "
568,"Computational simulation models support a rapid design process. Given model approximation and operating conditions uncertainty, designers must have confidence that the designs obtained using simulations will perform as expected. This paper presents a methodology for validating designs as they are generated during a simulation-based optimization process. Current practice focuses on validation of simulation models throughout the entire design space. In contrast, the proposed methodology requires validation only at design points generated during optimization. The goal of such validation is confidence in the resulting design rather than in the underlying simulation model. The proposed methodology is illustrated on a simple cantilever beam design subject to vibration."
569,"Risk management is an important element of product design. It helps to minimize the project- and product-related risks such as project budget and schedule overrun, or missing product cost and quality targets. Risk management is especially important for complex, international product design projects that involve a high degree of novel technology. This paper reviews the literature on risk management in product design. It examines the newly released international standard ISO 31000 “Risk management — Principles and guidelines” and explores its applicability to product design. The new standard consists of the seven process steps communication and consultation; establishing the context; risk identification; risk analysis; risk evaluation; risk treatment; and monitoring and review. A literature review reveals, among other findings, that the general ISO 31000 process model seems applicable to risk management in product design; the literature addresses different process elements to varying degrees, but none fully according to ISO recommendations; and that the integration of product design risk management with risk management of other disciplines, or between project and portfolio level in product design, is not well developed."
570,"Products are successful because they meet customer needs. However, many customer needs are not expressed in measurable terms. In addition, when such needs are achieved by a complex system made of hardware parts and software, decomposing customer needs to part-level specification is not a trivial task. This paper presents a model-based approach to address such problems. In the case study, the customer need was the noise and vibration level of an unconventional gasoline engine system when running at idle. The hardware component whose performance tolerance needed to be specified was a new type of fuel injectors. These new fuel injectors had higher piece-to-piece performance variations than the conventional fuel injectors. It was unclear whether such variation was acceptable for customer perceived powertrain quality. A virtual powertrain system simulation model was used to analytically evaluate the impact of the fuel injector performance variability. Monte Carlo simulation was carried out to assess the impact of injector variability. The results from the simulation were further refined using engine hardware testing. This study made recommendations for the acceptable level of hardware tolerance, which was different from what the supplier of the injectors had suggested."
571,"This study presents a methodology for computing stochastic sensitivities with respect to the design variables, which are the mean values of the input correlated random variables. Assuming that an accurate surrogate model is available, the proposed method calculates the component reliability, system reliability, or statistical moments and their sensitivities by applying Monte Carlo simulation (MCS) to the accurate surrogate model. Since the surrogate model is used, the computational cost for the stochastic sensitivity analysis is negligible. The copula is used to model the joint distribution of the correlated input random variables, and the score function is used to derive the stochastic sensitivities of reliability or statistical moments for the correlated random variables. An important merit of the proposed method is that it does not require the gradients of performance functions, which are known to be erroneous when obtained from the surrogate model, or the transformation from X-space to U-space for reliability analysis. Since no transformation is required and the reliability or statistical moment is calculated in X-space, there is no approximation or restriction in calculating the sensitivities of the reliability or statistical moment. Numerical results indicate that the proposed method can estimate the sensitivities of the reliability or statistical moments very accurately, even when the input random variables are correlated."
572,"For reliability-based design optimization (RBDO), generating an input statistical model with confidence level has been recently proposed to offset the inaccurate estimation of the input statistical model with Gaussian distributions. For this, the confidence intervals of mean and standard deviation are calculated using the Gaussian distributions of input random variables. However, if the input random variables are non-Gaussian, the use of the Gaussian distributions of input variables will provide inaccurate confidence intervals, and thus, yield undesirable confidence level of the reliability-based optimum design meeting the target reliability "
573,"Variability is inherent randomness in systems, whereas uncertainty is due to lack of knowledge. In this paper, a generalized multiscale Markov (GMM) model is proposed to quantify variability and uncertainty simultaneously in multiscale system analysis. The GMM model is based on a new imprecise probability theory that has the form of generalized interval, which is a Kaucher or modal extension of classical set-based intervals to represent uncertainties. The properties of the new definitions of independence and Bayesian inference are studied. Based on a new Bayes’ rule with generalized intervals, three cross-scale validation approaches that incorporate variability and uncertainty propagation are also developed."
574,"Model updating, which utilizes mathematical means to combine model simulations with physical observations for improving model predictions, has been viewed as an integral part of a model validation process. While calibration is often used to “tune” uncertain model parameters, bias-correction has been used to capture model inadequacy due to a lack of knowledge of the physics of a problem. While both sources of uncertainty co-exist, these two techniques are often implemented separately in model updating. This paper examines existing approaches to model updating and presents a modular Bayesian approach as a comprehensive framework that accounts for many sources of uncertainty in a typical model updating process and provides stochastic predictions for the purpose of design. In addition to the uncertainty in the computer model parameters and the computer model itself, this framework accounts for the experimental uncertainty and the uncertainty due to the lack of data in both computer simulations and physical experiments using the Gaussian process model. Several challenges are apparent in the implementation of the modular Bayesian approach. We argue that distinguishing between uncertain model parameters (calibration) and systematic inadequacies (bias correction) is often quite challenging due to an identifiability issue. We present several explanations and examples of this issue and bring up the needs of future research in distinguishing between the two sources of uncertainty."
575,"Reliability-based Design Optimization problems have been solved by two well-known methods: Reliability Index Approach (RIA) and Performance Measure Approach (PMA). RIA generates first-order approximate probabilistic constraints using the measures of reliability indices. For infeasible design points, the traditional RIA method suffers from inaccurate evaluation of the reliability index. To overcome this problem, the Modified Reliability Index Approach (MRIA) has been proposed. The MRIA provides the accurate solution of the reliability index but also inherits some inefficiency characteristics from the Most Probable Failure Point (MPFP) search when nonlinear constraints are involved. In this paper, the benchmark examples have been utilized to examine the efficiency and stability of both PMA and MRIA. In our study, we found that the MRIA is capable of obtaining the correct optimal solutions regardless of the locations of design points but the PMA is much efficient in the inverse reliability analysis. To take advantages of the strengths of both methods, a Hybrid Reliability Approach (HRA) is proposed. The HRA uses a selection factor that can determine which method to use during optimization iterations. Numerical examples from the proposed method are presented and compared with the MRIA and the PMA."
576,"This paper presents a computational framework that mathematically propagates material microstructure uncertainties to coarser system resolutions for use in multiscale design frameworks. The computational framework uses a homogenized stochastic constitutive relation that links microstructure uncertainty with stochastic material properties. The stochastic constitutive relation formulated in this work serves as the critical link between the material and product domains in integrated material and product design. Ubiquitous fine resolution uncertainty sources influencing prediction of material properties based on their structures are categorized, and stochastic cell averaging is achieved by two advanced uncertainty quantification methods: random process polynomial chaos expansion and statistical copula functions. Both methods confront the mathematical difficulty in randomizing constitutive law parameters by capturing the marked correlation among them often seen in complex materials, thus the results proffer a more accurate probabilistic estimation of constitutive material behavior. The method put forth in this research, though quite general, is applied to a plastic, high strength steel alloy for demonstration."
577,"Advancements in manufacturing technology significantly impact the design process. The ability to manufacture assembly components with specific tolerances has increased the need for tolerance allocation. This research proposes a framework that overcomes the drawbacks of the traditional tolerance control methods, and reduces subjectivity by using fuzzy set theory and decision support processes. The combination of fuzzy comprehensive evaluation and conjoint analysis facilitate the reduction of subjectivity in the tolerance control process. The application of the framework is demonstrated with two practical engineering problems. Tolerances are allocated for a clutch assembly and an O-ring seal in an accumulator."
578,"The Proper Orthogonal Decomposition (POD) method has been employed to extract the important signatures of the random field presented in an engineering product or process. Our preliminary study found that coefficients of the signatures are statistically uncorrelated but may be dependent. In general, the statistical dependence of the coefficients is ignored in the random field characterization for probability analysis and design. This paper thus proposes an effective approach to characterize the random field for probability analysis and design while accounting for the statistical dependence among the coefficients. The proposed approach is composed of two technical contributions. The first contribution is to develop a generic approximation scheme of random field as a function of the most important field signatures while preserving prescribed approximation accuracy. The coefficients of the signatures can be modeled as random field variables and their statistical properties are identified using the Chi-Square goodness-of-fit test. Second, the Rosenblatt transformation is employed to transform the statistically dependent random field variables into statistically independent random field variables. There exist so many transformation sequences when the number of random field variables becomes large. It was found that an improper selection of a transformation sequence may introduce high nonlinearity into system responses, which causes inaccuracy in probability analysis and design. Hence, a novel procedure is proposed for determining an optimal transformation sequence that introduces the least degree of nonlinearity to the system response after the Rosenblatt transformation. The proposed random field characterization can be integrated with one of the advanced probability analysis methods, such as the Eigenvector Dimension Reduction (EDR) method, Polynomial Chaos Expansion (PCE) method, etc. Three structural examples including a Micro-Electro-Mechanical Systems (MEMS) bistable mechanism are used to demonstrate the effectiveness of the proposed approach. The results show that the statistical dependence in random field characterization cannot be neglected for probability analysis and design. Moreover, it is shown that the proposed random field approach is very accurate and efficient."
579,"A first order structural optimization problem is examined to evaluate the effects of structural geometry on blast energy transfer in a fully coupled fluid structure interaction problem. The fidelity of the fluid structure interaction simulation is shown to yield significant insights into the blast mitigation problem not captured in similar empirically based blast models. An emphasis is placed on the accuracy of simulating such fluid structure interactions and its implications on designing continuum level structures. Higher order design methodologies and algorithms are discussed for the application of such fully coupled simulations on vehicle level optimization problems."
580,"The use of vibration-based techniques in damage identification has recently received considerable attention in many engineering disciplines. While various damage indicators have been proposed in the literature, those relying only on changes in the natural frequencies are quite appealing since these quantities can conveniently be acquired. The identification of damage involves an optimization step where response of a continuously updated finite elements model (FEM) is compared with the response of the experimental measurements and error between both responses is minimized. In this paper it is shown that such error function is highly multi-modal and that the same response can be obtained by more than one damage scenario. In order to find these optima a hybrid optimization approach is developed which utilizes two components; namely. Modified Continuous Reactive Tabu Search (MCRTS) and Real Coded Genetic Algorithms. MCRTS, the primary component, is a meta-heuristic capable of finding several optima in a multi-modal search space, which suites the nature of the problem at hand. GAs, the secondary component, although a global optimizer, is used as a local optimizer that is fired in promising regions of the search space as identified by the major component (MCRTS). It is used in favor of direct search methods to account for the presence of minor local optima. In order to test the algorithm, several beams are manufactured and crack damages are induced using wire-cutting, and the natural frequencies are tested experimentally. Such beams have two locations that can give the same response. The developed algorithm managed to find the two sought-for optima consistently in several runs. This proves the merit of this approach as being capable of handling the problem at hand."
582,"In this paper an efficient approach for generating tradeoff curves when performing topology optimization with manufacturing constraints is presented. By minimizing a new stiffness-volume ratio, or in-fact a new compliance-volume product, the tradeoff curve is generated by changing a new design parameter. The volume appearing in the objective is raised to the power of this new design parameter. In such manner different conceptual designs can be generated. By adopting a nested approach, the problem is easily solved by a simple numerical scheme. This is a nice feature of the approach which makes the numerical performance most efficient and robust. This feature makes it also easy to include manufacturing constraints by simply updating the move limits such that these constraints are satisfied. The design parametrization is done by the SIMP-model and patterns of checker-boards are prevented by adopting Sigmund’s filter. The efficiency of the approach is demonstrated by presenting tradeoff curves for both 2D- and 3D-problems."
583,"In a novel procedure for percutaneous mitral valve repair, inter-related hook-shaped anchors are inserted around the annulus to replace the surgeon’s suturing in open-heart ring annuloplasty. To properly attach to the tissue, the anchors should withstand large deformation applied during the delivery process and recover their original shape when released into the heart tissue. To this end, stress concentration is avoided along the anchors, which are fabricated of a super-elastic material, by means of shape optimization. Shape optimization consists in finding the smoothest anchor mid-curve possible, which minimizes the von Mises stresses applied during the delivery process. An optimization algorithm aimed at minimizing the weighted rms value of the curvature is introduced. A geometrically optimum shape is obtained by equally weighting the curvature values. Further reduction in the stress values is possible by weighting the curvature values along the anchor in an iterative procedure that yields a structurally optimum anchor. The weights at each iteration are defined proportional to the stress distribution along the anchor obtained in the previous iteration."
584,"In "
585,"Converting ambient vibration energy into electrical energy using piezoelectric energy harvester has attracted much interest in the past decades. In this paper, topology optimization is applied to design the optimal layout of the piezoelectric energy harvesting devices. The objective function is defined as to maximize the energy harvesting performance over a range of ambient vibration frequencies. Pseudo excitation method (PEM) is applied to analyze structural stationary random responses. Sensitivity analysis is derived by the adjoint method. Numerical examples are presented to demonstrate the validity of the proposed approach."
586,"This paper presents the use of a genetic algorithm in conjunction with geometric nonlinear finite element analysis to optimize the fastener pattern and lug location in an eccentrically loaded multi-fastener connection. No frictional resistance to shear was included in the model, as the connection transmitted shear loads into four dowel fasteners through bearing-type contact without fastener preload. With the goal of reducing the maximum von Mises stress in the connection to improve fatigue life, the location of the lug hole and four fastener holes were optimized to achieve 55% less maximum stress than a similar optimization using the traditional instantaneous center of rotation method. Since the maximum stress concentration was located at the edge of a fastener hole where fatigue cracks could be a concern, reduction of this quantity lowers the probability of crack growth for both bearing-type and slip-resistant connections. It was also found that the location of the maximum von Mises stress concentration jumped from the fastener region to the lug as the applied force angle was decreased below 45 degrees, thus the fastener pattern could not be optimized for lower angles."
588,"Existing mathematical models of the mitral valve allow the simulation of ring open-heart annuloplasty procedures intended to reduce the lumen of the valve. Using these models, only a posteriori effects can be predicted. With the advent of novel percutaneous annuloplasty approaches, there is a need to describe a priori effects; in particular, this paper focuses on a technique which consists of sequentially installing interconnected anchors around the mitral annulus, whose lumen is reduced by the tightening of the tethered wire. We develop here a static mathematical model of the mitral annulus that takes into account the mechanical response of its tissue and the surrounding muscular tissue. A number of roughly coplanar points corresponding to anchor positions, at about equal distantes, are identified on the annulus. Each of these points is then attached to a linearly elastic spring of a given stiffness, The spring-end is connected to a fixed pinned support, the other end supporting the wire, that forms a loop. With this model we estimate the anchor-points position vectors after lumen reduction and the wire tension that is needed to reduce the perimeter of the polygon defined by the anchor points to a given value, which, for each patient, is related to the desired lumen. This formulation leads to the minimization of the potential energy of the mechanical system over the position vectors of the anchor points after tightening, which are the design variables. These are found by solving the first-order normality conditions of the equality-constrained optimization problem. Preliminary experimental data obtained on cadaveric swine hearts validate the model: it can be used to predict, for a given perimeter size reduction, the wire tension as well as the anchor position after repair."
589,"The level set approach has been used as a powerful tool in designing structures with a proper safety margin against stability and buckling issues. In this article a closed form equation for critical buckling load of any arbitrary topology has been proposed and employed in Level Set formulation in order to maximize it. Results show that the Level Set Method is straight forward and easy to implement, with fewer limitations overall in the topology optimization of engineering structures."
590,"This paper explores the design optimization of parabolic-trough solar power generation systems. In these systems, solar radiation is focused onto receiver tubes in which a thermal carrier fluid is circulated. The collected thermal energy is then used to generate steam that powers a steam turbine to drive an electric generator. An optimization model is constructed that aims to minimize the cost of electric energy produced. In this model, the optimization is concerned with decision variables that affect: i) the solar field and ii) thermal storage. The steam turbine and generator are not part of the optimization model, as they are assumed to use the same off-shelf components that are used in fossil-fuel based power plants. It is understood that decisions concerning the solar field both affect and are affected by the design of the solar collector assemblies (SCAs), which are the support structures that hold the focusing mirrors. Design of the SCAs is a structural optimization problem that aims to minimize the cost of the structure while satisfying dimensional and loading constraints. Genetic algorithm (GA) is used for the optimization of the parabolic trough system model. For every candidate design examined by GA for the solar field and thermal storage, the most suitable structural design of the SCAs is obtained from solving the sub-problem of structural optimization. This “nested” optimization model is made possible by pre-analyzing a large range of SCA designs and recording them as a lookup database. The developed optimization model of the parabolic trough systems allows for parametric studies on how certain incentives, government policies and key technological developments may affect the system design decisions."
591,"In the work presented in this paper, we made an attempt to integrate the decisions for interrelated sub-problems of part design or selection, machine loading and machining optimization in a random flexible manufacturing system (FMS). The main purpose was to come up with an optimization model for achieving more generic and consistent decisions for the FMS and which can be practically implemented on the shop floor to help designers and other engineers in several ways, including, for instance, to optimize the designs of parts for specific FMS. In order to attain the generic decisions, an integer nonlinear programming (INLP) problem was formulated and solved to maximize the FMS throughput. Based on the results, the part design or selection, machine loading and machining optimization decisions can be simultaneously made. To get more insights of the results and also to check the validity of the model, a two-factor full factorial design was implemented for the sensitivity analysis, analysis of variance (ANOVA) and residual analysis. The computational analyses show that the tooling budget and available processing time were both statistically significant to throughput and confirmed that the model is valid with the data normally distributed."
592,"A method for automating the design of simple and compound gear trains using graph grammars is described. The resulting computational tool removes the tedium for engineering designers searching through the immense number of possible gear choices and combinations by hand. The variables that are automatically optimized by the computational tool include the gear dimensions as well as the location of the gears in space. The gear trains are optimized using a three-step process. The first step is a tree-search based on a language of gear rules which represent all possible gear train configurations. The second step optimizes the discrete values such as number of teeth through an exhaustive search of a gear catalog. The final step is a gradient-based algorithm which optimizes the non-discrete variables such as angles and lengths in the positioning of the gears. The advantage of this method is that the graph grammar allows all possible simple and compound gear trains to be included in the search space while the method of optimization ensures the optimal candidate for a given problem is chosen with the tuned parameters."
593,"In this paper, a generalization is suggested for the Heuristic Gradient Projection method. The previous Heuristic Gradient Projection method (HGP) has been developed for 3D-frame design and optimization. It mainly employed bending stress relations in order to simplify the process of iterations for stress constrained optimization. The General Heuristic Gradient Projection (GHGP) is used in a more general form to satisfy the stress constraints. Another direct search method is hybridized to satisfy other constraints on deflection. Two examples are solved using the new method. The proposed method is compared with the Hybrid Fuzzy Heuristic technique (FHGP) when solving a MEMS resonator. Results showed that the proposed hybrid technique with (GHGP) converges to the optimum solutions faster by an 8%. The MEMS weight is also decreased by 23.7%. For a macro level, the GHGP improved the solution time by 33.3%. The hybrid technique with (GHGP) improved the stresses in the members of the optimum ten-member cantilever."
594,"This paper presents an automated algorithm for design of vehicle structures for crashworthiness, based on the analyses of the structural "
595,"Dimensional analysis is a powerful tool used commonly to develop functional relations between variables affecting a physical system. Known for its versatility in incorporating several different energy domains, the method has allowed for dimensional manipulation of diverse parameters. However, the dimensional combinations developed have been, for the most part, strictly confined to static and time invariant systems. Expanding on the process we introduce a graphical approach to dimensionally model dynamic systems for design. Continuing this extension, we present a graphical and topological combination to illustrate the applicability of dimensional analysis as a state equation generation tool. This tool is comparable to conventional differential element analysis in system dynamics, but provides a systematic and visual approach to design modeling. Further, this exposition acts as a learning instrument where complex engineering equations are derived and interpreted through visual perception similar to a block diagram or flow chart. A dynamic system, in the form of a compressed air–water rocket, is also evaluated in this paper for elucidation."
596,"Complex systems need to perform in a variety of functional states and under varying operating conditions. Therefore, it is important to manage the different values of design variables associated with the operating states for each subsystem. The research presented in this paper uses multidisciplinary optimization (MDO) and changeable systems methods together in the design of a reconfigurable Unmanned Aerial Vehicle (UAV). MDO is a useful approach for designing a system that is composed of distinct disciplinary subsystems by managing the design variable coupling between the subsystem and system level optimization problems. Changeable design research addresses how changes in the physical configuration of products and systems can better meet distinct needs of different operating states. As a step towards the development of a realistic reconfigurable UAV optimization problem, this paper focuses on the performance advantage of using a changeable airfoil subsystem. Design principles from transformational design methods are used to develop concepts that determine how the design variables are allowed to change in the mathematical optimization problem. The performance of two changeable airfoil concepts is compared to a fixed airfoil design over two different missions that are defined by a sequence of mission segments. Determining the configurations of the static and changeable airfoils is accomplished using a genetic algorithm. Results from this study show that aircraft with changeable airfoils attain increased performance, and that the manner by which the system transforms is significant. For this reason, the changeable airfoil optimization developed in this paper is ready to be integrated into a complete MDO problem for the design of a reconfigurable UAV."
597,"This paper addresses the design optimization of a special class of steel structures, which is clear-span building built up via off-shelf standard steel-sections. The problem is of particular importance in small to medium span buildings due to an attractive opportunity for reduction of the manufacturing cost compared to trusses and custom-built beams. The problem is also difficult from an optimization perspective as it exhibits both continuous and discrete variables, as well as discontinuities and flat regions in the topology of the objective function. Genetic algorithms (GA) and a special stochastic sampling technique are considered for the problem, as well as a mixed GA and stochastic sampling approach. The stochastic sampling is guided via heuristic rules based on knowledge specific to the problem, and is thus perceived well suited to the optimization task. While all the tested algorithms produced satisfactory results, the mixed approach seemed to yield the most consistent performance."
598,"Modeling, simulation, and optimization play vital roles throughout the engineering design process; however, in many design disciplines the cost of simulation is high, and designers are faced with a tradeoff between the number of alternatives that can be evaluated and the accuracy with which they are evaluated. In this paper, a methodology is presented for using models of various levels of fidelity during the optimization process. The intent is to use inexpensive, low-fidelity models with limited accuracy to recognize poor design alternatives and reserve the high-fidelity, accurate, but also expensive models only to characterize the best alternatives. Specifically, by setting a user-defined performance threshold, the optimizer can explore the design space using a low-fidelity model by default, and switch to a higher fidelity model only if the performance threshold is attained. In this manner, the high fidelity model is used only to discern the best solution from the set of good solutions, so computational resources are conserved until the optimizer is close to the solution. This makes the optimization process more efficient without sacrificing the quality of the solution. The method is illustrated by optimizing the trajectory of a hydraulic backhoe. To characterize the robustness and efficiency of the method, a design space exploration is performed using both the low and high fidelity models, and the optimization problem is solved multiple times using the variable fidelity framework."
599,"A polytope-based representation is presented to approximate the feasible space of a design concept that is described mathematically using constraints. A method for constructing such design spaces is also introduced. Constraints include equality and inequality relationships between design variables and performance parameters. The design space is represented as a finite set of (at most) 3-dimensional (possibly non-convex) polytopes, i.e., points, intervals, polygons (both open and closed) and polyhedra (both open and closed). These polytopes approximate the locally connected design space around an initial feasible point. The algorithm for constructing the design space is developed by adapting consistency algorithm for polytope representations."
600,"Modularization of parts — a fairly recent trend in product development — facilitates part definitions in a standardized, machine-readable form, so that we can define a part based on its input(s), output(s), features, and geometric information. Standardizing part definitions will enable manufacturing companies to more easily identify part suppliers in global, virtual environments. This standard representation of parts also facilitates modular product design during parametric design. We will show that this problem of modular product design can be formulated as an AI Planning problem, and we propose a solution framework to support modular product design. Using part specification information for personal computers, we demonstrate the proposed framework and discuss its implications for global manufacturing."
601,"In today’s economy, engineering companies strive to reduce product development time and costs. One approach to assisting this goal is to introduce computer-aided methods and tools earlier in the development process. This requires providing robust design automation methods and tools that can support design synthesis and the generation of alternative design configurations, in addition to automated geometric design. A new method for automated gearbox design, tailored for integration within an existing commercial gearbox analysis tool, is described in this paper. The method combines a rule-based generative approach, based on a previous parallel grammar approach for mechanical gear systems, with domain specific heuristics and stochastic search using simulated annealing. Given design specifications that include a bounding box, the number of required speeds and their target ratios, a range of valid gearbox configurations is generated from a minimal initial configuration. Initial test results show that this new method is able to generate a variety of designs which meet the design specifications. The paper concludes with a discussion of the method’s current limitations and a description of the work currently underway to improve and extend its capabilities."
602,"Control tasks involving dramatic non-linearities, such as decision making, can be challenging for classical design methods. However, autonomous stochastic design methods have proved effective. In particular, Genetic Algorithms (GA) that create phenotypes by the application of genotypes comprising rules are robust and highly scalable. Such encodings are useful for complex applications such as artificial neural net design. This paper outlines an evolutionary algorithm that creates C++  programs which in turn create Artificial Neural Networks (ANNs) that can functionally perform as an exclusive-OR logic gate. Furthermore, the GAs are able to create scalable ANNs robust enough to feature redundancies that allow the network to function despite internal failures."
604,"The early evaluation of a proposed function structure for a product and also, the possibility to expose the potential failures related to this provides that the design process can be modeled in its entirety. However, so far there are no existed suitable models for the early phase of design process. This article presents an integrated approach aimed to explore the behaviors of concept designs in the early design phase. The approach is founded on a combination of Petri net, π-numbers, qualitative physics principles and Design Structure Matrix. The final aim is to implement this method on the SysML modeling language to integrate a simulation approach that is initially not standardized in the language. A second interest of the approach is to provide a coherent simulation framework that can be used as a reference to verify the coherency of other simulation models further in the design process."
605,"The modularity indicates a one-to-one mapping between functional concepts and physical components. It can allow us to generate more product varieties at lower costs. Functional concepts can be described by precise syntactic structures with functional terms. Different semantic measures can be used to evaluate the strength of the semantic link between two functional concepts from port ontology. In this paper, different methods of modularity based on ontology are first investigated. Secondly, the primitive concepts are presented based on port ontology by using natural language, and then their semantic synthesis is used to describe component ontology. The taxonomy of port-based ontology are built to map the component connections and interactions in order to build functional blocks. Next, propose an approach to computing semantic similarity by mapping terms to functional ontology and by examining their relationships based on port ontology language. Furthermore, several modules are partitioned on the basis of similarity measures. The process of module construction is described and its elements are related to the similarity values between concepts. Finally, a case is studied to show the efficiency of port ontology semantic similarity for modular concept generation."
606,"Many high fidelity analysis tools including finite-element analysis and computational fluid dynamics have become an integral part of the design process. However, these tools were developed for detailed design and are inadequate for conceptual design due to complexity and turnaround time. With the development of more complex technologies and systems, decisions made earlier in the design process have become crucial to product success. Therefore, one possible alternative to high fidelity analysis tools for conceptual design is metamodeling. Metamodels generated upon high fidelity analysis datasets from previous design iterations show large potential to represent the overall trends of the dataset. To determine which metamodeling techniques were best suited to handle high fidelity datasets for conceptual design, an implementation scheme for incorporating Polynomial Response Surface (PRS) methods, Kriging Approximations, and Radial Basis Function Neural Networks (RBFNN) was developed. This paper presents the development of a conceptual design metamodeling strategy. Initially high fidelity legacy datasets were generated from FEA simulations. Metamodels were then built upon the legacy datasets. Finally, metamodel performance was evaluated based upon several dataset conditions including various sample sizes, dataset linearity, interpolation within a domain, and extrapolation outside a domain."
607,"The developing new products of high quality, low unit cost, and shortening lead time to markets are the key elements required for enterprises to obtain competitive advantages. In order to improve the creativity and shorten lead time to markets, a methodology of automatic virtual entity simulation of conceptual design results is proposed. At the end of conceptual design, the conceptual design results are expressed in the symbolic schemes generated by the computerized approach with a higher capability to obtain the innovative conceptual design. Then, the symbolic scheme is identified into basic mechanisms and their connections. To the identified basic mechanisms, their kinematic analysis is carried out by matching basic Barranov trusses, and their virtual entities are modeled based on feature-based technique and encapsulated as one design object. Based on the structures of the basic mechanisms and their connections, a space layout to the mechanical system corresponding to the symbolic scheme is fulfilled then. With the pre-assembly approach, all parts in the mechanical system are put onto proper positions where the constraint equations are met. In this way, the virtual entity assembly model of the mechanical system corresponding to the symbolic scheme is set up. Changing the positions of the driving links continually, the virtual entity simulation of the mechanical system will be fulfilled. As a result, with the aid of this approach, we can not only obtain innovative conceptual design results with excellent performances, but also shorten the design time and the cost of product developments."
608,"Sensitivity analyses are frequently used during the design process of engineering systems to qualify and quantify the effect of parametric variation on the performance of a system. Two primary types of sensitivity analyses are generally used: local and global. Local analyses, generally involving derivative-based measures, have a significantly lower computational burden than global analyses but only provide measures of sensitivity around a nominal point. Global analyses, generally performed with a Monte Carlo sampling approach and variation-based measures, provide a complete description of a concept’s sensitivity but incur a large computational burden and require significantly more information regarding the distributions of the design parameters in a concept. Local analyses are generally suited to the early stages of design when parametric information is limited and a large number of concepts must be considered (necessitating a light computational burden). Global analyses are more suited towards the later stages of design when more information regarding parametric distributions is available and fewer concepts are being considered. Current derivative-based local approaches provide a significantly different set of measures than a global variation-based analysis. This makes a direct comparison of local to global measures impossible. To reconcile local and global sensitivity analyses, a hybrid local variation based sensitivity (HyVar) approach is presented. This approach has a similar computational burden to a local approach but produces measures in the same format as a global variation-based approach (contribution percentages). This HyVar sensitivity analysis is developed in the context of a functionality-based design and behavioral modeling framework. An example application of the method is presented along with a summary of results produced from a more comprehensive example."
609,"A key challenge facing designers creating innovative products is concept generation in conceptual design. Conceptual design can be more effective when the design space is broad and accelerated by including problem solving and solution triggering tools in its structure. The design space can be broadened by using an integrated design of product and material concepts approach. In this approach, structured analogy is used to transfer underlying principles from a solution suitable in one domain (i.e., product or mechanical domain) to an analogous solution in another domain (i.e., materials domain). The nature of design analogy does not require as full of an exploration of the target domain as would otherwise be necessary; affording the possibility of a more rapid development. The addition of problem solving and solution triggering tools also decreases the design time and/or improves the quality of the final solution. The fulfillment of this is realized through a combination of the Theory of Inventive Problem Solving (TRIZ) proposed by Altshuller, and the systematic approach of Pahl and Beitz, for products that are jointly considered at the material and product level. These types of problems are ones where a designer seeks to fulfill performance requirements placed on the product generally through both the product and the designed material. In this method, the systematic approach of Pahl and Beitz is used as the base method, and TRIZ is used as a means of transferring abstract information about the design problem between the domains with an aim of accelerating the conceptual design process. This approach also allows for cross design approach tools such as S-Field-Model-CAD integration with design repositories to be used to transfer information at different levels of abstraction; expanding the design space and effectively directing the designer. The explanation of this approach is presented through a very simple example of a spring design improvement."
610,"Transforming products, or more generally transformers, are devices that change state in order to facilitate new, or enhance an existing, functionality. Mechanical transformers relate to products that reconfigure and can be advantageous by providing multiple functions, while often conserving space. A basic example is a foldable chair that can be stowed when not in use, but provides ergonomic and structural seating when deployed. Utilizing transformation can also lead to novel designs that combine functions across domains, such as an amphibious vehicle that provides both terrestrial and aquatic transportation. In order to harness these assets of transformation, the Transformational Design Theory [1] was developed. This theory outlines a set of principles and facilitators that describe and embody transformation for the purpose of systematically assisting the design of transformers. To build on this theory, this paper analyzes a repository of popular transformer toys. Transformer toys are chosen for this study because of their richness in displaying a variety of kinematic aspects of transformation. Through this process, new definitions to describe transformation are garnered and a set of guidelines are developed to further aid designers. The empirical data set of transformer toys is rich in information and provides a basis for application to other fields, such as robotics and consumer products. These insights, in conjunction with the use of storyboarding, create a new method of designing transformers. This paper presents the method and concludes with a validation exercise in the creation of a new transformer toy."
611,"Six functions are identified as the most critical (“core” functions) to transportation vehicle energy systems. These selections are validated through analysis of 25 function structures as well as observations of a number of existing energy systems. Identifying which of the core functions and which of the energy types are involved in a given energy system is the Core-Function Modeling strategy (CFM strategy). These functions and energy types (the framework of CFM) are used to categorize approximately fifty processes and devices. This list is the Energy Morph Matrix (EMM). An experiment is performed that demonstrates how the EMM can be used as an aid to the concept generation process. The EMM also adapts well to a more automated approach for designing energy systems when used in combination with a search algorithm to identify chains of energy components. By incorporating a metric such as system efficiency or energy density into the search and computing this metric for each chain of energy components, these chains can be ranked and leading candidates can be highlighted for further analysis."
612,"This article presents a generic method to solve 2D multiobjective placement problem for free-form components. The proposed method is a relaxed placement technique combined with an hybrid algorithm based on a genetic algorithm and a separation algorithm. The genetic algorithm is used as a global optimizer and is in charge of efficiently exploring the search space. The separation algorithm is used to legalize solutions proposed by the global optimizer, so that placement constraints are satisfied. A test case illustrates the application of the proposed method. Extensions for solving the 3D problem are given at the end of the article."
613,"This article focuses on a key phase of the conceptual design, the synthesis of structural concepts of solution. Several authors have described this phase of Engineering Design. The Function-Behavior-Structure (FBS) is one of these models. This study is based on the combined use of a modified version of Gero’s FBS model and the latest developments of modeling languages for systems engineering. System Modeling Language (SysML) is a general-purpose graphical modeling language for specifying, analyzing, designing, and verifying complex systems. Our development shows how SysML types of diagrams match with our updated vision of the FBS model of conceptual design. The objective of this paper is to present the possibility to use artificial intelligence tools as members of the design team for supporting the synthesis process. The common point of expert systems developed during last decades for the synthesis of conceptual solutions is that their knowledge bases were application dependent. Latest research in the field of Ontology showed the possibility to build knowledge representations in a reusable and shareable manner. This allows the construction of knowledge representation for engineering in a more generic manner and dynamic mapping of the ontology layers. We present here how processing on ontology allows the synthesis of conceptual solutions."
614,"Successfully optimization of product designs calls for the continuous evolution of optimized design solutions, which is best achieved by collaboration among a group of experts who understand the intricacies of the product’s characteristics. The achievement of successful collaborations depends on optimization methodologies that focus on design characteristics located at deeper levels of hierarchically decomposed design problems, and the construction of optimization scenarios that have an explicit goal of maximizing the expected profits that result from the collaboration. This paper proposes methodologies and procedures based on hierarchical optimizations that aim to effectively conduct collaborative product design optimizations. The proposed methodologies are applied to a machine product design, and their effectiveness is demonstrated."
615,"This paper explores the effect of reward interdependence of strategies in a cooperative evolving team on the performance of the team. Experiments extending the Evolutionary Multi-Agent Systems (EMAS) framework to three dimensional layout are designed which examine the effect of rewarding "
616,"An information system that works in one application and environment may not work in another. Successful adoption of information systems requires that the organization evaluate candidates to ensure that they satisfy intended goals and consider the backgrounds and capabilities of the users. This paper describes an approach for evaluating and implementing information systems that satisfy technical requirements and organizational goals. Integral to this approach is the use of an assessment instrument consisting of objective-driven rubrics that are redundant to ensure consistency. This approach is applied in an NSF-supported CI-TEAM project to evaluate candidate systems to support an online cyber-collaboratory to enhance product dissection and reverse engineering activities in the classroom and to suggest improvements for the next generation system."
617,"A set-based approach to collaborative design is presented, in which Bayesian networks are used to represent promising regions of the design space. In collaborative design exploration, complex multilevel design problems are often decomposed into distributed subproblems that are linked by shared or coupled parameters. Collaborating designers often prefer conflicting values for these coupled parameters, resulting in incompatibilities that require substantial iteration to resolve, extending the design process lead time without guarantee of achieving a good design. In the proposed approach to collaborative design, each designer builds a locally developed Bayesian network that represents regions of interest in his design space. Then, these local networks are shared and combined with those of collaborating designers to promote more efficient local design space search that takes into account the interests of one’s collaborators. The proposed method has the potential to capture a designer’s preferences for arbitrarily shaped and potentially disconnected regions of the design space in order to identify compatible or conflicting preferences between collaborators and to facilitate a compromise if necessary. It also sets the stage for a flexible and concurrent design process with varying degrees of designer involvement that can support different designer strategies such as hill-climbing or region identification. The potential benefits are the capture of expert knowledge for future use as well as conflict identification and resolution. This paper presents an overview of the proposed method as well as an example implementation for the design of an unmanned aerial vehicle."
618,"This paper presents a method for assessing the quality of progressive design processes that seek to maximize the profitability of the product that is being designed. The proposed approach uses separations, a type of problem decomposition, to model progressive design processes. The subproblems in the separations correspond roughly to phases in the progressive design processes. We simulate the choices of a bounded rational designer for each subproblem using different search algorithms. We consider different types and versions of these search processes in order to determine if the results are robust to the decision-making model. We use a simple two-variable problem to help describe the approach and then apply the approach to assess motor design processes. Methods for assessing the quality of engineering design processes can be used to guide improvements to engineering design processes and generate more valuable products."
619,"Engineering designs are often determined by functional considerations, yet modeled with purely geometric parameters. Because of this, a difficult part of a design engineer’s job is tracking how changes to the geometric model might alter the functional performance of the design. This paper proposes a design interface that uses temporary functional views of geometric models to augment design engineers, helping them explore design space while continuously apprising them of the implications that modifications have on a design. The basic approach of the proposed interface is to use fast, interactive analysis tools in combination with feedback mechanisms to create temporary, functional design handles on top of the underlying geometric parametric structure. This design exploration tool is implemented in a research CAD system and demonstrated on illustrative examples."
620,"Early stages of engineering design processes are characterized by high levels of uncertainty due to incomplete knowledge. As the design progresses, additional information is externally added or internally generated within the design process. As a result, the design solution becomes increasingly well-defined and the uncertainty of the problem reduces, diminishing to zero at the end of the process when the design is fully defined. In this research a measure of uncertainty is proposed for a class of engineering design problems called "
621,"A supply chain connects product suppliers, manufacturers, as well as customers with the goal of managerial efficiency. Meanwhile, product design emphasizes the engineering efficiency of a product. Both supply chain management and product design have been drawing attention from numerous researchers. However, there has been only limited research on the integration of product design and supply chain. Despite this fact, there is significant potential for synergy in the integration of engineering and supply chain management, as well as managerial concepts into product design. In the paper, we present a methodology to form this synergistic connection. The methodology presented first generates functional requirements of a product. A design repository is then utilized to synthesize potential components of all sub-functions, providing multiple options for the potential conceptual designs. These concepts are screened by using a Design for Assembly (DfA) index and then a Design for Supply Chain (DfSC) index to select the best concept. An example from the bicycle industry is presented to demonstrate the benefit of supply chain considerations at the conceptual design phase."
622,"Choice models play a critical role in enterprise-driven design by providing a link between engineering design attributes and customer preferences. In our previous work, we introduced the hierarchical choice modeling approach to address the special needs in complex engineering system design. The hierarchical choice modeling approach utilizes multiple model levels to create a link between qualitative attributes considered by consumers when selecting a product and quantitative attributes used for engineering design. In this work, the approach is expanded to the Bayesian Hierarchical Choice Modeling (BHCM) framework, estimated using an All-at-Once (AAO) solution procedure. This new framework addresses the shortcomings of the previous method while providing a highly flexible modeling framework to address the needs of complex system design. In this framework, both systematic and random consumer heterogeneity is explicitly considered, the ability to combine multiple sources of data for model estimation and updating is significantly expanded, and a method to mitigate error propagated throughout the model hierarchy is developed. In addition to developing the new choice model approach, the importance of including a complete representation of consumer heterogeneity in the model framework is provided. The new modeling framework is validated using several metrics and techniques. The benefits of the BHCM method are demonstrated in the design of an automobile occupant package."
623,"This paper presents a comparative study of choice modeling and classification techniques that are currently being employed in the engineering design community to understand customer purchasing behavior. An in-depth comparison of two similar but distinctive techniques — the Discrete Choice Analysis (DCA) model and the C4.5 Decision Tree (DT) classification model — is performed, highlighting the strengths and limitations of each approach in relation to customer choice preferences modeling. A vehicle data set from a well established data repository is used to evaluate each model based on certain performance metrics; how the models differ in making predictions/classifications, computational complexity (challenges of model generation), ease of model interpretation and robustness of the model in regards to sensitivity analysis, and scale/size of data. The results reveal that both the Discrete Choice Analysis model and the C4.5 Decision Tree classification model can be used at different stages of product design and development to understand and model customer interests and choice behavior. We however believe that the C4.5 Decision Tree may be better suited in predicting attribute relevance in relation to classifying choice patterns while the Discrete Choice Analysis model is better suited to quantify the choice share of each customer choice alternative."
624,"Choice modeling is critical for assessing customer preferences as a function of product design attributes and customer profile information. Previous works have focused upon the use of survey data in which respondents are presented with a set of simulated product options from which they make a choice. However, such data does not represent real purchase behavior and these surveys require significant time and additional cost to administer. For these reasons, an approach to estimate a choice model using widely available customer satisfaction survey data for actual purchases is developed. Through a close examination of customer satisfaction survey data, several key characteristics are identified, including the lack of defined choice sets and missing choice attributes, the use of subjective measures such as ratings by customers to describe product attributes, multiple collinearity among many of the product attributes, and potentially insufficient attribute variation in the product designs evaluated by the respondents in the survey. A mixed logit based choice modeling procedure is developed in this paper to incorporate the use of both survey ratings as subjective measures and engineering attributes as quantitative measures in the model utility function. In order to accurately reflect choice behavior in actual market conditions, heterogeneity in customer preference is explicitly considered in the demand model. A case study using the Vehicle Quality Survey data acquired from J.D. Power and Associates demonstrates many of the key features of the proposed approach. The estimation results show the mixed logit model to be successful in modeling customer choices at the individual level, demonstrating the potential of being integrated with engineering models for engineering design."
625,"This paper articulates some of the challenges for what has been an implicit goal of design for market systems research: To predict demand for differentiated products so that counterfactual experiments can be performed based on changes to the product design (i.e., attributes). We present a set of methods for examining econometric models of consumer demand for their suitability in product design studies. We use these methods to test the hypothesis that automotive demand models that allow for nonlinear horizontal differentiation perform better than the conventional functional forms, which emphasize vertical differentiation. We estimate these two forms of consumer demand in the new vehicle automotive market, and find that using an ideal-point model of size preference rather than a monotonic model has model fit but different attribute substitution patterns. The generality of the evaluation methods and the range of demand model issues to be explored in future research are highlighted."
626,"Accurately capturing the future demand for a given product is a hard task in today’s new product development initiatives. As customers become more market-savvy and markets continue fragment, current demand models could greatly benefit from exploiting the rich contextual information that exists in customers’ product usage. As such, we propose a Usage Coverage Model (UCM) as a more thorough means to quantify and capture customer demand by utilizing factors of usage context in order to inform an integrated engineering design and choice modeling approach. We start by presenting the principles of the UCM model: terms, definitions, variable classes and relation classes so as to obtain a common usage language. The usage model exhibits the ability to differentiate between individuals’ product performance experiences. With Discrete Choice Analysis, individuals’ performance with a given product is compared against that of competitive products, capturing individual customers’ choice behavior and thereby creating an effective model of product demand. As a demonstration of our methods, we apply our model in a case study regarding the general task of cutting a wood board with a jigsaw tool. We conclude by presenting the scope of future work for the case study and the contribution of the entire current and future work to the field as a whole."
627,"Model fusion of results from disparate survey methodologies is a topic of current interest in both research and practice. Much of this interest has centered on the enrichment of stated-preference results with revealed-preference data, or vice versa, as it is considered that stated preference methods provide more robust trade-off information while revealed preference methods give better information about market equilibria. The motivation for this paper originates in the automotive industry, and is distinct in that it focuses on the reuse of existing data. Practitioners wish to glean as much information as possible from a large body of existing market research data, which may include minimally overlapping datasets and widely varying survey types. In particular, they wish to combine results from different types of stated preference methods. This paper presents two advancements in model fusion. One is a method for reducing data gathered in open-ended methods such as van Westendorp studies to a form amenable to analysis by multinomial logit, thus enabling the comparison of open-ended data to conjoint data on overlapping data sets. The other is a new statistical test for the fusibility of disparate data sets, designed to compare different methods of data comparison. This test is less sensitive than existing tests, which are most useful when comparing data sets that are substantially similar. The new test may thus provide more guidance in the development of new methods for fusing distinct survey types. Two examples are presented: a simple study of cell phone features administered as a test case for this research using both choice-based conjoint and van Westendorp methodologies, and a pair of existing larger-scale studies of automotive features with some attributes common to both studies. These examples serve to illustrate the two proposed methods. The examples indicate both a need for continued testing and several potentially fruitful directions for further investigation."
628,"The current research proposes an integrated framework for product design that incorporates simulation-based tools into the early design stage to achieve optimum multi-scale systems. The method to determine the appropriate mesostructure-property relations for the internal material structures of the system is through a topology optimization technique and a multi-scale design process. Specifically, the Reliability-based Topology Optimization (RBTO) and the simulation-based multi-attribute design method are integrated into an Inductive Design Exploration Method (IDEM). The RBTO method is contributed to determine of optimal topologies at the meso-scale. The simulation-based multi-attribute design method is considered for decision support process of the macro-scale systems. The IDEM offers the capability for concurrent design on multiple scales providing an approach for integration of the other two methods. An example of the developed multi-scale design framework is presented in terms of a hydrogen storage tank used in hydrogen fuel cell automotive applications. The multi-scale tank design will feature a high strength mesostructured wall resulting in a large weight reduction."
629,"A phase transition is a geometric and topological transformation process of materials from one phase to another, each of which has a unique and homogeneous physical property. Providing an initial guess of transition path for further physical simulation studies is highly desirable in materials design. In this paper, we present a metamorphosis scheme for periodic surface (PS) models by interpolation in the PS parameter space. The proposed approach creates multiple potential transition paths for further selection based on three smoothness criteria. The goal is to search for a smooth transformation in phase transition analysis."
630,"In this paper, we introduce a design exploration method for adaptive design systems (DEM-ADS), which is proposed to manage the uncertainty in the system design process. The proposed method includes the local regression model and an inverse design procedure derived from the existing Inductive Design Exploration Method (IDEM). To demonstrate the proposed method, the design of a photonic crystal coupler and waveguide containing two subsystem analyses is presented. The result indicates that the proposed method effectively attains solutions that are robust to uncertainty in the system more efficiently than IDEM."
631,"In this paper, we introduce the construct of microstructure-mediated design of material and product. The microstructure of the material is controlled within feasible bounds to achieve the performance targets of the product. We illustrate the efficacy of this construct via the integrated robust design of a submersible and Al-based matrix composites. The integrated design is carried out using an Inductive Design Exploration Method (IDEM) that facilitates robust design in the presence of model structure uncertainty (MSU). Model structural uncertainty (MSU), originating from assumptions and idealizations in modeling processes, is a form of uncertainty that is often virtually impossible to quantify. In this paper, we demonstrate a method, the Inductive Design Exploration Method (IDEM), that facilitates robust design in the presence of model structural uncertainty. We achieve robustness by trading off the degree of system performance and the degree of reliability based on structural uncertainty associated with system models (i.e., models for performances and constraints). IDEM is demonstrated in the design of a shell of a robotic submersible. The material considered is "
632,"A multiscale design methodology is proposed in this paper to facilitate the design of hierarchical material and product systems with the consideration of random field uncertainty that propagates across multiple length scales. Based on the generalized hierarchical multiscale decomposition pattern in multiscale modeling, a set of computational techniques are developed to manage the complexity of multiscale design under uncertainty. Novel design of experiments and metamodeling strategies are proposed to manage the complexity of propagating random field uncertainty through three generalized levels of transformation: the material microstructure random field, the material property random field, and the probabilistic product performance. Multilevel optimization techniques are employed to find optimal design solutions at individual scales. A hierarchical multiscale design problem that involves a 2-scale (submicro- and miro-scales) material design and a macro-scale product (bracket) design is used to demonstrate the applicability and benefits of the proposed methodology."
633,"This paper presents a GPU-based parallel Population Based Incremental Learning (PBIL) algorithm with a local search on bound constrained optimization problems. The genotype of an entire population is evolved in PBIL, which was derived from Genetic Algorithms. Graphics Processing Units (GPU) is an emerging technology for desktop parallel computing. In this research, the classical PBIL is adapted in the data-parallel GPU computing platform. The global optimal search of the PBIL is enhanced by a local Pattern Search method. The hybrid PBIL method is implemented in the GPU environment, and compared to a similar implementation in the common computing environment with a Central Processing Unit (CPU). Computational results indicate that GPU-accelerated PBIL method is effective and faster than the corresponding CPU implementation."
634,"A distributed variant of multi-objective particle swarm optimization (MOPSO) called multi-objective parallel asynchronous particle swarm optimization (MOPAPSO) is presented and the effects of distribution of objective function calculations to slave processors on the results and performance are investigated. Two benchmark examples were used to verify the capability of this implementation of MOPAPSO to match previously published results of MOPSO. The computationally intensive task of multi-objective Optimization Based Mechanism Synthesis (OBMS) was used to verify that significant performance improvements were realized through parallelization. The results show that MOPAPSO is able to match the results of MOPSO in significantly less time. The fact that MOPAPSO is distributed results in an effective optimization tool for complex multi-objective design problems."
635,"Particle swarm methodologies are presented for the solution of constrained mechanical and structural system optimization problems involving single or multiple objective functions with continuous or mixed design variables. The particle swarm optimization presented is a modified particle swarm optimization approach, with better computational efficiency and solution accuracy, is based on the use of dynamic maximum velocity function and bounce method. The constraints of the optimization problem are handled using a dynamic penalty function approach. To handle the discrete design variables, the closest discrete approach is used. Multiple objective functions are handled using a modified cooperative game theory approach. The applicability and computational efficiency of the proposed particle swarm optimization approach are demonstrated through illustrate examples involving single and multiple objectives as well as continuous and mixed design variables. The present methodology is expected to be useful for the solution of a variety of practical engineering design optimization problems."
636,"Economic and physical considerations often lead to equilibrium problems in multidisciplinary design optimization (MDO), which can be captured by MDO problems with complementarity constraints (MDO-CC) — a newly emerging class of problem. Due to the ill-posedness associated with the complementarity constraints, many existing MDO methods may have numerical difficulties solving the MDO-CC. In this paper, we propose a new decomposition algorithm for MDO-CC based on the regularization technique and inexact penalty decomposition. The algorithm is presented such that existing proofs can be extended, under certain assumptions, to show that it converges to stationary points of the original problem and that it converges locally at a superlinear rate. Numerical computation with an engineering design example and several analytical example problems shows promising results with convergence to the all-in-one (AIO) solution."
637,"This paper proposes a hierarchical optimization-based approach for two-dimensional rectangular layout design problems. While decomposition-based optimization has been a key approach for the complicated design problems under the trend of multidisciplinary design optimization, it has focused on continuous ones. While various approaches for layout design have been developed, they are based on any evolutionary algorithm for effectively handling its combinatorial nature. This paper aims to bring a new paradigm by combining decomposition-based optimization and evolutionary algorithms toward solving complicated layout design problems. In the approach, the Pareto optimality of subsystem-level layout against the optimality of system-level layout is extracted through two-level hierarchical formulation. Then, a computational design algorithm is developed. It represents the layout topology with sequence-pair and the shape of each subsystem or component with the aspect ratio, and optimizes them with genetic algorithms. The Pareto optimality of sub-levels is handled with multi-objective genetic algorithms, in which a set of Pareto are simultaneously generated. Top-level and sub-level layout problems are coordinated through exchange of preferable ranges of shapes and layout. An implemented approach is applied to an example problem for demonstrating its performance and capability."
638,"Solid freeform fabrication (SFF) processes based on mask image projection have the potential to be fast and inexpensive. More and more research and commercial systems have been developed based on these processes. For the SFF processes, the mask image planning is an important process planning step. In this paper, we present an optimization based method for mask image planning. It is based on a light intensity blending technique called pixel blending. By intelligently controlling pixels’ gray scale values, the SFF processes can achieve a much higher "
639,"Companies applying mass customization paradigm regard the design process as a configuration task where the solution is achieved through the extraction of a new instance from a modular product structure. In this context product configuration management tools are evermore important. Although tools have been already proposed, they fail in real industrial contexts. Main causes are recognizable in high efforts in systems implementation and lack of flexibility in products updating. This research aims to develop an approach to overcome drawbacks and simplify the implementation and the use of product configuration systems also in redesign activities. The paper initially reviews existing systems in terms of design knowledge representation methods and product structure formalization techniques. Then, an approach based on Configuration Virtual Prototypes which store and manage different levels of knowledge, is presented. In particular, a framework is outlined in order to represent design data and its formalization in configuration tools. Three different domains are managed and connected via Configuration Virtual Prototypes: Product Specifications, Geometrical Data and Product Knowledge. Specifically, geometrical data aspects are analyzed in detail providing approaches for eliciting knowledge introduced by parametric template CAD models. The approach will be exemplified through a real application example where an original tool has been developed on the based of the described method. Benefits of the system will be shown and briefly discussed, in particular in terms of reachable flexibility in solutions."
640,"Configurators have been generally accepted as important tools to interact with customers and elicit their requirements in the form of tangible product specification. These interactions, commonly called product configuring process, aim to find the best match between customers’ requirements and company’s offerings. Therefore an efficient configurator should take both product structure and customers’ preferences into consideration. In this paper, we present a novel iterative method of attributes selection for product configuring procedure. The algorithm is based on Shapley value, a concept used in game theory to estimate the usefulness of certain entities. It iteratively selects the most relevant attribute from the remaining attributes pool and proposes it for customers to configure. Thus it obtains customers’ specification in an adaptive manner in the sense that different customers may have different query sequences. Information content is used as the measure of usefulness. As a result, the most uncertainty can be eliminated and product development team has a better understanding of what customers want in a fix time horizon. Maximum a posteriori criterion is also exploited to give product recommendation based on the partially configured product configuration. Thus the customized 1-to-1 configuring procedure is presented and the recommendation can converge to a customer’s target with fewer interactions between the customers and designers. We also use a case of PC configurator to exemplify and test the viability of the presented method."
641,"The issue of overlapping or intersection in the toolpath is an issue in the metal deposition as the material is continuously added to the previous layer. Using various control schemes can achieve a better quality. However, such schemes fail to delivery satisfactory results when there are different intersection angles in the deposition toolpath. In order to overcome the problem caused by the intersection angle, the effect of them on the deposition should be well studied. This paper discusses the effect of toolpath intersection angle using design of experiment method. The impact of various parameters in the deposition process is studied. The approach and method can be integrated with path planning for a metal deposition process."
642,"In regular 3 axis layered manufacturing processes, the build direction is fixed throughout the process. Multi-axis laser (more than 3-axis motion) deposition process, the orientation of the part can affect the non-support buildability in the multi-axis hybrid manufacturing process. However, each orientation that satisfies the buildability and other constraints may not be unique. In this case, the final optimal orientation is determined based on build time. The build time computation algorithm for multi-axis hybrid system is presented in this paper. To speed up the exhaustive search for the optimal orientation, a multi-stage algorithm is developed to reduce the search space."
643,"The geometric variations in a tolerance-zone can be modeled with hypothetical point-spaces called Tolerance-Maps (T-Maps) for purposes of automating the assignment of tolerances during design. The objective of this paper is to extend this model to represent tolerances on circular runout which limit geometric manufacturing variations to a specified tolerance-zone. Such a zone is an annular area at one transverse cross-section for spherical, conical, or cylindrical objects (features), but it is a short cylinder when the feature is a round or annular segment of a plane. Depending on the kind of feature and the tolerances that are specified for it, the model may be used to represent variations within tolerance-zones for circular runout, size, position, orientation, and form. In this paper, the Tolerance-Map (T-Map) is a hypothetical volume of points that captures all the circular variations that can arise from these tolerances. The model is compatible with the ASME/ANSI/ISO Standards for geometric tolerances. T-Maps have been generated for other classes of geometric tolerances in which the variation of the feature are represented with a plane or line, and these have been incorporated into testbed software for aiding designers when assigning tolerances for assemblies. In this paper the T-Map for circular runout is created and, for the first time, circles are used to represent the geometric variations of a feature in tolerance-zones."
644,"Integration of finite element analysis (FEA) into design is important for complex product development. However, how to automatically and robustly generate the analysis model from the design model, and how to organize information needed by CAD and FEA for the efficient integration is still a big problem. In this study, analysis feature model (AFM) is proposed and developed as a central model for design and analysis. Five kinds of information are contained in AFM which are analysis-related information, geometric-related information, element-related information, boundary condition and coupling information between features. A systematic approach is elaborated to automatically generate the AFM from the design model which mainly contains four steps: automated recognition of analysis features, decomposition of the design model, reduction and combination into AFM. The analysis model can be easily obtained from the AFM. At last the proposed method is implemented and some examples are given."
645,"Current CAD systems provide utilities to position and orient parts with respect to each other in assemblies through associative relations that include geometric and parametric constraints. However, assembly features are not explicitly defined, nor available in CAD databases for exploitation in downstream applications. This paper examines the attributes of assembly features and proposes a template for their definition in a uniform way. The template can be used in conjunction with an EXPRESS-like language (N-Rep) to define assembly features that are implementation independent. The template includes slots for part features, mating relations, geometric, parametric, kinematic and structural relations. The current application for assembly features being explored is for Reverse Engineering of legacy parts. Assembly features serve as “knowledge containers” in that they allow one to encode form, fit and function in a uniform way so that replacement parts can be redesigned to meet key requirements."
646,"A virtual plant, built in a computer by using computer graphic (CG) and Virtual Reality (VR), can model the precise and whole structure of an integrated manufacturing system and simulate its physical and logical behavior in operation. This paper aims to reveal the advanced modeling and VR realization methods in developing a virtual forging plant for automatic and programmed open-die forging processes. Two sub-models, component model and process model, compose the overall modeling architecture of a virtual integrated open-die forging plant. The coordinated motion simulation of the integrated system is realized through a kinematic modeling method. A compound stiffness modeling method is then developed to simulate the mechanical behavior in operation. The process simulation of the virtual plant is conducted on the basis of the above two modeling methods. A practical application example of virtual plant for integrated open-die forging process is presented towards the end."
647,"The task of planning a path between two spatial configurations of an artifact moving among obstacles is an important problem in many geometrically-intensive applications. Despite the ubiquity of the problem, the existing approaches make specific limiting assumptions about the geometry and mobility of the obstacles, or those of the environment in which the motion of the artifact takes place. In this paper we propose a powerful approach for 2D path planning in a dynamic environment that can undergo drastic topological changes. Our algorithm is based on a potent paradigm for medial axis computation that relies on constructive representations of shapes with R-functions that operate on real-valued half-spaces as logic operations. Our approach can handle problems in which the environment is not fully known "
648,"Planning for Computerized Numerical Control (CNC) fabrication requires generation of process plans for the fabrication of parts that can be executed on CNC enabled machine tools. To create such plans, a large amount of domain specific knowledge is required to map the desired geometry of a part to a manufacturing process, thus decomposing design information into a set of feasible machining operations. Approaches to automate this planning process still rely heavily on human capabilities, such as planning and reasoning about geometry in relation to machining capabilities. In this paper, the authors present a new, shape grammar-based approach for automatically creating fabrication plans for CNC machining from a given part geometry. To avoid the use of static feature sets and their pre-defined mappings to machining operations, the method encodes knowledge of fundamental machine capabilities. A method for generating a vocabulary of removal volume shapes based on the available tool set and machine tool motions is defined in combination with a basic rule set for shape removal covering tool motion, removal volume calculation and CNC code generation. The use of shape grammars as a formalism enables systematic formulation of hard and soft constraints on spatial relations between the volume to be removed and the removal volume shape for a machining operation. The method is validated using an example of machining a simple part on a milling machine. Overall, the approach and method presented is an enabler for the creation of an autonomous fabrication system and CNC machine tools that are able to reason about part geometry in relation to available capabilities and carry out on-line planning for CNC fabrication."
649,"The design language allows the construction of a variety of airplan designs. The syntax of the design language relies on the standardized Unified Modeling Language (UML) and consists of an object-oriented vocabulary (i.e. points, lines, profiles, wings, etc.) comparable to building blocks, and design rules (i.e. building laws) which represent the building knowledge used. In the terminology of graph-based design languages, the building blocks are the information objects which represent the static aspects of the design because they represent indivisible design entities. They are represented as UML classes and instances and their interrelation forms an object-oriented class hierarchy. The design rules represent the dynamic aspects of the design and express the building knowledge as stepwise activities. Finally, a production system (i.e. a specific rule set) is able to create an airplane geometry and generates design variants through manual modifications of the production system."
650,"Hand-drawn sketches are powerful cognitive devices for the efficient exploration, visualization and communication of emerging ideas in engineering design. It is desirable that CAD/CAE tools be able to recognize the back-of-the-envelope sketches and extract the intended engineering models. Yet this is a nontrivial task for freehand sketches. Here we present a novel, neural network-based approach designed for the recognition of network-like sketches. Our approach leverages a trainable, detector/recognizer and an autonomous procedure for the generation of training samples. Prior to deployment, a Convolutional Neural Network is trained on a few labeled prototypical sketches and learns the definitions of the visual objects. When deployed, the trained network scans the input sketch at different resolutions with a fixed-size sliding window, detects instances of defined symbols and outputs an engineering model. We demonstrate the effectiveness of the proposed approach in different engineering domains with different types of sketching inputs."
651,"In current product design, significant effort is put into creating aesthetically pleasing product forms. Often times, the final shape evolves in time based on designers’ ideas externalized through early design activities primarily involving conceptual sketches. While designers negotiate and convey a multitude of different ideas through such informal activities, current computational tools are not well suited to work from such forms of information to leverage downstream design processes. As a result, many promising ideas either remain under-explored, or require restrictive added effort to be transformed into digital media. As one step toward alleviating this difficulty, we propose a new computational method for capturing and reusing knowledge regarding the shape of a developing design from designers’ hand-drawn conceptual sketches. At the heart of our approach is a geometric learning method that involves constructing a continuous space of meaningful shapes via a deformation analysis of the constituent exemplars. The computed design space serves as a medium for encoding designers’ shape preferences expressed through their sketches. With the proposed approach, designers can record desirable shape ideas in the form of raw sketches, while utilizing the accumulated information to create and explore novel shapes in the future. A key advantage of the proposed system is that it enables prescribed engineering and ergonomic criteria to be concurrently considered with form design, thus allowing such information to suitably guide conceptual design processes in a timely manner."
652,"This paper describes a method for using text description as a natural interface to construct models of mechanical systems. The goal is to convert the natural text description from the user to a system of equations that can be used to model and simulate the system. The algorithm of this process consists of three main stages: 1-Component Extraction, 2-Interaction detection and 3-Attribute detection. The description is first parsed to identify and instantiate components. The text is then scanned again to analyze the action verbs to identify component attributes and to detect connections between the components. Finally, numerical values and initial conditions are associated with the attributes. System equations are generated by detecting loops in the device description. The knowledge is gradually built up through progressive scanning and analysis of text. The paper describes the algorithms, presents a detailed example and identifies current assumptions and restrictions."
653,"Although many metamodeling methods have been developed in the past decades to model the relationships between input and output parameters, selection of an appropriate or the optimal metamodel for solving a certain engineering problem is not a trivial task. Various performance measures of different metamodels are strongly influenced by the characteristics of sample data. This research focuses on the study of the relationships between sample data characteristics and metamodel performance measures considering different types of metamodeling methods. In this research, sample quality merits are introduced to quantitatively model the characteristics of sample data. In this work, four types of metamodeling methods, including multivariate polynomial model, radial basis function model, kriging model and Bayesian neural network model, three sample quality merits, including sample size, uniformity and noise, and four performance evaluation measures, including accuracy, confidence, robustness and efficiency, are considered to study the relationships between the sample quality merits and the performance measures of the metamodeling methods."
654,"This paper presents a Response Surface Modeling (RSM) approach for solving the engine mount optimization problem for a motorcycle application. A theoretical model that captures the structural dynamics of a motorcycle engine mount system is first used to build the response surface model. The response surface model is then used to solve the engine mount optimization problem for enhanced vibration isolation. Design of Experiments (DOE), full factorial and fractional factorial formulations, are used to construct the governing experiments. Normal probability plots are used to determine the statistical significance of the variables and the significant variables are then used to build the response surface. The design variables for the engine mount optimization problem include mount stiffness, position vectors and orientation vectors. It is seen that RSM leads to a substantial reduction in computational effort and yields a simplified input-output relationship between the variables of interest. However, as the number of design variables increases and as the response becomes irregular, conventional use of RSM is not viable. Two algorithms are proposed in this paper to overcome the issues associated with the size of the governing experiments and problems associated with modeling of the orientation variables. The proposed algorithms divide the design space into sub-regions in order to manage the size of the governing experiments without significant confounding of variables. An iterative procedure is used to overcome high response irregularity in the design space, particularly due to orientation variables."
655,"Modeling or approximating high dimensional, computationally-expensive, black-box problems faces an exponentially increasing difficulty, the “curse-of-dimensionality”. This paper proposes a new form of high-dimensional model representation (HDMR) by integrating the radial basis function (RBF). The developed model, called RBF-HDMR, naturally explores and exploits the linearity/nonlinearity and correlation relationships among variables of the underlying function that is unknown or computationally expensive. This work also derives a lemma that supports the divide-and-conquer and adaptive modeling strategy of RBF-HDMR. RBF-HDMR circumvents or alleviates the “curse-of-dimensionality” by means of its explicit hierarchical structure, adaptive modeling strategy tailored to inherent variable relation, sample reuse, and a divide-and-conquer space-filling sampling algorithm. Multiple mathematical examples of a wide scope of dimensionalities are given to illustrate the modeling principle, procedure, efficiency, and accuracy of RBF-HDMR."
656,"The use of surrogates for facilitating optimization and statistical analysis of computationally expensive simulations has become commonplace. Usually, surrogate models are fit to be unbiased (i.e., the error expectation is zero). However, in certain applications, it might be interesting to safely estimate the response (e.g., in structural analysis, the maximum stress must not be underestimated in order to avoid failure). In this work we use safety margins to conservatively compensate for fitting errors associated with surrogates. We propose the use of cross-validation for estimating the required safety margin for a given desired level of conservativeness (percentage of safe predictions). We also check how well we can minimize the losses in accuracy associated with conservative predictor by selecting between alternate surrogates. The approach was tested on two algebraic examples for ten basic surrogates including different instances of kriging, polynomial response surface, radial basis neural networks and support vector regression surrogates. For these examples we found that cross-validation (i) is effective for selecting the safety margin; and (ii) allows us to select a surrogate with the best compromise between conservativeness and loss of accuracy. We then applied the approach to the probabilistic design optimization of a cryogenic tank. This design under uncertainty example showed that the approach can be successfully used in real world applications."
657,"Metamodeling techniques are increasingly used in solving computation intensive design optimization problems today. In this work, the issue of automatic identification of appropriate metamodeling techniques in global optimization is addressed. A generic, new hybrid metamodel based global optimization method, particularly suitable for design problems involving computation intensive, black-box analyses and simulations, is introduced. The method employs three representative metamodels concurrently in the search process and selects sample data points adaptively according to the values calculated using the three metamodels to improve the accuracy of modeling. The global optimum is identified when the metamodels become reasonably accurate. The new method is tested using various benchmark global optimization problems and applied to a real industrial design optimization problem involving vehicle crash simulation, to demonstrate the superior performance of the new algorithm over existing search methods. Present limitations of the proposed method are also discussed."
658,"Many meta-models have been developed to approximate true responses. These meta-models are often used for optimization instead of computer simulations which require high computational cost. However, designers do not know which meta-model is the best one in advance because the accuracy of each meta-model becomes different from problem to problem. To address this difficulty, research on the ensemble of meta-models that combines stand-alone meta-models has recently been pursued with the expectation of improving the prediction accuracy. In this study, we propose a selection method of weight factors for the ensemble of meta-models based on "
659,"Similarity methods have been widely employed in engineering design and analysis to model and scale complex systems. The Empirical Similitude Method (ESM) is one such method based on the use of experimental data. Using a variant of the similitude process involving experimental data, we present in this paper, the use of advanced numerical approximations, trigonometric functions in particular to model and predict the performance of design artifacts. Specifically, an airfoil design is modeled, and the values of the drag coefficient are estimated based on the advanced ESM. Intermediate test specimens are used to correlate experimental data to produce the required prediction parameters. Mathematical development and error analysis are also elaborated by delving into continuity and adaptivity features of numerical algorithms."
660,"Metamodeling techniques are now being widely used by many industries to replace complex and expensive simulation models so that optimization and probabilistic design studies can be done in a more practical and affordable way. Due to the complexity of many engineering design systems and to the lack of a deep understanding of the metamodeling methods by engineers, many questions related to metamodeling accuracy, confidence, robustness, efficiency, etc. are now frequently asked. The need to establish comprehensive guidelines for engineers to correctly and efficiently apply metamodeling methods to their optimization and probabilistic design tasks is becoming more and more important. Based upon experiences and lessons learned at General Electric in recent years, this paper discusses important metamodeling mathematical details and addresses several common issues engineers are likely to encounter when applying metamodeling techniques to realistic engineering problems. The paper provides detailed guidelines on the best practices of metamodel creation and its application to the design process. Many results from benchmark examples and real applications are included to justify certain guidelines and rules of thumb."
661,"A numerical study for a functional design of honeycomb meta-materials targeting flexible shear properties (about 6.5MPa effective shear modulus and 15% maximum effective shear strain) is conducted with two material selections — polycarbonate (PC) and mild-steel (MS), and five honeycomb configurations. Cell wall thicknesses are found for each material to reach the target shear modulus for available cell heights with five honeycomb configurations. PC honeycomb structures can be tailored with 0.4 to 1.3mm cell wall thicknesses to attain the 6.5MPa shear modulus. MS honeycombs can be built with 0.2mm or lower wall thicknesses to reach the target shear modulus. Sensitivity of wall thickness on effective properties may be a hurdle to overcome when designing metallic honeycombs. The sensitivity appears to be more significant with an increased number of unit cells in the vertical direction. PC auxetic honeycombs having 0.4 to 1.9 mm cell wall thicknesses show 15% maximum effective shear strain without local cell damage. Auxetic honeycombs having negative Poisson’s ratio show lower effective shear moduli and higher maximum effective shear strains than the regular counterparts, implying that auxetic honeycombs are candidate geometries for a shear flexure design."
662,"A new compliant parallel micromanipulator is proposed in this paper. The manipulator has three degrees of freedom (DOF) and can generate motions in a microscopic scale. It can be used for biomedical engineering and fiber optics industry. In the paper, the detailed design of the structure is first introduced, followed by the kinematic analysis and performance evaluation. Second, a finite-element analysis of resultant stress, strain, and deformations is evaluated based upon different inputs of the three piezoelectric actuators. Finally, the genetic algorithms and radial basis function networks are implemented to search for the optimal architecture and behavior parameters in terms of global stiffness, dexterity and manipulability."
663,"Sensitivity analysis has received significant attention in engineering design. While sensitivity analysis methods can be global, taking into account all variations, or local, taking into account small variations, they generally identify which uncertain parameters are most important and to what extent their effect might be on design performance. The extant methods do not, in general, tackle the question of which ranges of parameter uncertainty are most important or how to best allocate investments to partial uncertainty reduction in parameters under a limited budget. More specifically, no previous approach has been reported that can handle single-disciplinary multi-output global sensitivity analysis for both a single design and multiple designs under interval uncertainty. Two new global uncertainty metrics, i.e., radius of output sensitivity region and multi-output entropy performance, are presented. With these metrics, a multi-objective optimization model is developed and solved to obtain fractional levels of parameter uncertainty reduction that provide the greatest payoff in system performance for the least amount of “investment”. Two case studies of varying difficulty are presented to demonstrate the applicability of the proposed approach."
664,"Uncertainty in the input parameters to an engineering system may not only degrade the system’s performance, but may also cause failure or infeasibility. This paper presents a new sensitivity analysis based approach called Design Improvement by Sensitivity Analysis (DISA). DISA analyzes the interval parameter uncertainty of a system and, using multi-objective optimization, determines an optimal combination of design improvements required to enhance performance and ensure feasibility. This is accomplished by providing a designer with options for both uncertainty reduction and, more importantly, slight design adjustments. The approach can provide improvements to a design of interest that will ensure a minimal amount of variation in the objective functions of the system while also ensuring the engineering feasibility of the system. A two stage sequential framework is used in order to effectively employ metamodeling techniques to approximate the analysis function of an engineering system and greatly increase the computational efficiency of the approach. This new approach has been applied to two engineering examples of varying difficulty to demonstrate its applicability and effectiveness."
666,"Over the last few years, research activity in approximation (e.g. metamodels) and optimization (e.g. genetic algorithms) methods has improved upon current practices in engineering design and optimization of complex systems with respect to multiple performance metrics, by reducing the number of evaluations of the system’s model that are needed to obtain the set of non-dominated solutions to a given multi-objetive optimal design problem. To this end, several authors have proposed to enhance Multi-Objective Genetic Algorithms (MOGAs) with metamodel-based pre-screening criteria (PSC), so that only those solutions that have the most potential to improve the current approximation of the Pareto Front are evaluated with the (costly) system model. The main goals of this work are to compare the performance of several PSC with an array of test functions taken from the literature, and to study the potential effect on their effectiveness and efficiency of using multi-response metamodels, instead of building independent, individual metamodels for each objective function, as has been done in previous work. Our preliminary results show that no single PSC is observed to be superior overall, though the Minimum of Minimum Distances and Expected Improvement criteria outperformed other PSC in most cases. Results also show that the use of multi-response metamodels improved both the effectiveness and efficiency of PSC and the quality of solution at the end of the optimization in 50% to 60% of test cases."
667,"Many studies that examine the impact of renewable energy installations on avoided carbon-dioxide utilize national, regional or state averages to determine the predicted carbon-dioxide offset. The approach of this computational study was to implement a dispatching strategy in order to determine precisely which electrical facilities would be avoided due to the installation of renewable energy technologies. This study focused on a single geographic location for renewable technology installation, San Antonio, Texas. The results indicate an important difference between calculating avoided carbon-dioxide when using simple average rates of carbon-dioxide emissions and a dispatching strategy that accounts for the specific electrical plants used to meet electrical demands. The avoided carbon-dioxide due to renewable energy technologies is overestimated when using national, regional and state averages. This occurs because these averages include the carbon-dioxide emission factors of electrical generating assets that are not likely to be displaced by the renewable technology installation. The study also provides a comparison of two specific renewable energy technologies: photovoltaics (PV) and wind turbines. The results suggest that investment in PV is more cost effective for the San Antonio location. While the results are only applicable to this location, the methodology is useful for evaluating renewable technologies at any location."
668,"This paper proposes a structural optimization-based method for the design of compliant mechanism scissors in which the proposed design criteria are based on universal design principles. The first design criterion is the distance from the hand-grip to the center of gravity of the scissors, which should be minimized to reduce the physical effort required of the people using the device. The second design criterion is that of failure tolerance, where the effects of traction applied in undesirable directions upon the performance of the compliant mechanism should be minimized. Based on the proposed design criteria, a multiobjective optimization problem for the universal design of a compliant mechanism scissors is formulated. Furthermore, to obtain an optimal configuration, a new type of topology optimization technique using the level set function to represent structural boundaries is employed. This optimization technique enables rapid verification of resulting design configurations since the boundary shapes of the obtained design solution candidates can be easily converted to finite element models which are then used in large deformation analyses. Finally, the proposed design method is applied to design examples. The optimal configurations obtained by the proposed method provide good universal design performance, indicating the effectiveness and usefulness of the proposed method."
669,"This paper proposes a new integrated optimization of a functional structure and a components layout for supporting a conceptual design. A conceptual design is the second phase of a product development, where designers build up the functional structure and the components layout of the target design object as the design concept. However, a functional design and a layout design are very different tasks, there is a lot of flexibility for decision makings during them and its solution space is vast. Therefore, it is extremely difficult for designers to build up an optimal design concept by considering various design requirements themselves. To overcome this limitation, this paper develops a new design method based on optimization techniques. This method consists of two optimizations, a functional optimization and a layout optimization, and obtains the optimal solutions by cooperatively executing two optimizations. Specifically, a functional optimization based on a GA is the main part of the proposed method and executed just one time whereas a layout optimization is executed to calculate the layout with minimum area (or volume) for each design solution for each iteration during the process of the functional optimization and the result is used as one of its valuation characteristics. Using the proposed method, designers can simultaneously obtain both the functional structure and the components layout of the target design object that satisfies performance, cost and area at a high level. To demonstrate the flow of the proposed method and confirm its effectiveness, this paper describes the case study, where internal devices of a personal computer are designed using the proposed method."
670,"Mechanical components are cleaned after manufacturing processes like casting and machining using high pressure waterjets that help in both dislodging and removal of contaminants. In order to understand the dynamic relationships that exist in an actual cleaning process, it is essential to visualize the interaction of the water-jet with the part geometry. To aid in this, we have developed a simplified model of the water-mill that simulates the cleaning process on parts represented using standard CAD geometries. Our model of the cleaning process approximates the water-jets in a water-mill as a set of rays originating from the nozzles and evaluates the pressure the water-jet exerts when it hits the surface of the part. This model can be used to understand the effect of kinematic parameters like nozzle diameter and standoff distance on cleaning. Furthermore, this model can be used to optimize these parameters. Any standard optimization technique can be used for this optimization. As proof of concept, we used a Genetic Algorithm (GA) to optimize the process parameters for the simplified cleaning process on a flat plate and a curved surface. Analysis of the results indicates that the obtained solution is theoretically an optimum."
671,"Plug-in hybrid electric vehicle (PHEVs) technology has the potential to address economic, environmental, and national security concerns in the United States by reducing operating cost, greenhouse gas (GHG) emissions and petroleum consumption. However, the net implications of PHEVs depend critically on the distances they are driven between charges: Urban drivers with short commutes who can charge frequently may benefit economically from PHEVs while also reducing fuel consumption and GHG emissions, but drivers who cannot charge frequently are unlikely to make up the cost of large PHEV battery packs with future fuel cost savings. We construct an optimization model to determine the optimal PHEV design and optimal allocation of PHEVs, hybrid-electric vehicles (HEVs) and conventional vehicles (CVs) to drivers in order to minimize net cost, fuel consumption, and GHG emissions. We use data from the 2001 National Household Transportation Survey to estimate the distribution of distance driven per day across vehicles. We find that (1) minimum fuel consumption is achieved by assigning large capacity PHEVs to all drivers; (2) minimum cost is achieved by assigning small capacity PHEVs to all drivers; and (3) minimum greenhouse gas emissions is achieved by assigning medium-capacity PHEVs to drivers who can charge frequently and large-capacity PHEVs to drivers who charge less frequently."
672,"One-of-a-kind production (OKP) is a new manufacturing paradigm to produce customized products based on requirements of individual customers while maintaining the quality and efficiency of mass production. In this research, a customer-centric product modeling scheme is introduced to model OKP product families by incorporating the customer information. To develop this modeling scheme, data mining techniques, including fuzzy pattern clustering method, and hybrid attribute reduction method, are employed to achieve the knowledge from the historical data. Based on the achieved knowledge, the different patterns of OKP products are modeled by different sub-AND-OR trees trimmed from the original AND-OR tree. Since only partial product descriptions in a product family are used to identify the optimal custom product based on customer requirements, the efficiency of custom product identification process can be improved considerably. A case study to identify the optimal configuration and parameters of window products in an industrial company is used to demonstrate the effectiveness of the introduced approach."
673,"Manufacturing firms use product families to provide variety while maintaining economies of scale to improve manufacturing productivity. Designing a successful product family requires consideration of both customer preferences and the competition. This paper presents a design for market systems approach to product family design and solves the problem of designing a product family when the competition is simultaneously designing its product family. In particular, the problem is formulated as a two-player zero-sum game. Our analysis of this problem shows that it can be separated into multiple subproblems whose solution provides an optimal solution to the original problem. The paper presents an example to illustrate the approach."
674,"In this work, a methodology and an integrated tool framework has been developed for automated design of an industrial robot family consisting of four robot members. For each robot, performance requirements concerning payloads, reaches, and time performances are specified. A 3D design tool, namely SolidWorks, has been integrated with robot kinematics and dynamics simulation tools for simultaneous kinematics and dynamics design. A motor library comprising both geometric data and physical data has also been integrated in the tool framework. The automated design of the robot family has been formulated as a multi-objective and mixed variable design optimization problem. The arm modules are treated as continuous design variables while the motors are treated as discrete variables. Due to the characteristics of this mixed variable design optimization problem a genetic algorithm (GA) has been used. This work has successfully demonstrated the feasibility for achieving automatic design of an industrial robot family."
675,"Innovative companies that generate a variety of products and services for satisfying customers’ specific needs are invoking and increasing research on mass-customized products, but the majority of their efforts are still focused on general consumers who are without disabilities. This research is motivated by the need to provide a basis of universal design guidelines and methods, primarily because of a lack of knowledge on disabilities in product design as well as methods for designing and evaluating products for everyone. Product family design is a way to achieve cost-effective mass customization by allowing highly differentiated products to be developed from a common platform while targeting products to distinct market segments. By extending concepts from product family design and mass customization to universal design, we propose a method for developing a universal product family to generate economical feasible design concepts and evaluating design feasibility with respect to disabilities within dynamic market environments. We will model design strategies for a universal product family as a market economy where functional module configurations are generated through market segments based on a product platform. A coalitional game is employed to model module sharing situations regarding dynamic market environments and decides which functional modules provide more benefit when in the platform based on the marginal contribution of each module. To demonstrate implementation of the proposed method, we use a case study involving a family of mobile phones."
676,"The design of a product determines the flexibility of that product for future evolutions, which may arise from a variety of change modes such as new market needs or technological change. The energy, material, and information exchanged between components of a product along with the spatial relationships and movement between those components all influence the ability of that product’s design to be evolved to meet the new requirements of a future generation. Previous work has produced a set of guidelines for product flexibility for future evolution that have been shown to improve the ability of a design to be adapted when new needs arise. Although these guidelines are conceptually easy to understand, it is difficult to assess the extent to which a product follows the guidelines. This paper presents a systematic method to analyze the flexibility for future evolution of products based on selected guidelines. The High-Definition Design Structure Matrix is presented as a product representation model which captures sufficient interaction information to highlight potential design improvements based on the aforementioned guidelines. An interaction basis is used to facilitate the consistency and comparison of HD-DSM models created by different examiners and/or for different systems. The selected guidelines are interpreted in terms of the HD-DSM by creating analysis processes that relate to the characteristics described by the guideline. Two similar power screwdrivers are compared for flexibility for future evolution based on a quantitative analysis of their respective HD-DSMs."
677,"In this paper, an innovative method is presented which uses the properties of atomic theory to solve design modularization problems for product design. With the developed method, products can be modularized based upon different given constraints, e.g., material compatibility, part recyclability, and part disassemblability. The developed method can help engineers effectively create modular designs in the initial design stage, based upon different design requirements. With design considerations incorporated into new modules, a new design can be created which improves upon an original design, with respect to design requirements."
678,"Current market place is highly competitive and frequently changing, to survive companies need to quickly respond to the customers’ requirements. This challenging situation demands a robust platform design and development process to produce variety of products in the shortest possible time. The common components for a set of similar products under a family can be grouped into a common platform. Development of product platform requires measuring the similarity among a set of products. This paper presents an approach to measure the similarity among a set of CAD models of products to develop a common product platform. The measured similarity of geometries can allow designers to identify components that have the potential to be included in the common platform. The degree of similarity is determined by extracting the information and developing a suitable commonality index for a set of CAD models. The commonality index values are then used to determine the common platform for a set of assembly products by developing and calculating the Average Assembly Platform index value. The overall approach is followed by two case studies: Cell Phone casing models and Vacuum Cleaner models."
679,"Product portfolios need to present the widest coverage of user requirements with minimal product diversity. User requirements may vary along multiple performance measures, comprising the objective space, whereas the design variables constitute the design space, which is usually far higher in dimensionality. Here we consider the set of possible performances of interest to the user, and use multi-objective optimization to identify the non-domination or the pareto-front. The designs lying along this front are mapped to the design space; we show that these “good designs” are often restricted to a much lower-dimensional manifold, resulting in significant conceptual and computational efficiency. These non-dominated designs are then clustered in the design space in an unsupervised manner to obtain candidate product groupings which the designer may inspect to arrive at portfolio decisions. With help of dimensionality reduction techniques, we show how these clusters in low-dimensional manifolds embedded in the high-dimensional design space. We demonstrate this process on two different designs (springs and electric motors), involving both continuous and discrete design variables."
680,"In distributed design individual designers have local control over design variables and seek to minimize their own individual objectives. The amount of time required to reach equilibrium solutions in decentralized design can vary based on the design process architecture chosen. There are two primary design process architectures, sequential and parallel, and a number of possible combinations of these architectures. In this paper a game theoretic approach is developed to determine the time required for a parallel and sequential architecture to converge to a solution for a two designer case. The equations derived solve for the time required to converge to a solution in closed form without any objective function evaluations. This result is validated by analyzing a distributed design case study. In this study the equations accurately predict the convergence time for a sequential and parallel architecture. A second validation is performed by analyzing a large number of randomly generated two designer systems. The approach in this case successfully predicts convergence within 3 iterations for nearly 98% of the systems analyzed. The remaining 2% highlight one of the approach’s weaknesses; it is susceptible to numerically ill conditioned problems. Understanding the rate at which distributed design problems converge is of key importance when determining design architectures. This work begins the investigation with a two designer case and lays the groundwork to expand to larger design systems with multiple design variables."
681,"One of the critical situations facing the society across the globe is the problem of elderly homecare services (EHS) due to the aggravation of the society coupled with diseases and limited social resources. This problem has been typically dealt with by manual assistance from caregivers and/or family members. The emerging Ambience Intelligence (AmI) technology suggests itself to be of great potential for EHS applications, owing to its strength in constructing a pervasive computing environment that is sensitive and responsive to the presence of human users. The key challenge of AmI implementation lies in context awareness, namely how to align with the specific decision making scenarios of particular EHS applications. This paper proposes a context-aware information model in a smart home to tackle the EHS problem. Mainly, rough set theory is applied to construct user activity models for recognizing various activities of daily living (ADLs) based on the sensor platform constructed in a smart home environment. Subsequently, issues of case comprehension and homecare services are also discussed. A case study in the smart home environment is presented. Initial findings from the case study suggest the importance of the research problem, as well as the feasibility and potential of the proposed framework."
682,"Products are often paired with additional services to satisfy customers’ needs, differentiate product offerings, and remain competitive in today’s market. This research is motivated by the need to provide guidelines and methods to support the design of such services, addressing the lack of knowledge on customized service design as well as methods for designing and evaluating services for mass customization. We extend concepts from module-based product family design to create a method for designing families of services. In particular, we introduce a strategic platform design method for developing customized families of services using game theory to model situations involving dynamic market environments. A module-based service model is proposed to facilitate customized service design and represent the relationships between functions and processes that constitute a service offering. A module selection problem for platform design is considered as a strategic module sharing problem under collaboration, and we use a coalitional game to model module sharing and decide which modules provide more benefit when in the platform based on marginal contribution of each module. To demonstrate implementation of the proposed method, we use a case study involving a family of banking services."
683,"One goal of Designing for Human Variability (DfHV) is to optimize the interaction between user and device. Often, this interaction is dictated by the spatial dimensions or shape of the artifacts with which people interact. A novel approach that applies DfHV principles including virtual fitting trials to optimize the shape of an artifact is presented and applied to the design of a tool handle. By breaking the problem apart into discrete blocks, called the hand model and tool model, application of standard optimization techniques is facilitated. The benefits of the approach include the ability to consider handles with variable cross-sections and to systematically consider the effects of multiple sizes. The methodology presented here is configurable for any given population and may be applied to other DfHV design problems."
684,"Service design has been generally discussed in the engineering field in recent years. Many manufacturers have been focusing more on services than on products themselves. To ensure the feasibility of designed services, a service designer should consider not only customer values but also the requirements of a service provider. However, there are few standard methods to deal with the service provider’s requirements and to reflect them in the service design. In this research, the authors suggest a method to describe service provider’s requirements for the service design based on the Service Engineering methodology. In addition, the authors propose a design process to analyze service providers’ requirements and adjust the specifications of a designed service in order to fulfill the requirements of both service providers and service receivers simultaneously."
685,"In this work we extend a filter-based sequential quadratic programming (SQP) algorithm to solve reliability-based design optimization (RBDO) problems with highly nonlinear constraints. This filter-based SQP uses the approach of average importance sampling (AAIS) in calculating the values and the gradients of probabilistic constraints. AAIS allocates samples at the limit state boundaries such that relatively few samples are required in calculating constraint probability values to achieve high accuracy and low variance. The accuracy of probabilistic constraint gradients using AAIS is improved by a sample filter to eliminate sample outliers that have low probability of occurrence and high gradient values. To ensure convergence, this algorithm replaces the penalty function by an iteration filter to avoid the ill-conditioning problems of the penalty parameters in the acceptance of a design update. A sample-reuse mechanism is introduced to improve the efficiency of the algorithm by avoiding redundant samples. ‘Unsampled’ region, the region that is not covered by previous samples, is identified by the iteration step lengths, the trust region, and constraint reliability levels. As a result, this filter-based sampling SQP can efficiently handle highly nonlinear probabilistic constraints with multiple most probable points or functions without analytical forms. Several examples are demonstrated and compared with FORM/SORM and Monte Carlo simulation. Results show that by integrating the modified AAIS with the filter-based SQP, overall computation cost can be significantly improved in solving RBDO problems."
686,"An important part in the efficient and robust design of turbine blades is to capture the details of any manufacturing uncertainty. However, the data available detailing the manufacturing uncertainty inevitably contains variability due to inherent errors in any measurement process. The presented work proposes a methodology that employs existing probabilistic data analysis techniques, namely, Principal Component Analysis (PCA), Multivariate Analysis of Variance (MANOVA) and Fast Fourier Transform (FFT) analysis for separation of the measurement error from measurement data to obtain the underlying manufacturing uncertainty. This manufacturing uncertainty is further segregated in terms of the manufacturing uncertainty with time and the blade to blade manufacturing error. A method for dimensionality reduction is employed which utilizes prior information available on the variance of the measurement error for each measurement location. The application of the proposed methodology leads to reconstruction of new datasets that may be used for generating 3-d models of the manufactured blade shapes. These 3-d models may then be used further for Finite Element Analysis (FEA) in standard FEA tools."
687,"Velocity and acceleration analysis is an important tool for predicting the motion of mechanisms. The results, however, may be inaccurate when applied to manufactured products, due to the process variations which occur in production. Small changes in dimensions can accumulate and propagate in an assembly, which may cause significant variation in critical kinematic performance parameters. A new statistical analysis tool is presented for predicting the effects of variation on mechanism kinematic performance. It is based on the Direct Linearization Method developed for static assemblies. The solution is closed form, and may be applied to 2-D, open or closed, multi-loop mechanisms, employing common kinematic joints. It is also shown how form, orientation, and position variations may be included in the analysis to analyze variations that occur in kinematic joints. Closed form solutions eliminate the need of generating a large set of random assemblies, and analyzing them one-by one, to determine the expected range of critical variables. Only two assemblies are analyzed to characterize the entire population. The first determines the performance of the mean, or average assembly, and the second estimates the range of variation about the mean. The system is computationally efficient and well suited for design iteration."
688,"In this work the robustness of residual stresses in finite element simulations with respect to deviations in mechanical parameters in castings is evaluated. Young’s modulus, the thermal expansion coefficient and the hardening are the studied parameters. A 2D finite element model of a stress lattice is used. The robustness is evaluated by comparing purely finite element based Monte Carlo simulations and Monte Carlo simulations based on linear and quadratic response surfaces. Young’s modulus, the thermal expansion coefficient and the hardening are assumed to be normal distributed with a standard deviation that is 10% of their nominal value at different temperatures. In this work an improved process window is also suggested to show the robustness graphically. By using this window it is concluded that least robustness is obtained for high hardening values in combination to deviations in Young’s modulus and the thermal expansion coefficient. It is also concluded that quadratic response surface based Monte Carlo simulations substitute finite element based Monte Carlo simulations satisfactory. Furthermore, the standard deviation of the responses are evaluated analytically by using the Gauss formula, and are compared to results from Monte Carlo simulations. The analytical solutions are accurate as long as the Gauss formula is not utilized close to a stationary point."
689,"In the engineering design community, decision making methodologies to select the “best” design from among feasible designs is one of the most critical part of the design process. As the design models become increasingly realistic, the decision making methodology becomes increasingly complex. That is, because of the realistic design models, more and more decisions are made under uncertain environments without making any unrealistic assumptions. A decision maker is usually forced to work with uncertainties of which some stochastic information is known (aleatory) or no information is known (epistemic). In this paper, we discuss both forms of uncertainties and their modeling methodologies. We also define risk as a random function of these uncertainties and propose a risk quantification technique. Existing methods to handle aleatory uncertainties are discussed and an alternative search based decision making methodology is proposed to handle epistemic uncertainties. We illustrate our decision making methodology using the side-impact crashworthiness problem presented by Gu, et.al. [1]. In addition to the aleatory uncertainties considered by these researchers, we model a couple of non-design variables as epistemic uncertainties in our decision problem. Lack of information of these epistemic uncertainties increases the complexity of the side-impact crashworthiness problem significantly. However, the proposed methodology helps to identify a robust design with respect to epistemic uncertainty."
690,"Reliability is an important engineering requirement for consistently delivering acceptable product performance through time. As time progresses, the product may fail due to time phenomena such as time-dependent operating conditions, component degradation, etc. The degradation of reliability with time may increase the lifecycle cost due to potential warranty costs, repairs and loss of market share. In design for lifecycle cost, we must account for product quality, and time-dependent reliability. Quality is a measure of our confidence that the product conforms to specifications as it leaves the factory. Reliability depends on 1) the probability that the system will perform its intended function successfully for a specified interval of time (no hard failure), and 2) on the probability that the system response will not exceed an objectionable by the customer or operator, threshold for a certain time period (no soft failure). Quality is time-independent, and reliability is time-dependent. This article presents a design methodology to determine the optimal design of time-dependent, multi-response systems, by minimizing the cost during the life of the product. The conformance of multiple responses is treated in a series-system fashion. The lifecycle cost includes a production, an inspection, and an expected variable cost. All costs depend on quality and/or reliability. The key to our approach is the calculation of the so-called system cumulative distribution function (time-dependent probability of failure). For that we use an equivalent time-invariant “composite” limit state which is accurate for monotonic or non-monotonic in time, systems. Examples highlight the calculation of the cumulative distribution function and the design methodology for lifecycle cost."
691,"For obtaining correct reliability-based optimum design, an input model needs to be accurately estimated in identification of marginal and joint distribution types and quantification of their parameters. However, in most industrial applications, only limited data on input variables is available due to expensive experimental testing costs. The input model generated from the insufficient data might be inaccurate, which will lead to incorrect optimum design. In this paper, reliability-based design optimization (RBDO) with the confidence level is proposed to offset the inaccurate estimation of the input model due to limited data by using an upper bound of confidence interval of the standard deviation. Using the upper bound of the confidence interval of the standard deviation, the confidence level of the input model can be assessed to obtain the confidence level of the output performance, i.e. a desired probability of failure, through the simulation-based design. For RBDO, the estimated input model with the associated confidence level is integrated with the most probable point (MPP)-based dimension reduction method (DRM), which improves accuracy over the first order reliability method (FORM). A mathematical example and a fatigue problem are used to illustrate how the input model with confidence level yields a reliable optimum design by comparing it with the input model obtained using the estimated parameters."
692,"Due to expensive experimental testing costs, in most industrial engineering applications, only limited statistical information is available to describe the input uncertainty model. It would be unreliable to use an estimated input uncertainty model, such as distribution types and parameters including the standard deviations for the distributions, that is obtained from insufficient data for the design optimization. Furthermore, when input variables are correlated, we would obtain non-optimum design if we use the assumption of independency for the design optimization. In this paper, two methods for problems with lack of input statistical information — possibility-based design optimization (PBDO) and reliability-based design optimization (RBDO) with confidence level on the input model — are compared using a mathematical example and Abrams roadarm of an M1A1 tank. The comparison study shows that the PBDO could provide an unreliable optimum design when the number of samples is very small and that it provides an optimum design that is too conservative when the number of samples is relatively large. Furthermore, the optimum design does not converge to the optimum design obtained using the true input distribution as the number of samples increases. On the other hand, the RBDO with confidence level on the input model provides a conservative and reliable optimum design in a stable manner, and the optimum design converges to the optimum design obtained using the true input distribution as the number of samples increases."
693,"A simulation-based, system reliability-based design optimization (RBDO) method is presented which can handle problems with multiple failure regions. The method uses a Probabilistic Re-Analysis (PRRA) approach in conjunction with a trust-region optimization approach. PRRA calculates very efficiently the system reliability of a design by performing a single Monte Carlo (MC) simulation. Although PRRA is based on MC simulation, it calculates “smooth” sensitivity derivatives, allowing therefore, the use of a gradient-based optimizer. The PRRA method is based on importance sampling. It provides accurate results, if the support (set of all values for which a function is non zero) of the sampling PDF contains the support of the joint PDF of the input random variables and, if the mass of the input joint PDF is not concentrated in a region where the sampling PDF is almost zero. A sequential, trust-region optimization approach satisfies these two requirements. The potential of the proposed method is demonstrated using the design of a vibration absorber, and the system RBDO of an internal combustion engine."
694,"A radial-contour mode disk resonator has its own advantages, less energy loss and less airflow damping, over existing counterparts such as surface acoustic wave (SAW) resonators and quartz crystal microbalance (QCM) sensors. Taking these advantages of the disk resonators, we design a biological mass sensor in this paper. One of the important challenges in the design of biological mass sensors is inherent uncertainties of MEMS fabrication processes that may strongly affect to the disk resonator performances. Parameters of main effect on the sensor performance (i.e., mass sensitivity, "
695,"Traditional RBDO requires the sensitivity for both the most probable point (MPP) search in inverse reliability analysis and design optimization. However, the sensitivity is often unavailable or difficult to compute in complex multi-physics or multidisciplinary engineering applications. Hence, the response surface method (RSM) is often used to calculate both function evaluations and sensitivity effectively. Researchers have been developing the RSM for decades, and yet are still searching for an approach with an efficient sampling method for fast convergence while meeting the accuracy criteria. This paper proposes a new adaptive sequential sampling method to be integrated with the Kriging method for RBDO. By using the bandwidth of the prediction interval from the Kriging method, a new sampling strategy and a new local response surface accuracy criteria are proposed. In this sequential sampling method, the response surface is initiated using very few samples. An additional sampling point will then be determined by finding the point that has the largest absolute ratio between the bandwidth of the prediction interval and the predicted response within a neighboring area of current point of interest. The insertion of additional sampling will continue until the accuracy criterion of the response surface in the neighborhood of the current point of interest is achieved. Case studies show this proposed adaptive sequential sampling technique yields better result in terms of convergence speed compared with other sampling methods, such as the Latin hypercube sampling and the grid sampling, when the same sample size is used. Both a highly nonlinear mathematical example and a vehicle durability engineering example show that the proposed RSM yields accurate RBDO results that are comparable to the sensitivity-based RBDO results, as well as significant savings in computational time for function evaluation and sensitivity computation."
696,"Reliability-Based Design Optimization (RBDO) is an effective method to handle an optimization problem constrained by reliability performance. In spite of its great benefits, one of the most challenging issues for implementing RBDO is associated with very intensive computational demands of Reliability Analysis (RA). Moreover, an accurate and efficient RA method is indispensible to apply RBDO to practical engineering design problems. Among various RA methods, an enhanced Dimension Reduction (eDR) method is the most popular one due to the high computational efficiency. It is very desirable to obtain an accurate and efficient RA result by using the minimum number of sampling points. But, it is difficult to determine it. That is because it depends on the nonlinearity of a constraint from approximating a model and the degree of uncertainty from integrating a design factor. In this research, eDR method with variable sampling points has been studied and proposed to resolve the early mentioned difficulties. The main idea of the suggested method is to employ a different number of axial sampling points for each random design factor. It is according to the nonlinearity of a constraint and the degree of uncertainty of each random design factor. For each random variable, it begins to use three points first and decides to stop or increase the axial sampling points based upon the proposed criteria in this study. In case of increasing sampling points, it is incremented by one sampling point and ended up five sampling points at most. As it shown in the result, the efficiency of eDR method with variable sampling points for each random variable is superior to the one with fixed sampling points without sacrificing any accuracy. Through the three representative RA problems, it is verified that the proposed RA method generates the result 26.5% more efficiently on average than the conventional eDR method with fixed sampling points. Furthermore, the Performance Measure Approach (PMA) was used to evaluate the performance of RBDO using the new RA method. For the comparison, three mathematical and one engineering RBDO problems were solved by both eDR method with variable sampling points and conventional one with fixed sampling points. Finally, the comparison results clearly demonstrate that RBDO using the suggested RA method is superior to the conventional one in terms of accuracy and efficiency."
697,"Many real-world engineering design optimization problems are multi-objective and have uncertainty in their parameters. For such problems it is useful to obtain design solutions that are both multi-objectively optimum and robust. A robust design is one whose objective and constraint function variations under uncertainty are within an acceptable range. While the literature reports on many techniques in robust optimization for single objective optimization problems, very few papers report on methods in robust optimization for multi-objective optimization problems. The Multi-Objective Robust Optimization (MORO) technique with interval uncertainty proposed in this paper is a significant improvement, with respect to computational effort, over a previously reported MORO technique. In the proposed technique, a master problem solves a relaxed optimization problem whose feasible domain is iteratively confined by constraint cuts determined by the solutions from a sub-problem. The proposed approach and the synergy between the master problem and sub-problem are demonstrated by three examples. The results obtained show a general agreement between the solutions from the proposed MORO and the previous MORO technique. Moreover, the number of function calls for obtaining solutions from the proposed technique is an order of magnitude less than that from the previous MORO technique."
698,"Although a variety of uncertainty propagation methods exist for estimating the statistical moments and the probability of failure in design under uncertainty, current methods suffer from their limitations in providing accurate and efficient solutions to high-dimension problems with interactions of random variables. A new sparse grid based uncertainty propagation method is proposed in this work to overcome this difficulty. The existing sparse grid technique, originally invented for numerical integration and interpolation, is extended to uncertainty propagation in the probabilistic domain. In particular, the concept of Sparse Grid Numerical Integration (SGNI) is extended for estimating the first two moments of performance in robust design, while the Sparse Grid Interpolation (SGI) is employed to determine failure probability by interpolating the limit-state function at the Most Probable Point (MPP) in reliability analysis. The proposed methods are demonstrated by several high-dimension mathematical examples with notable variate interactions and one complex multidisciplinary rocket design problem. Results show that the use of sparse grid methods works better than popular counterparts. Furthermore, the automatic sampling, special interpolation process, and dimension-adaptivity feature make SGI more flexible and efficient than using the uniform sample based metamodeling techniques."
699,"Statistical sensitivity analysis (SSA) is an effective methodology to examine the impact of variations in model inputs on the variations in model outputs at either a prior or posterior design stage. A hierarchical statistical sensitivity analysis (HSSA) method has been proposed in literature to incorporate SSA in designing complex engineering systems with a hierarchical structure. However, the original HSSA method only deals with hierarchical systems with independent subsystems. Due to the existence of shared variables at lower levels, responses from lower level submodels that act as inputs to a higher level subsystem are both functionally and statistically dependent. For designing engineering systems with dependent subsystem responses, an extended hierarchical statistical sensitivity analysis (EHSSA) method is developed in this work to provide a ranking order based on the impact of lower level model inputs on the top level system performance. A top-down strategy, same as in the original HSSA method, is employed to direct SSA from the top level to lower levels. To overcome the limitation of the original HSSA method, the concept of a "
700,"As the role of predictive models has increased, the fidelity of computational results has been of great concern to engineering decision makers. Often our limited understanding of complex systems leads to building inappropriate predictive models. To address a growing concern about the fidelity of the predictive models, this paper proposes a hierarchical model validation procedure with two validation activities: (1) validation planning (top-down) and (2) validation execution (bottom-up). In the validation planning, engineers define either the physics-of-failure (PoF) mechanisms or the system performances of interest. Then, the engineering system is decomposed into subsystems or components of which computer models are partially valid in terms of PoF mechanisms or system performances of interest. Validation planning will identify vital tests and predictive models along with both known and unknown model parameter(s). The validation execution takes a bottom-up approach, improving the fidelity of the computer model at any hierarchical level using a statistical calibration technique. This technique compares the observed test results with the predicted results from the computer model. A likelihood function is used for the comparison metric. In the statistical calibration, an optimization technique is employed to maximize the likelihood function while determining the unknown model parameters. As the predictive model at a lower hierarchy level becomes valid, the valid model is fused into a model at a higher hierarchy level. The validation execution is then continued for the model at the higher hierarchy level. A cellular phone is used to demonstrate the hierarchical validation of predictive models presented in this paper."
701,"This paper presents an adaptive-sparse polynomial chaos expansion (adaptive-sparse PCE) method for performing engineering reliability analysis and design. The proposed method leverages three ideas: (i) an adaptive scheme to build sparse PCE with the minimum number of bivariate basis functions, (ii) a new projection method using dimension reduction techniques to effectively compute the expansion coefficients of system responses, and (iii) an integration of copula to handle nonlinear correlation of input random variables. The proposed method thus has three distinct features for reliability analysis and design: (a) no need of response sensitivities, (b) no extra cost to evaluate probabilistic sensitivity for design, and (c) capability to handle a nonlinear correlation. Besides, an error decomposition scheme of the proposed method is presented to help analyze error sources in probability analysis. Several engineering problems are used to demonstrate the effectiveness of the adaptive-sparse PCE method."
702,"RBDO problems have been intensively studied for many decades. Since Hasofer and Lind defined a measure of the second-moment reliability index, many RBDO methods utilizing the concept of reliability index have been introduced as the Reliability Index Approach (RIA). In the RIA, a reliability analysis problem is formulated to find the reliability index for each performance constraint and the solutions are used to evaluate the failure probability. However, the traditional RIA suffers from inefficiency and convergence problems. In this paper, we revisited the definition of the reliability index and revealed the convergence problem in the traditional RIA. Furthermore, a new definition of the reliability index is proposed to correct this problem and a modified Reliability Index Approach based on this definition is developed. Numerical examples using both the traditional RIA and the modified RIA are compared and discussed."
703,"In this paper a method for topology optimization of nonlinear elastic structures is suggested. The method is developed by starting from a total Lagrangian formulation of the system. The internal force is defined by coupling the second Piola-Kirchhoff stress to the Green-Lagrange strain via the Kirchhoff-St. Venant law. The state of equilibrium is obtained by first deriving the consistency stiffness matrix and then using Newton’s method to solve the non-linear equations. The design parametrization of the internal force is obtained by adopting the SIMP approach. The minimization of compliance for a limited value of volume is considered. The optimization problem is solved by SLP. This is done by using a nested approach where the equilibrium equation is linearized and the sensitivity of the cost function is calculated by the adjoint method. In order to avoid mesh-dependency the sensitivities are filtered by Sigmund’s filter. The final LP-problem is solved by an interior point method that is available in Matlab. The implementation is done for a general design domain in 2D by using fully integrated isoparametric elements. The implementation seems to be very efficient and robust."
704,"Structural design for crashworthiness is a challenging area of research due to large plastic deformations and complex interactions among diverse components of the vehicle. Previous research in this field primarily focused on energy absorbing structures that utilize a desired amount of material. These structures have been shown to absorb a large amount of the kinetic energy generated during the crash event; however, the large plastic strains experienced can lead to failure. This research introduces a new strain-based topology optimization algorithm for crash-worthy structures undergoing large deformations. This technique makes use of the hybrid cellular automaton framework combining transient, non-linear finite-element analysis and local control rules acting on cells. The set of all cells defines the design domain. In the proposed algorithm, the design domain is dynamically divided into two sub-domains for different objectives, i.e., high strain sub-domain (HSSD) and low strain sub-domain (LSSD). The distribution of these sub-domains is determined by a plastic strain limit value. During the design process, the material is distributed within the LSSD following a fully-internal-energy-distribution principle. To accomplish that, each cell in the LSSD is driven to a prescribed target or set point value by modifying its stiffness. In the HSSD, the material is distributed to satisfy a failure criterion given by a maximum strain value. Results show that the new formulation and algorithm are suitable for practical applications. The case studies demonstrate the potential significance of the new capability developed for a wide range of engineering design problems."
705,"This paper proposes a new level set-based topology optimization method for thermal problems that deal with generic heat transfer boundaries including design-dependent boundary conditions, based on the level set method and the concept of the phase field theory. First, a topology optimization method using a level set model incorporating a fictitious interface energy derived from the concept of the phase field theory is briefly discussed. Next, a generic optimization problem for thermal problems is formulated based on the concept of total potential energy. An optimization algorithm that uses the Finite Element Method when solving the equilibrium equation and updating the level set function is then constructed. Finally, several three-dimensional numerical examples are provided to confirm the utility and validity of the proposed topology optimization method."
706,"In the past decades, the stagnant growth of battery technology becomes the bottle-neck of new generation of portable and wearable electronics which ask for longer work time and higher power consumption. Energy harvesting device based on the direct piezoelectric effect that converts ambient mechanical energy to usable electric energy is a very attractive energy source for portable and wearable electronics. This paper discusses the design of piezoelectric energy harvesting strap buckle that can generate as much as possible electric energy from the differential forces applying on the buckle. Topology optimization method is employed to improve the efficiency of piezoelectric energy harvesting strap buckle in a limited design space. A stiffness or displacement constraint is introduced to substitute material volume constraint in this problem formulation to avoid useless optimum result with nearly zero material volume. The sensitivities of both objective function and design constraint are derived from the adjoint method. A design example of piezoelectric energy harvesting strap buckle using the proposed topology optimization method is presented and the result is discussed."
707,"A level-set-based method for robust shape and topology optimization (RSTO) is proposed in this work with consideration of uncertainties that can be represented by random variables or random fields. Uncertainty, such as those associated with loading and material, is introduced into shape and topology optimization as a new dimension in addition to space and time, and the optimal geometry is sought in this extended space. The level-set-based RSTO problem is mathematically formulated by expressing the statistical moments of a response as functionals of geometric shapes and loading/material uncertainties. Spectral methods are employed for reducing the dimensionality in uncertainty representation and the Gauss-type quadrature formulae is used for uncertainty propagation. The latter strategy also helps transform the RSTO problem into a weighted summation of a series of deterministic topology optimization subproblems. The above-mentioned techniques are seamlessly integrated with level set methods for solving RSTO problems. The method proposed in this paper is generic, which is not limited to problems with random variable uncertainties, as usually reported in other existing work, but is applicable to general RSTO problems considering uncertainties with field variabilities. This characteristic uniquely distinguishes the proposed method from other existing approaches. Preliminary 2D and 3D results show that RSTO can lead to designs with different shapes and topologies and superior robustness compared to their deterministic counterparts."
708,"This paper provides two separate methodologies for implementing the Voronoi Cell Finite Element Method (VCFEM) in topological optimization. Both exploit two characteristics of VCFEM. The first approach utilizes the property that a hole or inclusion can be placed in the element: the design variables for the topology optimization are sizes of the hole. In the second approach, we note that VCFEM may mesh the design domain as "
709,"The current paper examines the static performance of 2D infinite lattice materials with hexagonal Bravais lattice symmetry. Two novel microscopic cell topologies are proposed. The first topology is a semi-regular lattice that has the modified Schafli symbol 34 .6, which describes the type of regular polygons surrounding the joints of the lattice. Here, 34 .6 indicates four (4) regular triangles (3) successively surrounding a node followed by a regular hexagon (6). The second topology is an irregular lattice that is referred here as Double Hexagonal Triangulation (DHT). The lattice material is considered as a pin-jointed micro-truss where determinacy analysis of the material micro structure is used to distinguish between bending dominated and stretching dominated behaviours. The finite structural performance of unit cells of the proposed topologies is assessed by the matrix methods of linear algebra. The Dummy Node Hypothesis is developed to generalize the analysis to tackle any lattice topology. Collapse mechanisms and states of self-stress are deduced from the four fundamental subspaces of the kinematic and the equilibrium matrices of the finite unit cell structures, respectively. The generated finite structural matrices are employed to analyze the infinite structural performance of the lattice using the Bloch’s theorem. To find macroscopic strain fields generated by periodic mechanisms, the Cauchy-Born hypothesis is adopted. An explicit expression of the microscopic cell element deformations in terms of the macroscopic strain field is generated which is employed to derive the strain energy density of the lattice material. Finally, the strain energy density is used to derive the material macroscopic stiffness properties. The results showed that the proposed lattice topologies can support all macroscopic strain fields. Their stiffness properties are compared with those of lattice materials with hexagonal Bravais symmetry available in literature. The comparison showed that the lattice material with 34 .6 cell topology has superior isotropic stiffness properties. When compared with the Kagome’ lattice, the 34 .6 lattice generates isotropic stiffness properties, with additional stiffness to mass ratio of 18.5% and 93.2% in the direct and the coupled direct stiffness, respectively. However, it generates reduced shear stiffness to mass ratio by 18.8%."
710,"In recent years, the interest of small and medium sized enterprises towards Virtual Reality (VR) systems is strongly increased thanks both to the improvement of VR tools effectiveness and to the cost reduction of technologies implementation. Due to the growing number of installed systems, many SMEs (Small Manufacturing Enterprises) companies require robust methods for evaluating technology performance. In this context, the present paper presents a metrics-based approach in order to analyze the VR system performance. It is specifically dedicated to the design review process during styling product design. The evaluation parameters are related to the effective communication and preservation of design intent. Metrics are classified in two main classes. The first one is related to the product, the process and the characteristics of VR technology. The second one is related to the design intent meanings preservation along the design process. Two experimental case studies are reported in order to test the approach in different operative fields."
711,"Knowledge discovery in multi-dimensional data is a challenging problem in engineering design. For example, in trade space exploration of large design data sets, designers need to select a subset of data of interest and examine data from different data dimensions and within data clusters at different granularities. This exploration is a process that demands both humans, who can heuristically decide what data to explore and how best to explore it, and computers, which can quickly identify features that may be of interest in the data. Thus, to support this process of knowledge discovery, we need tools that go beyond traditional computer-oriented optimization approaches to support advanced designer-centered trade space exploration and data interaction. This paper is an effort to address this need. In particular, we propose the Interactive Multi-Scale Nested Clustering and Aggregation (iMSNCA) framework to support trade space exploration of multi-dimensional data common to design optimization. A system prototype of this framework is implemented to allow users to visually examine large design data sets through interactive data clustering, aggregation, and visualization. The paper also presents a case study involving morphing wing design using this prototype system. By using visual tools during trade space exploration, this research suggests a new approach to support knowledge discovery in engineering design by assisting diverse user tasks, by externalizing important characteristics of data sets, and by facilitating complex user interactions with data."
712,"The objective of this research is to develop an immersive interface and a design algorithm to facilitate the synthesis of compliant mechanisms from a user-centered design perspective. Compliant mechanisms are mechanical devices which produce motion or force through deflection or flexibility of their parts. Using the constraint-based method of design, the design process relies on the designer to identify the appropriate constraint sets to match the desired motion. Currently this approach requires considerable prior knowledge of how non-linear flexible members produce motion. As a result, the design process is based primarily on the designer’s previous experience and intuition. A user-centered methodology is suggested where the interface guides the designer throughout the design process, thus reducing the reliance on intuitive knowledge. This methodology supports constraint-based design methods by linking mathematical models to support compliant mechanism design in an immersive virtual environment. A virtual reality (VR) immersive interface enables the designer to input the intended motion path by simply grabbing and moving the object and letting the system decide which constraint spaces apply. The user-centered paradigm supports an approach that focuses on the designer defining the motion and the system generating the constraint sets, instead of the current method which relies heavily on the designer’s intuition to identify appropriate constraints. The result is an intelligent design framework that will allow a broader group of engineers to design complex compliant mechanisms, giving them new options to draw upon when searching for design solutions to critical problems."
713,"Thanks to recent advances in computing power and speed, designers can now generate a wealth of data on demand to support engineering design decision-making. Unfortunately, while the ability to generate and store new data continues to grow, methods and tools to support multi-dimensional data exploration have evolved at a much slower pace. Moreover, current methods and tools are often ill-equipped at accommodating evolving knowledge sources and expert-driven exploration that is being enabled by computational thinking. In this paper, we discuss ongoing research that seeks to transform decades-old decision-making paradigms rooted in operations research by considering how to effectively convert data into knowledge that enhances decision-making and leads to better designs. Specifically, we address decision-making within the area of trade space exploration by conducting human-computer interaction studies using multi-dimensional data visualization software that we have been developing. We first discuss a Pilot Study that was conducted to gain insight into expected differences between novice and expert decision-makers using a small test group. We then present the results of two Preliminary Experiments designed to gain insight into procedural differences in how novices and experts use multi-dimensional data visualization and exploration tools and to measure their ability to use these tools effectively when solving an engineering design problem. This work supports our goal of developing training protocols that support efficient and effective trade space exploration."
714,"In this paper, a haptic modeling and simulation system is developed to assist handheld product design. With haptic feedback, users could create, interact and evaluate the virtual product directly and intuitively without producing the physical prototype. This saves the cost and reduces time-to-market, which is especially meaningful for the rapidly changing handheld mobile devices. To provide a comfortable and accurate operation, a virtual vibration actuator is devised to add into the touch screen. Unlike the previous research that mainly focuses on the design of the product shape, the proposed system also models the interaction between the user (finger) or tool (pen) and handheld device (button/screen). To obtain realistic simulation and replace the physical prototype, the complex shape and deformation of the finger are considered when calculating the feedback force. A computational efficient collision detection method for complex shape objects is proposed to tackle the challenge of a high update rate of more than 1 kHz for real-time realistic haptic rendering. Moreover, the proposed system incorporates the haptic modeling of vibration interaction and menu interface design into the product design simulation system. A case study of handheld device design is used to illustrate the proposed system."
715,"In new product development, quickly generating many concepts that a potential consumer prefers is a challenge. This paper presents the inaugural application of software agents implementing a shape grammar to generate product designs according to a utility function that represents consumer preference. The method is composed of three sub-processes: a shape grammar interpreter, an agent interpreter, and a utility investigator. These work together to explore the design space and can constrain product form designs according to a utility function that represents consumer design preference."
716,"An Interactive Genetic Algorithm system is proposed for designing a car silhouette while involving the style designer in the evaluation process of a population of individuals. This IGA is based on the principle of an indirect encoding of a closed curve genotype using a primary Fourier decomposition. A crossing over operator is proposed for mixing the parents’ genes by a random weighted average into a new child’s genotype. A perceived similarity index between two genotypes is built to check that our IGA is able to converge toward a target individual starting from the genes of an initial population."
717,"Generative design and fabrication refers to the ability to autonomously generate designs while simultaneously generating all information to directly fabricate them. This technique is driven by the increasing need to rapidly and flexibly fabricate customized parts and individually designed products. For the automation of the design-to-fabrication process chain, intensive and dynamically updated knowledge from the domains of design and fabrication must be provided. To allow for a flexible, autonomous fabrication, the knowledge modeled must dynamically reflect the state of the fabrication system and its capabilities. This paper presents an approach to unify knowledge for generative design and generative fabrication using shape grammars. With shape grammars, the geometry of designs and their mapping to removal volumes corresponding to fabrication processes on CNC machine tools are represented. The process instructions for fabrication are included by augmenting the removal volume shapes with labels. A new shape grammar approach to represent designs and fabrication processes is presented and validated on an example functional part as a proof-of-concept. The approach enables pushing knowledge downstream, from design and process planning directly to the fabrication system itself providing a stepping stone towards awareness of machine capabilities in fabrication systems and autonomous process planning for customized parts."
718,"This paper describes the development and validation of analytical and numerical procedures for the simulation of temperature and cure profiles for the pultrusion process. The governing equations for heat transfer and the resin cure reaction during pultrusion are presented and finite-difference procedure is developed to solve the governing equations. Then analytical and numerical procedures are integrated into the ACES (Automated Concurrent Engineering System) software environment. It is shown that the procedures are numerically stable and predict the temperature and cure profiles, which are in good agreement with those published by other researchers. This analysis leads to the prediction of the temperature and cure distribution along the length and through the thickness of the composite. The prediction of the temperature and cure distribution enables proper selection of process variables, which affects the production rate and quality of the parts."
719,"Managing design knowledge is an important concern for industry, including engineering. Engineering firms are facing pressures to increase the quality of their products, to have even shorter lead times and reduced costs. There is also a trend towards globalization resulting in complex supply chains and the need to manage teams that are not necessarily co-located. Design knowledge needs to be exchanged and accessed efficiently. Other motivations for managing design knowledge are to provide a trail for product liability legislation and to retain design knowledge and experience as engineering designers retire. Fuzzy Cognitive Map (FCM) is one of the main formalisms for modeling, representing and reasoning about causal knowledge. Despite the fact that FCM has been used extensively in causal knowledge engineering, there is a lack of methodology for the systematic construction of FCM. Although some techniques were used in the individual construction processes, these techniques were either not systematically documented or too specific to the problem at hand. FCM and Bayesian Belief Network (BBN) are two major frameworks for modeling, representing and reasoning about causal design knowledge. Despite their extensive use in causal design knowledge engineering, there is no reported work which compares their respective roles. This paper deals with three topics, which are systematic constructing FCM, a methodology for FCM-BBN conversion, and comparison FCM and BBN. BBN has a sound mathematical foundation and reasoning capabilities, also it has an efficient evidence propagation mechanism and a proven track record in industryscale applications. However, BBN is less friendly and flexible, and often very time-consuming to generate appropriate conditional probabilities. Thus, Fuzzy Cognitive Map (FCM) is used for the indirect knowledge acquisition, and the causal knowledge in FCM is systematically converted to BBN. Finally, we compare BBNs directly generated by domain experts and generated from FCM, with a realistic industrial example, a fuel nozzle for an aerospace engine."
720,"While the modern product development requires more knowledge-intensive and collaborative environment, the capture, retrieval, accessibility, and reusability of that design knowledge are increasing critical. In this paper, a rough set theory generates demanded rules and selects the appropriate minimal rules among the demanded design rules associated to the assembly design knowledge. The design rules are infrequently captured and often ignored due to its complexity. Rough set theory synthesizes approximation of concepts, analyzes data by discovering patterns, and classifies into certain decision classes. Such patterns can be extracted from data by means of methods based on Boolean reasoning and discernibility. This paper shows the feasibility of rough-set based rule selection considering complex design data objects in order to obtain efficient assembly design decision."
721,"The design of a complex mechanical product is usually a top-down process carried out by different teams or designers that are geographically distributed. A systematical variation propagation mechanism is very important to fully support such a design process. In this paper, based on the framework for collaborative top-down assembly design previously proposed by the authors, an agent based approach is presented for addressing variation propagation for collaborative top-down assembly design. The approach achieves variation propagation during the collaborative top-down assembly design through the interaction and cooperation of the agents located at the clients and server. To make the variation propagation automated and intelligent, four kinds of variation reasoning including hierarchical variation reasoning, engineering constraint variation reasoning, feature variation reasoning, and assembly constraint variation reasoning are identified, and the corresponding algorithms are developed and utilized. Meanwhile, a distributed assembly model is put forward to effectively support the design variation propagation for the collaborative top-down assembly design. The approach is implemented and a variation propagation example is given."
722,"Assembling a product is a delicate process at the borderline between design and manufacturing. Shortening the time between them plays a central role both for quality assurance and fast time to market. In this paper, we describe an automatic tool based on a new method, integrating assembly design, sequence optimization, and advanced rigid body path planning. First, we introduce a greedy algorithm for assembling a product, part by part, based on state-of-the-art path planning. We exploit all the six degrees of freedom of a rigid body to search for collision-free paths, instead of limited motions. Then, we use assembly design in order to limit the search for an optimal assembling sequence and to guarantee geometrical quality among the sequences examined. Disassembly path planning is used here to further cut the state space and to give a quality measure to the sequences. Eventually, we present results for an industrial test case, which has been successfully solved by applying our method."
723,"Foremost step in the development of any electromechanical product is its design, and conceptual design is the most ambiguous and creative phase of design. There exist only a few computational tools that aid designers at conceptual design stage, and mostly designers rely on personal experience or experience of co-workers to generate quality designs. The proposed framework aims at generating robust computerized conceptual designs by incorporating Modularity, Design for Assembly (DFA) and Design for Variety (DFV) principles at the conceptual stage. Conceptual design alternatives obtained from the proposed framework are ranked based on minimum assembly time, and are composed of modules in a way that future changes in customer needs are satisfied only by replacing certain modules. The framework involves searching a design repository of components by using functional-basis and pre-defined graph grammar rules, to generate all possible conceptual design alternatives. These design alternatives are ranked and filtered using a DFA index, and top two alternatives are selected. Selected designs are modularized and filtered using a DFV index to obtain the best design alternative. This paper provides a detailed discussion of the proposed framework, and its working is illustrated through the design of a mounting system for holding a Variable Message Sign (VMS)."
724,"In this paper, we give a review of recent transition path search methods for nanoscale phase transition simulation A potential energy surface (PES) characterizes detailed information about phase transitions where the transition path is related to a minimum energy path on the PES. The minimum energy path connects reactant to product via saddle point(s) on the PES. Once the minimum energy path is generated, the activation energy required for transitions can be determined. Using transition state theory, one can estimate the rate constant of the transition. The rate constant is critical to accurately simulate the transition process with sampling algorithms such as kinetic Monte Carlo."
725,"A review of saddle point search methods on a potential energy surface is presented in this paper. Finding saddle points on a complex potential energy surface is the major challenge in modeling and simulating the kinetics of first-order phase transitions. Once the saddle points have been identified and the activation energy for the transition is known, one can apply the kinetic Monte Carlo method to simulate the transition process. We consider some factors while reviewing the methods, such as whether the solution is global, the knowledge of the Hessian during the search, the capability to locate multiple saddle points and higher order saddle points, the kind of approximations used for potential energy surface, if any; and the convergence of the methods."
726,"A multiscale design approach is proposed in this paper considering the impacts of product manufacturing process and material on product performance. A framework is established to integrate designs of manufacturing process, material and product based on the information flow across these three domains. Random field is employed to realistically model the uncertainty existing in material microstructure which spatially varies in a product inherited from the manufacturing process. An efficient procedure for uncertainty propagation from the material random field to the end product performance is established. To reduce the dimensionality of random field representation, a reduced order Karhunen-Loeve expansion is used with a discretization scheme applied to finite element meshes. The univariate dimension reduction method and the Gaussian quadrature formula are used to efficiently quantify the uncertainties in product performance in terms of its statistical moments, which are critical information for design under uncertainty. A control arm example is used to demonstrate the proposed approach. The impact of the initial microscale porosity random field produced during a casting process on the product damage is studied and a reliability-based design of the control arm is performed."
727,"Providing nano engineers and scientists efficient and easy-to-use tools to create geometry conformations that have minimum energies is highly desirable in material design. Recently we developed a periodic surface model to assist the construction of nano structures parametrically for computeraided nano-design. In this paper, we present a feature-based approach for crystal construction. The proposed approach starts to create models of basic features by the aide of periodic surfaces followed by operations between basic features. The goal is to introduce a rapid construction method for complex crystal structures."
728,"This paper describes an approach to automate the design for sheet metal parts that are not only novel and manufacturable but also satisfies multiple objective functions such as material cost and manufacturability. Unlike commercial software tools such as Pro/SHEETMETAL which aids the user in finalizing and determining the sequence of manufacturing operations for a specified component, our approach starts with spatial constraints in order to create the component geometries and helps the designer design. While there is an enormous set of parts that can feasibly be generated with sheet metal, it is difficult to define this space systematically. To solve this problem, we currently have 88 design rules that have been developed for four basic sheet metal operations: slitting, notching, shearing, and bending. A recipe of the operations for a final optimal design is then presented to the manufacturing engineers thus saving them time and cost. The technique revealed in this paper represents candidate solutions as a graph of nodes and arcs where each node is a rectangular patch of sheet metal, and modifications are progressively made to the sheet to maintain the parts manufacturability. They are presented in the form of Standard Tessellation Language files (.stl) that can be transferred into available modeling software for further analysis. The overall purpose of this research is to provide creative designs to the designer granting him/her a new perspective and to check all the solutions for manufacturability in the early stage of design process. An example sheet metal design problem is shown in this paper with some of the preliminary designs that our approach created."
729,"A new approach for automated conceptual design of hydraulic system is developed in this paper based on the energy characteristic state model. The energy characteristics of hydraulic systems are firstly represented by qualitative vectors, and the integral hydraulic systems are visualized as a set of subsystems with single-action for synthesis. Then, two categories of basic transformation units are defined from general hydraulic components, and the qualitative matrix representations for these basic transformation units are also established according to the function analysis. Finally, the conceptual design model for hydraulic system is established, in which the design processes of single-action subsystems are transformed into the decomposition of system-level matrices and the matching of component-level matrices. A design example is also given to illustrate the proposed approach."
730,"Tolerancing is an essential part of a product design process. It is traditionally performed after the product structure has been determined. The product precision requirements may not be fully utilized for the decision making for selecting the best structure in the early stage of product design — the conceptual design phase. A growth design method has been proposed for product structure design and concurrent tolerancing, in which the product form and tolerance can be generated in the same way as an organism grows. The basic feature of biology, cell division, has been borrowed as the design guiding principle. Product conceptual model has been built as the basis of form and tolerance growth design. The relationship of product function, structure and tolerance was analysed to explain on what and how they act in product design process. Tolerance mathematic model expressed by screw parameters and expression model based on "
731,"Ontology has been known as an important means to represent design knowledge in product development, however, most ontology creation has not yet been systematically carried out. Port, as the location of intended interaction between a component and its enviornment, plays an important role in product conceptual design. It constitutes the interface of a component and defines its boundary. This paper introduces an approach, it is convenient to abstractly represent the intended exchange of signals, energy and/or material, and creat and manage port-based domain ontology, to port-based ontology modeling (PBOM) for product conceptual design. In this paper, port concept and port functional description through using natural language are first presented and their semantic synthesis is used to describe port ontology. Secondly, an ontology repository which contains the assorted primitive concepts and primitive knowledge to map the component connections and interactions is built. Meanwhile a model of port-based multi-views which contains functional view, behavior view and configuration view is articulated, and the attributes and taxonomy of ports in a hierarchy are presented. Next, a port-based ontology language (PBOL) is described to represent the process of port ontology refinement, and a port-based FBS modeling framework is constructed to describe system configuration. Furthermore, a formal knowledge framework to manage comprehensive knowledge is proposed, which could help designers create, edit, organize, represent and visualize product knowledge. Finally, a revised tape case is employed to validate the efficiency of the port ontology for product conceptual design and illustrate its application."
732,"Modern design methodologies have used Function Component Matrices in a variety of different ways in order to support various facets of an engineering design process. The mapping of functions to components can be used to model and capture the dependencies and relationships that exist. This process is accomplished by breaking down complicated functions into smaller, easier to understand functions. This decomposition allows engineers to get a better understanding for how a change in each component within a product will affect the overall operation of the product. Being able to recognize the impact of the propagation of a sub-function change will give designers a better understanding of the flexibility (or lack thereof) of choices they have when designing a product for customization. In turn they can be used to inform the consumer regarding the consequences their customization choices can have on the final product. This paper discusses how a Functional Component Matrix (FCM) can be used to assist in this process of product customization and understanding change propagation."
733,"Previous work developed a knowledge-based method named Design GuideLines Collaborative Framework (DGLs-CF), adopted during product redesign and process reconfiguration and aimed at dealing and matching with particular manufacturing and verification technologies, according to ISO GPS concepts. Aim of present work is improving the role of the ISO GPS within the DGLs-CF, not only to raise coherence in terms of concepts, but to gain some important internal and external improvements for the DGLs-CF. The main activity toward achieving this goal is the formalization of DGLs-CF product features by means of the ISO GPS features. The procedure is proposed and the consequences of its application on the elements of the DGLs-CF are evaluated. A case of study as an example of application is also presented. A significant improvement in the DGLs-CF is realized, both in terms of knowledge structure and organization, and in terms of the possibility of interfacing it with other methodologies, tools, and environments."
734,"To minimize the coordination efforts among design teams and expedite the design process via parallel workflows, a cooperative and decentralized environment is often considered for team-based design. The "
735,"Planning of an upstream design process that includes creative and concurrent activities has become more important for product development in a competitive market. A significant characteristic of upstream planning is that the design process is one of knowledge creation. During this process, a designer makes progress toward a more advanced knowledge level that corresponds to a more advanced design achievement. In most cases of a creative and concurrent design process, however, a designer has to compromise design achievement because of constraints such as delivery time, cost, and another designer’s intention. Therefore, in planning a design process, it is more essential to set an acceptable level of design achievement and to predict whether or not a planned design process can ensure this level, than to predict the design time needed to totally achieve the design goals. This paper proposes a new method of design process planning that focuses on quantitative prediction of knowledge level achieved in a creative and concurrent design process. A growth curve model using fuzzy numbers is introduced to predict the final achievement of each task and final achievement of consistency between tasks after running a planned design process. The reliability model of a serial system is used to calculate the total acceptability of the design achievement. An experimental system that supports design process planning based on the proposed method is developed. This paper demonstrates its application to a student design project in order to show the power of the method."
736,"There has been a recent emergence of communities working together in large numbers to develop new products, services, and systems. Collaboration at such scales, referred to as "
737,"This paper introduces a taxonomy of physical prototypes, benchmarking it with existing classifications to elucidate the advantages and disadvantages. The proffered taxonomy is evaluated by three approaches: (i) checking the orthogonality of the individual elements of the taxonomy, (ii) benchmarking the proposed taxonomy with existing classifications of prototypes, and (iii) demonstrating its utility by applying it to classify different prototypes. The results show the proposed taxonomy is capable of distinguishing prototypes with greater precision than what is possible with current classifications. Further, the proposed classification is generally orthogonal, allowing for more consistent delineation between prototypes. However, the findings also reveal that some of the taxa are interdependent; exploring these interdependencies to develop a deeper understanding of how prototyping influences design thinking is the focus of future research."
738,"So far manufacturing tolerance variability over samples has been widely considered in many engineering design problems. Traditionally the tolerance variability is modeled as a spatially independent random parameter although the variability is a function of spatial variables ("
739,"A primary concern in product design is ensuring high system reliability amidst various uncertainties throughout a product life-cycle. To achieve high reliability, uncertainty data for complex product systems must be adequately collected, analyzed, and managed throughout the product life-cycle. However, despite years of research, system reliability assessment is still difficult, mainly due to the challenges of evolving, insufficient, and subjective data sets. Therefore, the objective of this research is to establish a new paradigm of reliability prediction that enables the use of evolving, insufficient, and subjective data sets (from expert knowledge, customer survey, system inspection & testing, and field data) over the entire product life-cycle. This research will integrate probability encoding methods to a Bayesian updating mechanism. It is referred to as Bayesian Information Toolkit (BIT). Likewise, Bayesian Reliability Toolkit (BRT) will be created by incorporating reliability analysis to the Bayesian updating mechanism. In this research, both BIT and BRT will be integrated to predict reliability even with evolving, insufficient, and subjective data sets. It is shown that the proposed Bayesian reliability analysis can predict the reliability of door closing performance in a vehicle body-door subsystem where the relevant data sets availability are limited, subjective, and evolving."
740,"Mistakes in the design process have been recognized as a major source of product quality loss. There are several methods currently used to identify and quantify these mistakes. However, these methods typically do not provide a useful context within which to quantitatively incorporate mistakes into the design process in a beneficial way. This paper presents an approach to determine when it is appropriate to perform error checking to eliminate a potential mistake. The proposed approach is intended to be used when time is a limited design resource and design goals are technically attainable. It is proposed that the cost of a mistake can be quantified as the amount of time a mistake adds or subtracts from the overall time required to achieve the design’s objectives. To determine this, an optimization problem is formulated which minimizes time spent in the design process. In this optimization problem the design variables are the binary choice whether or not to perform an error check. The approach is demonstrated in two case studies, one a simple theoretical design problem and the other the design of an I-beam. The results of these case studies demonstrate the approach’s effectiveness, and present several avenues for future work."
741,"A model structure for the corrosion-fatigue degradation phenomenon has been proposed for oil pipelines. At first, a well acknowledged model for corrosion-fatigue, such as Wei’s “Super Position Model”, was pursued as a reference. By reiterating the reference model using generic data from literature and applying Monte Carlo simulation, the simplest possible structure for the model was identified. The correlation of the proposed model with the environmental effects, such as loading stress and frequency, surrounding the pipelines were estimated. The sources of random variability emerging from many sources have been reasonably embodied into only two random variables. Yet again, the scarce of field and experimental data for this particular critical degradation phenomenon has compelled the research study to rely profoundly on generic data from open literature. Hence, the best distribution estimates for the two random variables were computed. Subsequent to number of iterations, the proposed model was modified to rather simpler form. All the proposed model forms have been cross checked against the original reference model which resulted in a satisfying agreement."
743,"Design Structure Matrices (DSM) and Domain Mapping Matrices (DMM) are commonly used to model and analyze the relationships within one domain (DSM) or between two domains (DMM). Being assembled into one larger square matrix, having DSMs on its diagonal and DMMs in all other fields, a so-called Multiple Domain Matrix (MDM) is formed. When relating two domains using a DMM, a problem arises when the nature of one individual relationship between the two domains is to be described. Usually, this is modeled by annotating each relationship with the additional information, much like comments in spreadsheet software. This, however, is yet impossible if the relationships should be in matrix notation to allow for algorithmic matrix analyses. Equally, this way, the annotations are not accessible as elements of another matrix, e.g. as DSM. This paper suggests a generic principle to solve the described problem in a way consistent with the matrix methodology. It proposes an approach using MDM and is thereby able to unambiguously provide the nature of each relationship between the elements of two domains. As a DSM is a mere case of a DMM having two identical domains, the approach proposed can equally be used to enrich the relationships within a DSM."
744,"In literature, design structure matrix (DSM), which is a square matrix, has been widely used to address single-domain dependency relationships (e.g., product architecture, process workflow, and organization structure). To extend the DSM efforts, a rectangular matrix becomes a logical format to capture and analyze cross-domain dependency relationships, namely, domain mapping matrix (DMM) [1]. In this context, this paper proposes a unified framework for decomposition of DSM and DMM. The unified framework consists of four methodological phases to offer the functions of DSM clustering, DSM sequencing, and DMM decomposition. To support the development of this framework, various decomposition-related techniques from applied mathematics and engineering design are reviewed. Three matrix examples have been used to illustrate the framework’s applicability."
745,"Functional decomposition is used in conceptual design to divide an overall problem with an unknown solution into smaller problems with known solutions. The procedure for functional decomposition, however, has not been formalized. In a larger effort to understand and develop rules for functional decomposition, this paper develops rules for composition of reverse-engineered functional models. First, the functional basis hierarchy is used in an attempt to compose the functional model of a hair dryer, which does not produce the desired results. Second, a set of rules for composition is presented and applied to the hair dryer functional model. This composed functional model is more similar to the desired decomposition result than the functional model developed by changing hierarchical levels. Ten additional functional models are also composed and the results shown. The findings demonstrate that composition rules can be developed empirically through analysis of functional models."
746,"We propose a deterministic approach for global optimization of large-scale nonconvex quasiseparable problems encountered frequently in engineering systems design, such as multidisciplinary design optimization and product family optimization applications. Our branch and bound-based approach applies Lagrangian decomposition to 1) generate tight lower bounds by exploiting the structure of the problem and 2) enable parallel computing of subsystems and the use of efficient dual methods for computing lower bounds. We apply the approach to the product family optimization problem and in particular to a family of universal electric motors with a fixed platform configuration taken from the literature. Results show that the Lagrangian bounds are much tighter than convex underestimating bounds used in commercial software, and the proposed lower bounding scheme shows encouraging efficiency and scalability, enabling solution of large, highly nonlinear problems that cause difficulty for existing solvers. The deterministic approach also provides lower bounds on the global optimum, eliminating uncertainty of solution quality produced by popular applications of stochastic and local solvers. For instance, our results demonstrate that prior product family optimization results reported in the literature obtained via stochastic and local approaches are suboptimal, and application of our global approach improves solution quality by about 10%. The proposed method provides a promising scalable approach for global optimization of large-scale systems-design problems."
747,"Adaptable design aims to extend the utilities of designs and products. Adaptability is classified into product and design adaptabilities. Product adaptability indicates that functionality and life can be extended for both economical and environmental benefits. Design adaptability improves design reuse by using existing designs to develop new designs more efficiently. To evaluate adaptable design, it is necessary to develop a method for quantitative measurement of the adaptability of products. A new method has been developed that first analyzes the independencies of functions and functional modules and then evaluates the adaptability of interfaces with two indices, as well as the performances of adaptive requirements. The effectiveness of the proposed method is demonstrated by an illustrative example of personal computer motherboards. The results show that the method can evaluate and reveal product adaptability for improving design and providing innovative design."
748,"Products that transform into multiple states give access to greater flexibility and functionality in a single system. These “transformers” capture the imagination and can be elegant, compact, and convenient. Mechanical transformers are usually designed "
749,"There is a need for products that can automatically adapt to various environmental and working conditions. Since a standard theoretical framework for designing such adaptable products is not yet established, only few rules, generalized methods, software tools, and guidelines for design for adaptability can be found in literature. The goal of this paper is to address issues associated with designing adaptable product architecture and to propose a first step to develop methods and tools to deal with these issues. The paper first gives various definitions and an overview of product adaptability. Then it discusses adaptable product architectures, external conditions, and customer needs that are crucial aspects in designing adaptable products. The research proposes a scheme of an adaptable product that can constitute part of a formalized design method for adaptability. Finally, it illustrates the design choices that should be made to arrive at an adaptable product architecture."
750,"Engineering approaches for optimizing designs within a market context generally take the perspective of a single producer, asking what design and price point will maximize producer profit predicted by consumer choice simulations. These approaches treat competitors and retailers as fixed or nonexistent, and they take business-oriented details, such as the structure of distribution channels, as separate issues that can be addressed "
751,"The quest for producing vehicles friendlier to the environment is often impeded by the fact that a producer private good objective, such as maximum profit, competes with the public good objective of minimizing impact on the environment. Contrary to commercial claims, there may be no defined decision maker in the vehicle production and consumption process who takes ownership of the public good objective, except perhaps the government. One way ecofriendly products could become more successful in the marketplace is if public and private good objectives become more aligned to each other. This paper introduces three metrics for comparing Pareto curves in bi-objective problems in terms of relative level of objective competition. The paper also presents a quantitative way of studying an individual firm’s trade-off between profit and fuel consumption for automotive products, currently undergoing an historic evolution in their design. We show how changes in technology, preferences, competition, and regulatory scenarios lead to Pareto frontier changes, possibly eliminating it altogether."
752,"Engineering optimization methods for new product development model consumer demand as a function of product attributes and price in order to identify designs that maximize expected profit. However, prior approaches have ignored the ability of competitors to react to a new product entrant; thus these methods can overestimate expected profit and select suboptimal designs that perform poorly in a competitive market. We propose an efficient approach to new product design accounting for competitor pricing reactions by imposing Nash and Stackelberg conditions as constraints, and we test the method on three product design case studies from the marketing and engineering design literature. We find that a Stackelberg leader strategy generates higher profit than a Nash strategy. Both strategies are superior to ignoring competitor reactions: In our case studies, ignoring price competition results in overestimation of profits by 12%–79%, and accounting for price competition increases realized profits by up to 3.4%. The efficiency, convergence stability, and ease of implementation of the proposed approach enables practical implementation for new product design problems in competitive markets."
753,"Modern retailers now control in excess of 70% of many markets and thereby control the access manufacturers have to the end customer. Success of a new product design therefore depends upon acceptance of the product by the powerful retailer as well as the end customer. One prevalent approach to increasing both retailer and manufacturer revenues is to improve the attractiveness of a product offering by bundling related items together for one price. To be most effective, bundled products should be developed with an integrated design approach that seeks to achieve utility for the end customer as well as cost efficiencies through measures such as using common parts. We propose a bundled product design approach that endogenizes the profit maximizing prices set by the channel controlling (monopolist) retailer. The approach accounts for demand dependencies between the product categories and thus the impact of the bundle and cross-category competition on proposed engineering designs is known. Additionally, the approach simultaneously considers uncertainty in engineering design, competing manufacturer product attributes and customer preferences to ensure acceptable product profitability and market share under interval uncertainty. A bundled product design case study is presented for two complimentary power tools. Manufacturer profit and market share are optimized both deterministically and under uncertainty. We find that considering demand dependencies can create optimal bundle and individual product designs that increase profits for both retailer and manufacturer."
754,"Human appraisals are becoming increasingly important in the design of engineering systems to link engineering design attributes to customer preferences. Human appraisals are used to assess consumers’ opinions of a given product design, and are unique in that the experiment response is a function of both the product attributes and the respondents’ demographic attributes. The design of a human appraisal is characterized as a split-plot design, in which the respondent demographic attributes form the whole-plot factors while the product attributes form the split-plot factors. The experiments are also characterized by random block effects, in which the design configurations evaluated by a single respondent form a block. An experimental design algorithm is needed for human appraisal experiments because standard experimental designs often do not meet the needs of these experiments. In this work, an algorithmic approach to identify the optimal design for a human appraisal experiment is developed, which considers the effects of respondent fatigue and the block and split-plot structure of such a design. The developed algorithm seeks to identify the experimental design which maximizes the determinant of the Fisher Information Matrix, labeled as the "
756,"The understanding of customer preference, in relation to product attributes, is a key challenge in industrial design. It is based on preference measurement, which is a complex task, crucial for sound design decisions. In this paper, we study experimentally the influence of the preference elicitation method on the results of the test. For this, we designed a case study concerning the perception of vehicle’s front-end by a panel of subjects. Two different preference elicitation methods were used in the same session: direct rating on a scale, and pairwise comparison. Conjoint Analysis was used next to model the preference and to compute the part worth of the different design factors. Two conjoint methods were examined: rating based and choice based. The paper presents a comparison of the results of the different modeling with conjoint analysis, in order to assess the reliability and the validity of the models."
757,"The application of market demand models in engineering design is now a well-established practice. One could consider the archetypical application to be a random utility model used in conjunction with a parametric design representation to optimize the design of a single product with respect to a risk-adjusted measure of profit. Much of the work in this area over the past decade has been focused on various extensions of this archetypical framework, such as problem decomposition and product family design. A wide variety of market demand models have been applied, including models derived from classic economic methods and random utility models spanning from multinomial logit through generalized extreme value to mixed logit. While there has been some discussion of the properties of the various choices of market demand models used in prior work, the most recent work in this area suggests that the consequences of market demand model specification in engineering design problems are both more significant than once realized and not yet fully understood. In this paper, we explore the consequences of market demand model specification specifically in the context of engineering design through both a review of prior work and an illustrative example problem featuring a market demand model parameterized in terms of reservation price. These results demonstrate that choices in market demand model specification — especially those relating to representation of customer heterogeneity — can lead to substantially different conclusions in a discrete product configuration design problem."
758,"The benefits of applying market demand models in engineering design have been well established. Arguably the most common approach has been to apply a random utility model, although any representation of market demand based on customers’ responses to surveys or in market situations can be used to this end. In practice, various methods have been used to collect market data through the use of surveys, typically based either on choices from among sets of alternatives or statements of willingness-to-pay. The applicability of the data collected has been limited, however, by the complexity of the relationships between market research methods and market demand models. The prospects of reusing an enormous amount of available market research data and increasing practitioners’ freedom in applying market research data to forecast market demand compel us to investigate the correspondence between market demand models. In this paper, we build on the recent work of Cook and Wissmann to demonstrate a bilateral correspondence between the popular multinomial logit model and Cook’s S-Model. Results from two independent case studies are presented showing that the utility function in a multinomial logit model may be reparameterized as a value function in the S-Model and vice-versa. The techniques demonstrated in this paper for defining direct comparisons between previously incompatible market research methods and demand models provide the practitioner with powerful new capabilities for validating market models, fusing market models to expand customer-based trade spaces, and presenting results to decision-makers in familiar formats."
759,"This paper presents an approach to enhance Hooke-Jeeves optimization search algorithm using fuzzy logic. Hooke-Jeeves algorithm, similar to many other search algorithms, use predetermined fixed parameters. These parameters do not depend on the objective function values at the current search location. Fuzzy logic controllers are incorporated in at various stages of the algorithm to create a new optimization search algorithm: Fuzzy-Controlled Hooke-Jeeves algorithm. The results of this paper shows that incorporating fuzzy logic in Hooke-Jeeves algorithm can enhance the ability of the algorithm to reach extremum in different typical optimization test cases and design problems."
760,"This paper explores the ability of a team of autonomous software agents to be effective in unknown and changing optimization environments by evolving to use the most successful algorithms at the points in the optimization process where they will be the most effective. We present the core framework and methodology which has potential applications in layout, scheduling, manufacturing, and other engineering design areas. The communal agent team organizational structure employed allows cooperation of agents through the products of their work and creates an ever changing set of individual solutions for the agents to work on. In addition, the organizational structure allows the framework to be adaptive to changes in the design space that occur during the optimization process — making our approach extremely flexible to the kinds of dynamic environments encountered in engineering design problems. An evolutionary approach is used, but evolution occurs at the strategic, rather than solution level — where the strategies of agents in the team (the decisions for picking, altering, and inserting a solution) evolve over time. As an application of this approach, individual solutions are tours in the familiar combinatorial optimization problem of the traveling salesman. With a constantly changing set of these tours, the team, each agent running a different solution strategy, must evolve to apply the solution strategies which are most useful given the set at any point in the process. As a team, the evolutionary agents produce better solutions than any individual algorithm. We discuss the extensions to our preliminary work that will make our framework highly useful to the design and optimization community."
761,"Genetic algorithms have been extensively used as a reliable tool for global optimization. However these algorithms suffer from their slow convergence. To address this limitation, this paper proposes a two-fold approach to address these limitations. The first approach is to introduce a twinkling process within the crossover phase of a genetic algorithm. Twinkling can be incorporated within any standard algorithm by introducing a controlled random deviation from its standard progression to avoiding being trapped at a local minimum. The second approach is to introduce a crossover technique: the weighted average normally-distributed arithmetic crossover that is shown to enhance the rate of convergence. Two possible twinkling genetic algorithms are proposed. The performance of the proposed algorithms is successfully compared to simple genetic algorithms using various standard mathematical and engineering design problems. The twinkling genetic algorithms show their ability to consistently reach known global minima, rather than nearby sub-optimal points with a competitive rate of convergence."
762,"Trade space exploration is a promising decision-making paradigm that provides a visual and more intuitive means for formulating, adjusting, and ultimately solving design optimization problems. This is achieved by combining multi-dimensional data visualization techniques with visual steering commands to allow designers to “steer” the optimization process while searching for the best, or Pareto optimal, designs. In this paper, we compare the performance of different combinations of visual steering commands implemented by two users to a multi-objective genetic algorithm that is executed “blindly” on the same problem with no human intervention. The results indicate that the visual steering commands — regardless of the combination in which they are invoked — provide a 4x–7x increase in the number of Pareto solutions that are obtained when the human is “in-the-loop” during the optimization process. As such, this study provides the first empirical evidence of the benefits of interactive visualization-based strategies to support engineering design optimization and decision-making. Future work is also discussed."
763,"Designers perform many tasks when developing new products and systems, and making decisions may be among the most important of these tasks. The trade space exploration process advocated in this work provides a visual and intuitive approach for formulating and solving single- and multi-objective optimization problems to support design decision-making. In this paper, we introduce an advanced sampling method to improve the performance of the visual steering commands that have been developed to explore and navigate the trade space. This method combines speciation and crowding operations used within the Differential Evolution (DE) algorithm to generate new samples near the region of interest. The accuracy and diversity of the resulting samples are compared against simple Monte Carlo sampling as well as the current implementation of the visual steering commands using a suite of test problems and an engineering application. The proposed method substantially increases the efficiency and effectiveness of the sampling process while maintaining diversity within the trade space."
764,"The use of vibration-based techniques in damage identification has recently received considerable attention in many engineering disciplines. While various damage indicators have been proposed in the literature, those relying only on changes in the natural frequencies are quite appealing since these quantities can conveniently be acquired. Nevertheless, the use of natural frequencies in damage identification is faced with many obstacles, including insensitivity and non-uniqueness issues. The aim of this paper is to develop a viable damage identification scheme based only on changes in the natural frequencies and to attempt to overcome the challenges typically encountered. The proposed methodology relies on building a Finite Element Model (FEM) of the structure under investigation. A modified Particle Swarm Optimization (PSO) algorithm is proposed to facilitate updating the FEM in accordance with experimentally-determined natural frequencies in order to predict the damage location and extent. The method is tested on beam structures and was shown to be an effective tool for damage identification."
765,"This paper describes an enhanced version of a new global optimization method, Multi-Agent Normal Sampling Technique (MANST) described in reference [1]. Each agent in MANST includes a number of points that sample around the mean point with a certain standard deviation. In each step the point with the minimum value in the agent is chosen as the center point for the next step normal sampling. Then the chosen points of all agents are compared to each other and agents receive a certain share of the resources for the next step according to their lowest mean function value at the current step. The performance of all agents is periodically evaluated and a specific number of agents who show no promising achievements are deleted; new agents are generated in the proximity of those promising agents. This process continues until the agents converge to the global optimum. MANST is a standalone global optimization technique and does not require equations or knowledge about the objective function. The unique feature of this method in comparison with other global optimization methods is its dynamic normal distribution search. This work presents our recent research in enhancing MANST to handle variable boundaries and constraints. Moreover, a lean group sampling approach is implemented to prevent sampling in the same region for different agents. The overall capability and efficiency of the MANST has been improved as a result in the newer version. The enhanced MANST is highly competitive with other stochastic methods such as Genetic Algorithm (GA). In most of the test cases, the performance of the MANST is significantly higher than the Matlab™ GA Toolbox."
766,"In this paper, a compact packing algorithm for the placement of objects inside a container is described. The proposed packing algorithm packs three-dimensional free-form objects inside an arbitrary enclosure such that the packing efficiency is maximized. The proposed packing algorithm can handle objects with holes or cavities and its performance does not degrade significantly with the increase in complexity of the enclosure or the objects. The packing algorithm takes as input the triangulated geometry of the container and all the objects to be packed and outputs the list of objects that can be placed inside the enclosure. The packing algorithm also outputs the location and orientation of all the objects, the packing sequence, and the packed configuration. An improved layout algorithm that works with arbitrary container geometry is also proposed. Several heuristics to improve the performance of the packing algorithm as well as certain aspects that facilitate fast and efficient handling of CAD data are also discussed. A comprehensive benchmarking of the proposed packing algorithm on synthetic and hypothetical problems reflects its superior performance as compared to other similar approaches."
767,"Based on general principles of robust design and axiomatic design, relationship among robustness, structural parameters, design parameters and uncontrollable factors has been established. Various factors that affect system robustness were analyzed mathematically to determine the relationship between robustness and structural characteristics of the linear system. The relations among functional requirements were also explored. Accordingly, an optimization model was established to determine design parameters. This new robust design approach can be used for linear mechanical system analysis."
768,"Anthropometric data are widely used in the design of chairs, seats, and other furniture intended for seated use. These data are valuable for determining the overall height, width, and depth of a chair, but contain little information about body shape that can be used to choose appropriate contours for backrests. A new method is presented for statistical modeling of three-dimensional torso shape for use in designing chairs and seats. Laser-scan data from a large-scale civilian anthropometric survey were extracted and analyzed using principal component analysis. Multivariate regression was applied to predict the average body shape as a function of overall anthropometric variables. For optimization applications, the statistical model can be exercised to randomly sample the space of torso shapes for automated virtual fitting trials. This approach also facilitates trade-off analyses and other the application of other design decision-making methods. Although seating is the specific example here, the method is generally applicable to other designing for human variability situations in which applicable body contour data are available."
769,"In many engineering applications, both random and interval variables may exist. The mixture of both types of variables has been dealt with in robust design for single disciplinary systems. This work focuses on robustness assessment for general multidisciplinary systems with the presence of both random and interval variables. To alleviate the intensive computational demand, we propose a semi-second-order Taylor expansion method to estimate robustness. A performance function is approximated linearly in terms of random variables, but the interaction terms between random and interval variables are kept. Robustness is measured by the standard deviation of a system performance. The maximum and minimum standard deviations over the interval variables are identified. With proposed method, the impact of both random and interval variables on the system performance can be found efficiently."
770,"Designing for human variability frequently necessitates an estimation of the spatial requirements of the intended user population. These measures are often obtained from “proportionality constants” which predict the lengths of relevant anthropometry using stature. This approach is attractive because it is readily adapted to new populations—only knowledge of a single input, stature, is necessary to obtain the estimates. The most commonly used ratios are those presented in Drillis and Contini’s report from 1966 [1]. Despite the prevalence of their use, these particular values are limited because the size and diversity of the population from which these ratios were derived is not in the literature, and the actual body dimensions that each ratio represents are not clear. Furthermore, they are often misinterpreted and used inappropriately. This paper introduces a new approach, the “boundary ratio” which mitigates many of these issues. Boundary ratios improve on the traditional application of proportionality constants by: 1) explicitly defining the body dimensions, 2) defining constants for the 5th , 50th , and 95th  percentile measures, and 3) providing distinct constants for males and females when necessary. This approach is shown to better model the range of variability exhibited in population body dimensions."
771,"Inspection is an essential part of the entire manufacturing chain providing measurement feedback to the process planning system. Fully automated machining requires automatic inspection process planning and real-time inspection results feedback. As inspection process planning is still based on G&M codes containing low-level information or vendor-specific bespoke routines, inspection process planning is mostly isolated from machining process planning. With the development of new data model standards STEP and STEP-NC providing high-level product information for the entire manufacturing chain, it is achievable to combine machining and inspection process planning to generate optimal machining and inspection sequences with real-time measurement results feedback. This paper introduces an integrated process planning system architecture for combined machining and inspection. In order to provide real-time inspection feedback, On-Machine Inspection (OMI) is chosen to carry out inspection operations. Implementation of the proposed architecture has been partially carried out with a newly developed data model and interpreter software. A case study was carried out to test the feasibility of the proposed architecture."
772,"Multi-axis slicing for solid freeform fabrication (SFF) manufacturing process can yield non-uniform thickness layers, or 3-D layers. Using the traditional parallel layer construction approach to build such a layer leads to a staircase which requires machining or other post processing to form the desired shape. This paper presents a direct 3-D layer deposition approach. This approach uses an empirical model to predict the layer thickness based on experimental data. The toolpath between layers is not parallel; instead, it follows the final shape of the designed geometry and the distance between the toolpath in the adjacent layers varies at different locations. Directly depositing a 3-D layer not only eliminates the staircase effect, but also improves the manufacturing efficiency by shortening the deposition and machining times. A single track deposition experiment has demonstrated these advantages. Thus, it is a beneficial addition to the traditional parallel deposition method."
773,"Process planning for parts made on CNC machines is usually performed in sequence and carried out without feeding back information about the fabrication process to the planning stage. Shifting planning capabilities to the machine level enables consideration of direct machine feedback. In this paper a method for feature decomposition and tool path planning for pocket milling on a 3 axis milling machine is presented. Based on standard computerized numerical control (CNC) technology, G-code is transferred to the fabrication machine whereat adaptation of processing parameters can take place within a feature. In order to enable a feedback loop from the processing to the planning stage, subfeature elements are defined. Interrelations between subsequent elements are identified and standardized with regard to the manufacturing needs to realize fully machined features. For each element a tool path is generated and translated into valid G-code. After fabrication of each subfeature element processing parameters can be improved. The presented method is implemented in a prototype software tool based on an open source CAD/CAM/CAE kernel and extends an existing open source CAM software framework."
774,"One of the methods design engineers use to achieve alignment between precision features of two mating parts is through locating or alignment features. The most common locating features are a pair of small holes and pins that would define the in-plane position and orientation of two parts with respect to each other. These locating features establish a datum frame for all other critical features of a part. Design engineers would be interested to know the worst-case and probabilistic value of misalignments resulting from their choices of tolerances that assure fit of the datum features as well as their choices of tolerances that locate the precision features with respect to such a datum. This paper uses two-dimensional tolerance analysis methods to formulate this alignment problem. Worst-case estimates as well as Monte Carlo simulation have been used to analyze the misalignments resulting from play in the datum fit. It is shown that a number of dimensionless parameters can characterize the misalignment and these are used to create graphs adequate for design work."
775,"Trends, ergonomics and engineering analysis post more challenges than ever to product shape designs, especially in the freeform area. In this paper, freeform feature handles are proposed for easing of difficulties in modifying an existing freeform shape. Considering the variations of curvature as the footprint of a freeform feature(s), curvature analysis is applied to find manipulators, e.g. handles, of a freeform feature(s) in the shape. For these, a Laplacian based pre-processing tool is proposed first to eliminate background noise of the shape. Then least square conformal mapping is applied to map the 3D geometry to a 2D polygon mesh with the minimum distortions of angle deformation and non-uniform scaling. By mapping the curvature of each vertex in the 3D shape to the 2D polygon mesh, a curvature raster image is created. With image processing tools, different levels of curvature changing are identified and marked as feature point(s) / line(s) / area(s) in the freeform shape. Following the definitions, the handles for those intrinsic freeform features are established by the user based on those feature items. Experiments were conducted on different types of shapes to verify the rightness of the proposed method. Different effects caused by different parameters are discussed as well."
777,"The modeling of many practical problems in design and manufacturing involving moving objects relies on sweeps and their boundaries, which are mathematically described by the envelopes to the family of shapes generated by the moving object. In many problems, such as the design and analysis of higher pairs or tool path planning, contact changes between the moving object and the boundary of its swept volume become critical because they often translate into functional changes of the system under consideration. However, the difficulty of this task quickly escalates beyond the reach of existing approaches as the complexity of the shape and motion increases. We recently proposed a sweep boundary evaluator for general sweeps in conjunction with efficient point sampling and surface reconstruction algorithms that relies on our novel point membership classification (PMC) test for general solid sweeps. In this paper we describe a new approach that automates the prediction of changes in the state of contact between a shape of arbitrary complexity moving according to an affine motion, and the boundary of its swept set. We show that we can predict when and where such contact changes occur with only minimal additional computational cost by exploiting the data output by our sweep boundary evaluator. We discuss the problem and the associated computational issues in a 2D framework, and we conclude by discussing the extension of our approach to 3D moving objects."
779,"This paper addresses the way models mixing various types of geometric representations (e.g. NURBS curves and patches, polylines, meshes), potentially immersed in spaces of different dimensions (e.g. NURBS patch and its 2D trimming lines), can be deformed simultaneously. The application domains range from the simple deformation of a set of NURBS curves in a 2D sketcher to the simultaneous deformation of meshes, patches as well as trimming lines lying in parametric spaces. The deformation itself results from the solution of an optimization problem defined by a set of geometric constraints and deformation behaviors. This new breakthrough on how geometric models can be manipulated has been made possible thanks to our linear mechanical model of deformation that can be coupled to manifolds of dimension zero (e.g. points, vertices) and one (e.g. edges, segments) whatever the spaces dimension. An extended constraints toolbox is also proposed that enables the specification of both characteristic points/curves and continuity conditions between the various geometric models. The link between the semantics of the deformation behaviors and the geometric models is ensured through the use of multiple minimizations. The approach is illustrated with several examples coming from our prototype software."
780,"This paper considers the problem of inferring the geometry of an object from values of the signed distance sampled on a uniform grid. The problem is motivated by the desire to effectively and efficiently model objects obtained by 3D imaging technology that is now ubiquitous in medical diagnostics. Recently developed techniques for automated segmentation convert intensity to signed distance, and the voxel structure imposes the uniform sampling grid. While specification of the signed distance function throughout the ambient space would provide an implicit model that uniquely specifies the object, a set of uniformly sampled signed distance values may uniquely determine neither the distance function nor the shape of the object. Here we employ essential properties of the signed distance to construct upper and lower bounds on the allowed variation in signed distance in 1, 2, and 3 dimensions. The bounds are combined to produce interval-valued extensions of the signed distance function including a tight global extension and more computationally efficient local bounds that provide useful criteria for root exclusion/isolation."
781,"Camoids are three dimensional cams that can produce more complex follower output than plain disc cams. A camoid follower motion is described by a surface rather than a curve. The camoid profile can be directly synthesized once the follower surface is fully described. To define a camoid follower motion surface it is required that the surface pass by all predefined constraints. Constraints can be follower position, velocity and acceleration. These design constraints are scattered all along the camoid follower surface. Hence a fitting technique is needed to satisfy these constraints which include position and its derivatives (velocity and acceleration). Furthermore if the fitting function can be of a parametric nature, then it would be possible to optimize the follower surface to obtain better performance according to a specific objective. Previous research has established a method to fit camoid follower surface positions, but did not tackle the satisfaction of derivative constraints. This paper presents a method for defining a camoid follower characteristic surface B-Splines on two steps first synthesizing the sectional cam curves then using a surface interpolation technique to generate the follower characteristic surface. The fitting technique is parametric in nature which allows for its optimization. Real coded Genetic algorithms are used to optimize the parameters of the surface to meet a specified objective function. A demonstration problem to illustrate the suggested methodology is presented."
782,"Recently, meshes of engineering objects have been easily acquired by 3D laser or high-energy industrial X-ray CT scanning systems and they are widely used in product developments. For the effective use of scanned meshes in inspection, re-design, and simulation of the objects, it is important to reconstruct CAD models from the meshes. Engineering objects often exhibit Euclidean symmetries for their functionalities. Therefore, it is essential to detect such symmetries when reconstructing CAD models with compact data representations which are similar to the ones already defined in CAD systems. However, existing methods for reconstructing CAD models have not focused on detecting such symmetries. In this paper, we propose a new method that detects partial or global Euclidean symmetries, including translation, rotation, and reflection, from scanned meshes of engineering objects based on the combination of the ICP and the region growing algorithms. Our method can robustly and efficiently extract pairs of symmetric regions and their transformations under which the pair can be closely matched to each other. We demonstrate the effectiveness of the proposed method from experiments on various scanned meshes."
784,"Since the vehicle program in automotive industry gets more and more extensive, the costs related to inspection increase. Therefore, there are needs for more effective inspection preparation. In many situations, a large number of inspection points are measured, despite the fact that only a small subset of points is needed. A method, based on cluster analysis, for identifying redundant inspection points has earlier been successfully tested on industrial cases. Cluster analysis is used for grouping the variables into clusters, where the points in each cluster are highly correlated. From every cluster only one representing point is selected for inspection. In this paper the method is further developed and multiple linear regression is used for evaluating how much of the information that is lost when discarding an inspection point. The information loss can be quantified using an efficiency measure based on linear multiple regression, where the part of the variation in the discarded variables that can be explained by the remaining variables is calculated. This measure can be illustrated graphically and that helps to decide how many clusters that should be formed, i.e. how many inspection points that can be discarded."
785,"When a cylindrical datum feature is specified at maximum material condition (MMC) or least material condition (LMC) a unique circumstance arises: a virtual condition (VC) cylindrical boundary must be defined [1]. The geometric relationship between a cylindrical point cloud obtained from inspection equipment and a VC cylinder has not been specifically addressed in previous research. In this research, novel approaches to this geometric analysis are presented, analyzed, and validated. Two of the proposed methods are new interpretations of established methods applied to this unique geometric circumstance: least squares and the maximum inscribing cylinder (MIC) or minimum circumscribing cylinder (MCC). The third method, the Hull Normal method, is a novel approach specifically developed to address the VC cylinder problem. Each of the proposed methods utilizes a different amount of sampled data, leading to various levels of sensitivity to sample size and error. The three methods were applied to different cylindrical forms, utilizing various sampling techniques and sample sizes. Trends across sample size were analyzed to assess the variation in axial orientation when compared to the true geometric form, and a relevant case study explores the applicability of these methods in real world applications."
786,"In this paper, the four-point interpolatory subdivision scheme for curve generation is adapted to the interpolation of a set of positions of a cylindrical tool represented by dual quaternions. The resulting discrete model of the tool path lends itself naturally to an algorithm for computing the characteristic curve belonging to the boundary surface of the swept volume of a cylinder at each of the discrete positions. This approach to compute the discrete model of the swept surface of a motion is numerically robust and computationally efficient since it is based only on linear combinations. The results have applications in NC simulation and verification, robot path planning, and computer graphics."
787,"In this paper, a new design of a parallel manipulator is proposed for industrial applications, specifically for material surface finishing processes. Though most current parallel mechanisms have been based on the Stewart-Gough platform which has 6 degrees of freedom (DOF), the focus of this design is on a 3-DOF manipulator with one novel configuration. In order to benefit production, a parallel kinematic machine (PKM) capable of high speed industrial operations with high accuracy and rigidity is necessary. First, system modelling includes mobility study, inverse kinematic model, Jacobian matrix, singularity analysis and workspace calculation are conducted. Then, a CAD model is presented showing the optimum design features and detailed mechanics. Finally, finite element analysis is carried out for the device optimization."
788,"To increase productivity of marine propellers by raising machining efficiency, this paper presents the zigzag/spiral tool paths generation algorithm based on the arc base curve approach for three-axis machining of curved surfaces of propellers. By considering the shapes of selected cutters with different types of tool paths generated by the proposed procedure, machining efficiency can be calculated and simulated. To verify the accuracy and effectiveness of the developed approach, numerical and experimental results of machining of propeller surfaces are compared. It was proved that the machining time can be cut down up to 19% by using zigzag tool paths with a toroidal cutter. In addition, the machining knowledge revealed here can be accumulated for benefit evaluation in the manufacturing process with existing CAD/CAM systems. From the cost model, design, and process views, the overall cost savings after 5 years are investigated, and the expected benefit yield is about 45%."
789,"A set-based multiscale and multidisciplinary design method has been proposed in which distributed designers manage interdependencies by exchanging targets and Pareto sets of solutions. Prior research has shown that the set-based method (SBM) has the potential to reduce the number of costly iterations between design teams, relative to centralized optimization approaches, while expanding the variety of high-quality, system-wide solutions. These results have been obtained with representative examples in a laboratory setting. The goal of this research is to investigate whether similar results are obtained from an industrial trial, implemented in an industry design environment. The SBM is applied to the design of a downhole module for our industrial partners at Schlumberger, a developer of oilfield tools and services. The design was conducted on location at Schlumberger by an intern who converted the existing Schlumberger design process into a set-based design process. Results indicate that the SBM delivers the benefits predicted in the laboratory, along with a host of advantageous side effects, such as a library of back-up design options for future design projects."
790,"In applications broadly defined for actuation, magnetostrictive materials possess intrinsic rapid response times while providing small and accurate displacements and high-energy efficiency, which are some of the essential parameters for fast control of fuel injector valves for decreased engine emissions and lower fuel consumption. This paper investigates the application of Terfenol-D as a magnetostrictive actuator material for CNG fuel injection actuation. A prototype fuel injector assembly, which includes Terfenol-D as the core actuator material, was modeled in Finite Element Method Magnetics (FEMM) simulation software for 2D magnetics simulation. FEMM was used in order to determine the coil-circuit parameters and the required flux density or applied magnetic field to achieve the desired magnetostrictive strain, and consequently, the injector needle lift. The FEMM magnetic simulation was carried out with four different types of AWG coil wires and four different coil thicknesses of the entire injector assembly in order to evaluate the relationship between the different coil types and thicknesses against the achieved strain or injector lift."
791,"This paper focuses on the parametric modeling and optimization of the Chemical Vapor Deposition (CVD) process for the deposition of thin films of silicon from silane in a vertical impinging CVD reactor. The parametric modeling using Radial Basis Function (RBF) for various functions which are related to the deposition rate and uniformity of the thin films are studied. These models are compared and validated with additional sampling data. Based on the parametric models, different optimization formulations for maximizing the deposition rate and the working areas of thin film are performed."
792,"Optimizing the performances of parallel manipulators by adjusting the structure parameters can be a difficult and time-consuming exercise especially when the parameters are multifarious and the objective functions are too complex. Artificial intelligence approaches can be investigated as the effective criteria to address this issue. In this paper, genetic algorithms and artificial neural network are implemented as the intelligent optimization criteria of global stiffness and dexterity for spatial six degree-of-freedom (DOF) parallel manipulator. The objective functions of global stiffness and dexterity are calculated and deduced according to the kinetostatic model. Neural networks are utilized to model the solutions of performance indices. Multi-objective optimization is developed by Pareto-optimal solution. The effectiveness of the proposed methodology is proved by simulation."
793,"Product family design is a well recognized method to address the demands of mass customization. A potential drawback of product families is that the performance of individual members are reduced due to the constraints added by the common platform, i.e. parts and components need to be shared by other family members. This paper presents a formal mathematical framework where the product family design problem is stated as an optimization problem and where optimization is used to find an optimal product family. The object of study is kinematics design of a family of industrial robots. The robot is a serial manipulator where different robots share arms from a common platform. The objective is to show the trade-off between the size of the common platform and the kinematics performance of the robot."
794,"Gradient search methods for linkage synthesis have difficulty on the design convergence due to the non-convexity and non-uniqueness characteristics on linkage design. Although evolutionary optimization approach can solve non-convex problems and produce multiple optimal solutions, it cannot be applied to linkage synthesis easily because a general and efficient method for mechanism analysis is not available. In order to automate the linkage synthesis process, a novel linkage analysis method, called the Constrained Superposition Method (CSM), is presented in this paper. The CSM is based on the concept of Finite Element Analysis. The CSM can analyze any linkage mechanism with Single-Input-Single-Output (SISO). To further improve the efficiency of evolutionary optimization process, two feasibility checks are introduced to ensure the connectivity and mobility. Finally, four linkage synthesis examples are presented and discussed to demonstrate the effectiveness of this method."
795,"Genetic Programs that have phenotypes created by the application of genotypes comprising rules are robust and highly scalable. Such encodings are useful for complex applications such as controller design. This paper outlines an evolutionary algorithm capable of creating a controller for 2 DOF, path following robot. The controllers are embodied by Artificial Neural Networks capable of full functionality despite multiple failures."
796,"Researchers have paid relatively little attention to the fact that most of what is considered design is more like redesign than original design. Redesign activities are characterized by an attempt to leverage experience, knowledge, and the capital that a company has already invested into existing engineering systems. In this paper, a method for undertaking strategic redesign is proposed and explained. This method includes support for designers making decisions in redesign problems when there exist systems to be leveraged and multiple new systems to be created. In addition, strategy is introduced to the problem through the consideration that new systems may not be offered all at once, as is often assumed in product family design research. In this paper, the aim of the designer is assumed to be a creation, through redesign, of a series of new systems with desirable and distinct performance levels. In addition, a plan is required to involve as little redesign effort throughout the life of the family of systems as possible. The proposed approach is based upon the concepts of Constructal Theory and previous work to create methods for the design of mass customized families of systems. In addition, two metrics are developed to represent considerations unique to redesign as opposed to original design. These metrics for redesign effort and commonality value are utilized in the overall objective formulation for the proposed approach to redesign. Through a simple redesign scenario involving a family of universal motors, it is shown that the overall approach proposed can lead the designer towards promising redesign plans involving leveraging of existing systems, but that the constructal-inspired approach in and of itself has certain limitations when applied to redesign."
797,"Designers are continuously challenged to manage complexity in embodiment design processes (EDPs), in the context of integrated product and materials design. In order to manage complexity in design processes, a systematic strategy to embodiment design process generation and selection is presented in this paper. The strategy is based on a value-of-information-based "
798,"In this paper, the authors propose computerized support for fault tree analysis (FTA) based on a new design knowledge management approach called quantity dimension indexing. FTA is a method of analyzing and visualizing the causes of fault events by expanding a fault event hierarchically to its possible cause events and constructing a tree diagram representing the entire structure of the problem. When a designer finds or encounters a problem during a product design and development process, an effective way of ensuring the security and safety of the product is to identify all the possible causes of the problem by FTA and fix them. Although FTA is an effective method, it is not easy for a designer to construct a complete fault tree without any misunderstanding or oversight. A promising approach for supporting FTA is to utilize a computerized knowledge management method. Although many knowledge management techniques for literal expression have been developed, they are not necessarily suitable for managing the engineering design knowledge of physical phenomena. To solve this problem, the authors propose a new design knowledge management approach called quantity dimension indexing and computerized support for FTA such as the verification of consistency of a fault tree and fault tree construction advice. By analyzing fault tree examples based on actual design activities in a company, the possible feasibility and future promise of the proposed approach are indicated."
799,"Surrogate models are commonly used to replace expensive simulations of engineering problems. Frequently, a single surrogate is chosen based on past experience. Previous work has shown that fitting multiple surrogates and picking one based on cross-validation errors ("
800,"Quality and performance are two important customer requirements in vehicle design. Driveline clunk negatively affects the perceived quality and must be therefore, minimized. This is usually achieved using engine torque management, which is part of engine calibration. During a tip-in event, the engine torque rate of rise is limited until all the driveline lash is taken up. However, the engine torque rise, and its rate can negatively affect the vehicle throttle response which determines performance. Therefore, the engine torque management must be balanced against throttle response. In practice, the engine torque rate of rise is calibrated manually. This paper describes an analytical methodology for calibrating the engine torque, considering uncertainty, in order to minimize the clunk disturbance, while still meeting throttle response constraints. A set of predetermined engine torque profiles which span the practical range of interest, are used and the transmission turbine speed is calculated for each profile using a bond-graph vehicle model. The turbine speed quantifies the clunk disturbance. Using the engine torque profiles and the corresponding turbine speed responses, a time-dependent metamodel is created using principal component analysis and Kriging. The metamodel predicts the turbine speed response due to any engine torque profile and is used in a deterministic and reliability-based optimization which minimizes the clunk disturbance while still meeting the throttle response target. Compared with commonly used production calibration, the clunk disturbance is reduced substantially without negatively affecting the vehicle throttle response."
801,"Radial Basis Function (RBF) metamodels have recently attracted increased interest due to their significant advantages over other types of non-parametric metamodels. However, because of the interpolation nature of the RBF mathematics, the accuracy of the model may dramatically deteriorate if the training data set used contains duplicate information, noise or outliers. Also constructing the metamodel may be time consuming whenever the training data sets are large or a high dimensional model is required. In this paper, we propose a robust and efficient RBF metamodeling approach based on data pre-processing techniques that alleviate the accuracy and efficiency issues commonly encountered when RBF models are used in typical real engineering situations. These techniques include 1) the removal of duplicate training data information, 2) the generation of smaller uniformly distributed subsets of training data from large data sets and 3) the quantification and identification of outliers by principal component analysis (PCA) and Hotelling statistics. Simulation results are used to validate the generalization accuracy and efficiency of the proposed approach."
802,"The optimal design of complex systems in engineering requires the availability of mathematical models of system’s behavior as a function of a set of design variables; such models allow the designer to find the best solution to the design problem. However, system models (e.g. CFD analysis, physical prototypes) are usually time-consuming and expensive to evaluate, and thus unsuited for systematic use during design. Approximate models, or metamodels, of system behavior based on a limited set of data allow significant savings by reducing the resources devoted to modeling during the design process. In our work in engineering design based on multiple performance criteria, we propose the use of Multi-response Bayesian Surrogate Models (MRBSM) to model several aspects of system behavior jointly, instead of modeling each individually. By doing so, it is expected that the observed correlation among the response variables can be used to achieve better models with smaller data sets. In this work, we study the approximation capabilities of several covariance functions needed for multi-response metamodeling with MRBSM, performing a simulation study in which we compare MRBSM based on different covariance functions against metamodels built individually for each response. Our preliminary results indicate that MRBSM outperforms individual metamodels in 46% to 67% of the test cases, though the relative performance of the studied covariance functions is highly dependent on the sampling scheme used and the actual correlation among the observed response values."
803,"Large-scale design problems are high dimensional and deeply-coupled in nature. The complexity of such large-scale systems prevents designers from solving them as a whole. Analytical target cascading (ATC) provides a systematic approach in solving decomposed large-scale systems that has solvable subsystems. By coordinating between subsystems, ATC can obtain the same optima as they were undecomposed. However, a convergent coordination requires series of ATC iterations that may hinder the efficiency of ATC. In this research, a sequential linearization technique is proposed to improve the efficiency of ATC. The proposed linearization technique is applied to each ATC iteration, therefore each iteration has all linear subsystems that can be solved with high efficiency. One further motivation of the proposed strategy is its perceived potential in handling multilevel problems with random design variables. As previously studied, the sequential linear programming (SLP) algorithm in [1] provides a good balances between efficiency, accuracy and convergence for single-level design optimization under random design variables. The proposed linearization technique can integrate with the SLP algorithm for multilevel systems. The global convergence of this approach is ensured by a filter to determine the acceptance of the optima at each iteration and the corresponding trust region. A geometric programming example and a structure design example demonstrate the efficiency of the proposed method over standard ATC solution process without loss of accuracy."
804,"In this paper, a parallelization model for PSO through sharing of digital pheromones between multiple particle swarms to search n-dimensional design spaces is presented. Digital pheromones are models simulating real pheromones produced by insects for communication to indicate a source of food or a nesting location. Particle swarms search the design space with digital pheromones aiding communication within the swarm to improve search efficiency. Digital pheromones have demonstrated the capability of searching design spaces within PSO in the previous work by authors in both single and coarse granular parallel computing environments. Multiple swarms are simultaneously deployed across various processors in the coarse granular scheme and synchronization is carried out only when all swarms achieved convergence. This was done in an effort to reduce processor-to-processor communication and network latencies. With an appropriate parallelization scheme, the benefits of digital pheromones and swarm communication can potentially outweigh the network latencies resulting in improved search efficiency and accuracy. A swarm is deployed in the design space across different processors to explore this idea. Each part of the swarm is made to communicate with each other through an additional processor. Digital pheromones aiding within a swarm, communication between swarms is facilitated through the developed parallelization model. In this paper, the development and implementation of this method together with benchmarking test cases are presented."
805,"Many engineering systems are too complex to design as a single entity. Decomposition-based design optimization methods partition a system design problem into subproblems, and co-ordinate subproblem solutions toward an optimal system design. Recent work has addressed formal methods for determining an ideal system partition and coordination strategy, but coordination decisions have been limited to subproblem sequencing. An additional element in a coordination strategy is the linking structure of the partitioned problem, i.e., the allocation of constraints that guarantee that the linking variables among subproblems are consistent. There can be many alternative linking structures for a decomposition-based strategy which can be selected for a given partition, and this selection should be part of an optimal simultaneous partitioning and coordination scheme. This paper develops a linking structure theory for a particular class of decomposition-based optimization algorithms, Augmented Lagrangian Coordination (ALC). A new formulation and coordination technique for parallel ALC implementations is introduced along with a specific linking structure theory, yielding a partitioning and coordination selection method for ALC that includes consistency constraint allocation. This method is demonstrated using an electric water pump design problem."
806,"Astute choices made early in the design process provide the best opportunity for reducing the life cycle cost of a new product. Optimal decisions require reasonably detailed disciplinary analyses, which pose coordination challenges. These types of complex multidisciplinary problems are best addressed through the use of decomposition-based methods, several of which have recently been developed. Two of these methods are collaborative optimization (CO) and analytical target cascading (ATC). CO was conceived in 1994 in response to multidisciplinary design needs in the aerospace industry. Recent progress has led to an updated version, enhanced collaborative optimization (ECO), that is introduced in this paper. ECO addresses many of the computational challenges inherent in CO, yielding significant computational savings and more robust solutions. ATC was formalized in 2000 to address needs in the automotive industry. While ATC was originally developed for object-based decomposition, it is also applicable to multidisciplinary design problems. In this paper, both methods are applied to a set of test cases. The goal is to introduce the ECO methodology by comparing and contrasting it with ATC, a method familiar within the mechanical engineering design community. Comparison of ECO and ATC is not intended to establish the computational superiority of either method. Rather, these two methods are compared as a means of highlighting several promising approaches to the coordination of distributed design problems."
807,"Manufacturing that minimizes the exhaustion of natural resources, energy used, and deleterious environmental impact is increasingly demanded by societies that seek to protect global environments as much as possible. To achieve this, lifecycle design (LCD) is an essential component of product design scenarios, however LCD approaches have not been well integrated in optimal design methods that support quantitative decision making. This study presents a method that yields quantitative solutions through optimization analysis of a conceptual product design incorporating lifecycle considerations. We consider two types of optimization approaches that have different aims, namely, (1) to reduce the use of raw materials and energy consumption, and (2) to facilitate the reuse of the product or its parts when it reaches the end of its useful life. We also focus on how the optimization results differ according to the approach used, from the view point of the 3R concept (Reduce, Reuse and Recycling). Our method obtains optimum solutions by evaluating objectives fitted to each of these two optimization approaches with respect to the product’s lifecycle stages, which are manufacturing, use, maintenance, disposal, reuse and recycling. As an applied example, a simple linear robot model is presented, and Pareto optimum solutions are obtained for the multiobjective optimization problem whose evaluated objectives are the operating accuracy and the different lifecycle costs for the two approaches. The characteristics of the evaluated objectives and design variables, as well as the effects of using material properties as design parameters, are also examined."
808,"In machine product designs, a variety of characteristics such as product performances, manufacturing cost, and robustness of characteristics are evaluated, and the need for improvements is increasingly stringent over time. Such characteristics almost always have interrelationships, and systematic evaluation and optimization must be performed to obtain preferable product design solutions. To conduct effective system optimization, the complex interrelationships among characteristics that are included, and sometimes hidden, in the optimization problem, must be understood and dealt with. To construct optimization methodologies for such problems, these relationships must be clarified, and the characteristics simplified. Simplifying the characteristics makes the essence of the optimization problem clearer, and facilitates examining the interrelationships among the simplified characteristics during the optimization process. Based on the simplified characteristics, “priority relationships”, i.e., relationships among simplified characteristics that will be optimized first, and “conflicting relationships”, i.e., tradeoff relationships that will be simultaneously optimized, are obtained. Hierarchical optimization procedures develop naturally as the relationships among the simplified characteristics are clarified. This paper focuses on the priority relationships of characteristics in system optimization procedures. General and specific rules concerning priority relationships are presented, and these form the basis for the constructed optimization procedures. An applied example of a machine tool product design is presented to demonstrate the effectiveness of the proposed methodology."
809,"In the context of globalization and mass customization, selecting the appropriate product configuration requires a simultaneous consideration of multiple criteria or objectives, which are in conflict with each other. The large solution space implies that analyzing each feasible solution is a combinatorial problem. Furthermore, no single optimal solution exists; on the contrary, there is a set of valid optimal solutions, i.e., the solution set is Pareto-optimal. We present the configuration problem from the perspective of using two types of attributes: static, i.e., the attributes that have pre-defined and constant values throughout the configuration process, and dynamic, i.e., attributes whose values vary according to decisions that are being made during the configuration process. We pose the product configuration as a multiobjective optimization problem requiring that multiple objective functions cannot be combined into a single objective function. We demonstrate the applicability of using Multi-Objective Genetic Algorithms (MOGA) to solve the problem and converge to a Pareto-optimal solution set from the large number of feasible solutions."
810,"Plug-in hybrid electric vehicle (PHEV) technology is receiving attention as an approach to reducing U.S. dependency on foreign oil and emissions of greenhouse gases (GHG) from the transportation sector. Because plug-in vehicles require large batteries for energy storage, battery weight can have a significant impact on vehicle performance: Additional storage capacity increases the range that a PHEV can travel on electricity from the grid; however, the associated increased weight causes reduced efficiency in transforming electricity and gasoline into miles driven. We examine vehicle simulation models for PHEVs and identify trends in fuel consumption, operating costs, and GHG emissions as battery capacity is increased. We find that PHEVs with large battery capacity consume less gasoline than small capacity PHEVs when charged every 200 miles or less. When charged frequently, small capacity PHEVs are less expensive to operate and release fewer GHGs, but medium and large capacity PHEVs are more efficient for drivers that charge every 25–100 miles. While statistics on average commute length suggest that frequent charges are possible, answering the question of which PHEV designs will best help to achieve national goals will require a realistic understanding of likely consumer driving and charging behavior as well as future trends in electricity generation."
811,"In this paper, we explore the design of thermoelectric (TE) windows for applications in building structures. "
812,"Component reuse in multiple products has become a popular way to take advantage of the economies of scale across a family of products. Amongst electronic system developers there is a desire to use common electronic parts (chips, passive components, and other parts) in multiple products for all the economy of scale reasons generally attributed to platform design. However, the parts in electronic systems (especially those manufactured and supported over significant periods of time), are subject to an array of long-term lifecycle supply chain disruptions that can offset savings due to part commonality depending on the availability of finite resources to resolve problems on multiple products concurrently. In this paper we address the application of product platform design concepts to determine the best reuse of electronic components in products that are subject to long-term supply chain disruptions such as reliability and obsolescence issues. A detailed total cost of ownership model for electronic parts is coupled with a finite resource model to demonstrate that, from a lifecycle cost viewpoint, there is an optimum quantity of products that can use the same part beyond which costs increase. The analysis indicates that the optimum part usage is not volume dependent, but is dependent on the timing of the supply chain disruptions. This work indicates that the risk and timing of supply chain disruptions should be considered in product platform design."
813,"High dimensionality and computational complexity are curses typically associated with many product family design problems. In this paper, we investigate interactive methods that combine two traditional technologies — optimization and visualization — to create new and powerful strategies to expedite high dimensional design space exploration and product family commonality selection. In particular, three different methods are compared and contrasted: (1) exhaustive search with visualization, (2) individual product optimization with visualization, and (3) product family optimization with visualization. Among these three, the individual product optimization with visualization methods appears to be the most suitable one for engineer designers, who do not have strong optimization background. This method allows designers to “shop” for the best designs iteratively, while gaining key insight into the tradeoff between commonality and individual performance. The study is conducted in the context of designing a UTC product using an in-house, system-level simulation tool. The challenges associated with (1) design space exploration involving mixed-type design variables and infeasibility, and those associated with (2) visualizing "
814,"Market differentiation strategies must identify competitive advantages when offering a line of products varying in features, price, quality, and/or aesthetics. Although this concept is well-known, many companies still have difficulties positioning their own products within their own product lines and against competitors. Few approaches combine two or more facets to answer the product differentiation problem. In this study, two novel indices are proposed to audit shape and functional differentiation within a family of products. The shape index appraises the shape similarity between the products upon digitization, while the functional assessment is based on functions characteristics of the product. Customers’ perception data is obtained experimentally and compared to these indices to validate the result. Pairs of products are evaluated, and the average scores are considered as the indices for a product family. A case study illustrates the usage of these two indices and performance of these tools as well. This approach can be used during detailed studies as well as early stages of the design process to help validate product family positioning."
815,"The development process is a key aspect of ultimate product success. The front-end of the development activity is the foundation for building new products by first gathering customers’ needs, identifying the company’s goals, and assessing the competitive landscape. By doing so, this crucial activity directly impacts eventual development cost, which includes engineering resources, manufacturing, etc. In this paper, we study a specific design approach, namely, product family design, which allows companies to increase revenue by developing an entire family of products targeting different market segments while reducing lead-time and manufacturing costs. However, there is a significant amount of risk given the costs of developing complex shared architectures, and there are many examples from industry where product families have failed. Thus, the development stage is critical, and a well-structured development strategy can bring success while a poor one can cause significant problems during product launch, as recent case studies illustrate. In this platform-based study, we assess two drivers of this product family design: (1) a platform-driven strategy and (2) a product-driven strategy. Three facets are examined: the product, the company, and the competition. The goal is to recommend a planning framework to aid companies in selecting the right process considering their product, strategy, and environment."
816,"Previously, we introduced a new method for improving commonality in a highly customized, low volume product line using component product platforms. The method provides a bottom-up platform approach to redesign family members originally developed one-at-a-time to meet specific customer requirements. In this paper, we extend the method with an Activity-Based Costing (ABC) model to specifically capture the manufacturing costs in the product line, including the cost associated with implementing a platform strategy. The valve yoke example is revisited in this paper, the customized ABC model is defined, two design strategy alternatives are addressed, and the new method is used to determine which alternative is better at resolving the tradeoff between commonality, total cost, and product performance. The proposed method shows promise for creating a product platform portfolio from a set of candidate component platforms that is most cost-effective within an existing product line. The proposed method allows for arbitrary leveraging as it does not rely solely on the traditional vertical, horizontal, or beachhead strategies advocated for the market segmentation grid, and this is especially beneficial when applied to an existing product line that was develop one-at-a-time time such that artifact designs are inconsistent from one to another."
817,"Manufacturers in various industries are seeking to redesign their existing product families to better satisfy their diverse customer needs while maintaining competitive cost structures. Failure to carefully balance the commonality/variety tradeoff during product family redesign will catastrophically hamper the widely sought benefits of both appropriate commonality and variety. Existing product family redesign approaches often focus on increasing the degree of commonality or variety unilaterally and to their utmost, without considering the appropriate commonality/variety tradeoff based on both marketing and engineering resource concerns. The result is redesigned product families that are unachievable or much delayed. In this paper, the Focused Product Family Improvement Method (FPFIM) is proposed to help manufacturers utilize their limited engineering efforts to efficiently respond to market needs using their own competitive focus and commonality/variety tradeoff analysis. This method uses a graphical evaluation tool, the Product Family Evaluation Graph, to determine the necessary direction of improvement for product family redesign — either increasing appropriate commonality or increasing appropriate variety. A set of indices, the Commonality Diversity Index for commonality and variety, support the FPFIM in identifying components with undesirable commonality or undesirable variety, prime targets of redesign to satisfy the redesign intent. To illustrate the proposed method, an example application with four single-use camera families is presented."
818,"Product family design facilitates mass customization by allowing highly differentiated products to be developed around a platform while targeting products to distinct market segments. Therefore, effective platforming of products is a cost-effective way to achieve mass customization The objective in this research is to develop a Strategic Module-based Platform Design Method (SMPDM) to determine a platform design strategy to support product family design in a dynamic and uncertain environment. Ontologies are used to represent products and enable sharing and reuse of design information. Data mining techniques are used to identify a platform and modules by utilizing design information stored in a large database or repository. To determine a platform for family design in dynamic and uncertain market environments, the SMPDM uses agent-based decision-making, involving a market-based negotiation mechanism and a game theoretic approach based on module-based platform concepts and a mathematical model. To demonstrate and validate the usefulness of the proposed method, it is applied to a family of power tools and tested in multiple scenario-based experiments. The SMPDM provides an optimal platform design strategy that can be adapted to various dynamic and uncertain market environments. Therefore, the SMPDM can help develop design strategies to manage and create a cost-effective variety of products based on a platform in support of mass customization."
819,"We consider two questions related to functional part families: a) how to characterize function in a computational framework, and b) how does the structure-to-function model generalize when the design changes, e.g. by changing the set of design variables? For the first, we observe that function is defined on the space of behaviours of the part, whereas structure is defined in the space of design parameters. For mechanical assemblies, as the design parameters change, their effect on the motion parameters can be complex, and cannot be automated in full generality. Thus, the mapping from structure-to-function involves considerable designer knowledge. For computational purposes, we quantify this function by defining part-family-specific Configuration Space (C-space) constructions, and also a metric that operates on these C-spaces to define each function. When the design is changed, either by changing the design space (structure), or by the user expectation (function), can existing design knowledge from the earlier part family migrate to the new product family? We make a start towards exploring how this knowledge can be modified when the part family is evolved, for example by introducing additional design variables, or by changing functional roles. Using examples from several lock designs, we present a small prototype for this process of modeling function and design change, implemented on a commercial CAD engine."
820,"Product family design is a framework for effectively and efficiently meeting with spread customers’ needs by sharing components or modules across a series of products. This paper systematizes product family design toward its extension to throughout consideration of commonalization, customization and lineup arrangement under the optimal design paradigm. That is, commonalization is viewed as the operation that restricts the feasible region by fixing a set of design variables related to commonalized components or modules against later customization and final lineup offered to customers. Customization is viewed as the operation that arranges lineup by adjusting another set of design variables related to reserved freedom for customers’ needs. Their mutual and bi-directional relationships must be a matter of optimal design. This paper discusses the mathematical fundamentals of optimal product family design throughout commonalization, customization and lineup arrangement under active set strategy, and demonstrates a case study with a design problem of centrifugal compressors for showing the meaning of throughout optimal design."
821,"It is challenging to perform probabilistic analysis and design of large-scale structures because it requires repeated finite-element analyses of large models and each analysis is expensive. This paper presents a methodology for probabilistic analysis and reliability-based design optimization of large-scale structures that consists of two re-analysis methods; one for estimating the deterministic vibratory response and another for estimating the probability of the response exceeding a certain level. Deterministic re-analysis can analyze efficiently large-scale finite element models consisting of tens or hundreds of thousand degrees of freedom and large numbers of design variables that vary in a wide range. Probabilistic re-analysis calculates very efficiently the system reliability for different probability distributions of the design variables by performing a single Monte Carlo simulation. The methodology is demonstrated on probabilistic vibration analysis and a reliability-based design optimization of a realistic vehicle model. It is shown that computational cost of the proposed reanalysis method for a single reliability analysis is about 1/20th  of the cost of the same analysis using NASTRAN. Moreover, the probabilistic re-analysis approach enables a designer to perform reliability-based design optimization of the vehicle at a cost almost equal to that of a single reliability analysis. Without using the probabilistic re-analysis approach, it would be impractical to perform reliability-based design optimization of the vehicle."
822,"Uncertainties in material microstructure features can lead to variability in damage predictions based on multiscale microstructure-property models. In this paper, we present an analytical approach for uncertainty analysis by combining a dimension reduction technique for evaluation of statistical moments of a random response, such as damage, with probability distribution fitting based on the extended generalized lambda distribution. This approach is used to analyze the effects of uncertainties pertaining to structure-property relations of an internal state variable plasticity-damage model that predicts failure. Using an un-notched A356 cast aluminum alloy tension specimen as an example, the predictions for damage uncertainty based on the proposed approach are compared with those found using the first order Taylor series approximation and direct Monte Carlo simulation. In particular, the spatial variabilities in microstructural properties, the constitutive model parameter sensitivities, and the effect of boundary condition uncertainties on the damage evolution and final failure are examined. The results indicate that the higher the strain the greater the scatter in damage, even when the uncertainties in the material plasticity and microstructure parameters are kept constant. For A356, the mathematical sensitivity analysis results related to damage uncertainty are consistent with the physical nature of damage progression. At the very beginning, the initial porosity and void nucleation are shown to drive the damage evolution. Then, void coalescence becomes the dominant mechanism. And finally when approaching closer to failure, fracture toughness is found to dominate the damage evolution process."
823,"Reliability-based design of a pressure tank under time-independent random and time-dependent stochastic uncertainties is considered. This pressure tank is an essential element in a reverse osmosis (RO) filtration system for storing filtered water and providing a useable flow rate from the faucet outlet. In this study, we consider the randomness in the welding strength between the upper and lower tanks, and the stochastic pressure applied to the inner surfaces of the tank as the main sources of uncertainty. A pressure tank with 90% reliability against fracture failure is desired. To enable the re-design of the pressure tank, the geometry is parametrized and then used as design variables in a shape optimization scheme. Kriging models are created to approximate the expensive finite element analyses in accessing the performances of each design. The uncertainty model of the welding strength between the upper and lower tanks is found to be well represented by a Gaussian distribution. The stochastic behavior of the pressure loading is modeled by a Markov-chain process. All models are integrated in a reliability-based design optimization problem formulation that has both time-independent and time-dependent reliability constraints. The first passage time and crossover rate are considered in the time-dependent reliability constraint and results of different constraint formulations are compared. The final optimal design satisfies all reliability constraints and reduces the material usage by as many as 46% comparing to the original design."
824,"In a gradient-based design optimization, it is necessary to know sensitivities of the constraint with respect to the design variables. In a reliability-based design optimization (RBDO), the constraint is evaluated at the most probable point (MPP) and called the probabilistic constraint, thus it requires the sensitivities of the probabilistic constraints at MPP. This paper presents the rigorous analytic derivation of the sensitivities of the probabilistic constraint at MPP for both First Order Reliability Method (FORM)-based Performance Measure Approach (PMA) and Dimension Reduction Method (DRM)-based PMA. Numerical examples are used to demonstrate that the analytic sensitivities agree very well with the sensitivities obtained from the finite difference method (FDM). However, since the sensitivity calculation at the true DRM-based MPP requires the second-order derivatives and additional MPP search, the sensitivity derivation at the approximated DRM-based MPP, which does not require the second-order derivatives and additional MPP search to find the DRM-based MPP, is proposed in this paper. A convergence study illustrates that the sensitivity at the approximated DRM-based MPP converges to the sensitivity at the true DRM-based MPP as the design approaches the optimum design. Hence, the sensitivity at the approximated DRM-based MPP is proposed to be used for the DRM-based RBDO to enhance the efficiency of the optimization."
825,"For RBDO problems with correlated input variables, it is necessary to obtain the input joint distribution (CDF, cumulative distribution function). Then Rosenblatt transformation is used to transform the correlated input variables into the independent standard normal variables for the purpose of inverse reliability analysis. However, in practical industry RBDO problems, often only the marginal CDFs and paired samples are available from limited experimental data. In this paper, a copula, which is a link between a joint CDF and marginal CDFs, is proposed to generate an input joint CDF from these marginal CDFs and paired samples. To identify the right copula from limited data, Bayesian method is proposed to use in this paper. Using Bayesian method, the number of samples required to properly identify the right copula is investigated for different types of copulas and for different correlation coefficients. A real industry problem is used to show how a copula can be identified from the limited experimental data."
826,"In this paper, a topology optimization method is constructed for thermal problems with generic heat transfer boundaries in a fixed design domain that includes design-dependent effects. First, the topology optimization method for thermal problems is briefly explained using a homogenization method for the relaxation of the design domain, where a continuous material distribution is assumed, to suppress numerical instabilities and checkerboards. Next, a method is developed for handling heat transfer boundaries between material and void regions that appear in the fixed design domain and move during the optimization process, using the Heaviside function as a function of node-based material density to extract the boundaries of the target structure being optimized so that the heat transfer boundary conditions can be set. Shape dependencies concerning heat transfer coefficients are also considered in the topology optimization scheme. The optimization problem is formulated using the concept of total potential energy and an optimization algorithm is constructed using the Finite Element Method and Sequential Linear Programming. Finally, several numerical examples are presented to confirm the usefulness of the proposed method."
828,"The importance of computer aided engineering (CAE) in product development processes and research has been increasing throughout the past years. Consequently, optimization tools gained more and more importance. In state-of-the-art processes and methods concerning structural optimization it is assumed that there exists a set of external loads or load functions acting on the part. Very often modern products represent complex mechatronic system. The fact that the system’s dynamic properties and its overall behaviour may change due to geometric modifications of a part caused by an optimization process is typically neglected. In order to take into account the interaction between the part, dynamic system, control system and the changing mechanical behaviour with all its consequences for the optimization process, a simulation of the complete mechatronic system is integrated into the optimization process within the research work presented in this paper. A hybrid multibody system (MBS) simulation, that is, a MBS containing flexible bodies, in conjunction with a cosimulation of the control system represented by tools of the Computer Aided Control Engineering (CACE) is integrated into the optimization process. The research work presented in this paper is a contribution towards the integration of existing CAE methods into a continuous process for structural optimization. The benefits will be illustrated by an example, namely a part of the humanoid robot ARMAR III of the collaborative research centre for “Humanoid Robots” [1]. Especially the optimization of two parts at a time within one optimization loop allows an efficient optimization of structures “within” their surrounding mechatronic system."
829,"In this paper, an improved initial random population strategy using a binary (0–1) representation of continuum structures is developed for evolving the topologies of path generating complaint mechanism. It helps the evolutionary optimization procedure to start with the structures which are free from impracticalities such as ‘checker-board’ pattern and disconnected ‘floating’ material. For generating an improved initial population, intermediate points are created randomly and the support, loading and output regions of a structure are connected through these intermediate points by straight lines. Thereafter, a material is assigned to those grids only where these straight lines pass. In the present study, single and two-objective optimization problems are solved using a local search based evolutionary optimization (NSGA-II) procedure. The single objective optimization problem is formulated by minimizing the weight of structure and a two-objective optimization problem deals with the simultaneous minimization of weight and input energy supplied to the structure. In both cases, an optimization problem is subjected to constraints limiting the allowed deviation at each precision point of a prescribed path so that the task of generating a user-defined path is accomplished and limiting the maximum stress to be within the allowable strength of material. Non-dominated solutions obtained after NSGA-II run are further improved by a local search procedure. Motivation behind the two-objective study is to find the trade-off optimal solutions so that diverse non-dominated topologies of complaint mechanism can be evolved in one run of optimization procedure. The obtained results of two-objective optimization study is compared with an usual study in which material in each grid is assigned at random for creating an initial population of continuum structures. Due to the use of improved initial population, the obtained non-dominated solutions outperform that of the usual study. Different shapes and nature of connectivity of the members of support, loading and output regions of the non-dominated solutions are evolved which will allow the designers to understand the topological changes which made the trade-off and will be helpful in choosing a particular solution for practice."
830,"Classical topology optimization aims at achieving a problem suited material distribution in a structure by identification of lightly loaded areas and local element-wise reduction of stiffness. The resulting topologic layout often contains small substructures which are complicated to manufacture, hence requiring an additional manual smoothing during the structural interpretation phase. One major drawback of this approach is that the results still have to be interpreted by an engineer and consequently be translated into a feasible structure. In order to gain a first conceptual yet topologically sound design proposal for composite structures, this paper presents an alternate method for an explicit, pattern based topology modification approach combined with numerical simulation of tape-laying technology. It is assumed that certain patterns exist in stress fields that are extractable by pattern recognition algorithms known from image processing. In the case that prototypical structural reinforcements for such stress patterns can be defined, an automatic topology modification algorithm with the goal of increasing the stiffness is feasible. The classification of these stress patterns is achieved by using dimensionless features matching the stress patterns with their appropriate reinforcements. When integrated into a rule-based conceptual design environment, this explicit topology modification offers the potential to generate simple and easily manufacturable topological reinforcement proposals in an automated structural design loop."
831,"This paper introduces a new methodology for the design of structures by geometry and topology optimization accounting for loading and boundary conditions as well as material properties. The Fuzzy Heuristic Gradient Projection (FHGP) method is used as a direct search technique for the geometry optimization, while the Complex Method (CM) is used as a random search technique for the topology optimization. In the proposed method, elements are designed such that they all have the same amount of stresses using the Fuzzy Heuristic Gradient Projection method. On the other hand, the complex method is used for the topology optimization step satisfying any constraint other than the stress constraint. The developed hybrid fuzzy technique is applied for different applications ranging from micro-scale to macro-scale applications. The method is applied to a micro-mechanical resonator as a microelectro-mechanical system (MEMS). The resonator is solved for minimum weight and is subjected to an equality frequency constraint and an inequality stress constraint. The proposed method is compared with the Multi-objective Genetic Algorithms (MOGAs) on solving the MEMS resonator. Results showed that the proposed hybrid fuzzy technique converges to optimum solutions faster than (MOGAs). The time consumed is improved by a 77%."
832,"In optimization problems that aim to minimize noise, elastic structures have been designed so that fundamental eigenfrequencies depart from excitation frequencies. Moreover, for the sake of simplicity, sound pressure responses have rarely been calculated. In this paper, we propose a new topology optimization method for the design of poroelastic material layouts that minimize sound pressure levels by sound attenuation. In this method, the surrounding air is exactly modeled, and poroelastic material is located in a space filled with air to efficiently dissipate power. The Biot’s theory is incorporated into the optimization scheme to deal with poroelastic material, and we utilize a new bi-material continuum that consists of poroelastic material combined with an equivalent representation of air in the Biot’s theory. Several design problems are presented to demonstrate that the proposed method can provide optimal layouts of poroelastic material that reduce sound pressure levels within specified frequency ranges."
833,"Compliant mechanisms are a new type of mechanism, designed to be flexible to achieve a specified motion as a mechanism. Such mechanisms can function as compliant thermal actuators in Micro-Electro Mechanical Systems (MEMS) by intentionally designing configurations that exploit thermal expansion effects in elastic material when appropriate portions of the mechanism structure are heated. This paper presents a new structural optimization method for the design of compliant thermal actuators based on the level set method and the Finite Element Method (FEM). First, an optimization problem is formulated that addresses the design of compliant thermal actuators considering the magnitude of the displacement at the output location. Next, the topological derivatives that are used when introducing holes during the optimization process are derived. Based on the optimization formulation and the level set method, a new structural optimization algorithm is constructed that employs the FEM when solving the equilibrium equations and updating the level set function. The re-initialization of the level set function is performed using a newly developed geometry-based re-initialization scheme. Finally, several design examples are provided to confirm the usefulness of the proposed structural optimization method."
834,"The problem of integrating topological optimization tools in product development process (PDP) is becoming more and more urgent since nowadays they are widely employed in several engineering fields (civil, aeronautics, aerospace, automotive). The interest for these tools is due to their capacity to better mechanical properties through a global optimization of the product in terms of weight, stiffness, resistance and cost. In particular, there is a lack of specific tools for automatic feature recognition on voxel models generated by the topological optimization tools. Our paper presents an innovative methodology that allows the integration of topological optimizers in the product development process by means of a wise and rational knowledge management and an efficient data exchange between different systems. The target has been reached through the implementation of CAD automation modules which decrease the working time and give the possibility to effectively schematize the designer’s knowledge."
836,"Most structural products have complex geometry to meet customer’s demand of high functionality. Since manufacturing those products in one piece is either impossible or uneconomical, most structural products are assemblies of components with simpler geometries. The conventional way to design structural assemblies is to design overall geometry first, and then decompose the geometry to determine the part boundary and joint locations. This two-step process, however, can lead to sub-optimal designs since the product geometry, even if optimized as one piece, would not be optimal after decomposition. This paper presents a method for synthesizing structural assemblies directly from the design specifications, without going through the two-step process. Given an extended design domain with boundary and loading conditions, the method simultaneously optimizes the topology and geometry of an entire structure and the location and configuration of joints, considering structural performance, manufacturability, and assembleability. As a relaxation of our previous work utilizing a beam-based ground structure [1], this paper presents a new formulation in a continuum design domain, which greatly enhances the ability to represent complex structural geometry observed in real-world products. A multi-objective genetic algorithm is used to obtain Pareto optimal solutions that exhibits trade-offs among stiffness, weight, manufacturability, and assembleability."
837,"The excellent mechanical properties of laminated composites cannot be exploited without a careful design of stacking sequence of the layers. An important variable in the search of the optimum stacking sequence is the number of layers. The larger is this number, the harder as well as longer is the search for an optimal solution. To tackle efficiently such a variable-dimensional problem, we introduce here a multi-level optimization technique. The proposed method, called Layer Separation (LS), increases or decreases the number of layers by gradually separating a layer into two, or by merging two layers into one. LS uses different levels of laminate representation ranging from a coarse level parameterization, which corresponds to a small number of thick layers, to a fine level parameterization, which corresponds to a large number of thin layers. A benefit of such differentiation is an increase of the probability of finding the global optimum. In this paper, LS is applied to the design of composite laminates under single and multiple loadings. The results show that LS convergence rate is not inferior to that of other optimization techniques available in the literature. It is faster than an evolutionary algorithm, more efficient than a layerwise method, simple to perform, and it has the advantage of possibility of termination at any point during the optimization process."
838,"Several model validation and prediction error modeling techniques are studied and compared in this paper to help establish stopping criteria and identify critical regions in the design space in a sequential sampling framework. This study leads to the proposal of a two-phase sequential sampling and meta-modeling strategy, which is realized by the support of a multi-dimensional data visualization tool. These techniques have been successfully applied in the development and setup of a system-level parametric tool to support Heating, Ventilating, and Air Conditioning design. Maintaining the same level of accuracy, we observe a savings of 6–30 times the simulation effort needed for current practice. The benefits and drawbacks of the method are discussed, and opportunities are identified for future improvement."
839,"Statistical analysis of functional responses based on functional data from both computer and physical experiments has gained increasing attention due to the dynamic nature of many engineering systems. However, the complexity and huge amount of functional data bring many difficulties to apply traditional or existing methodologies. The objective of the present study is twofold: (1) prediction of functional responses based on functional data and (2) prediction of bias function for validation of a computer model that predicts functional responses. In this paper, we first develop a functional regression model with linear basis functions to analyze functional data. Then combining data from both computer and physical experiments, we use the functional analysis modeling to predict the bias function which is crucial for validating a computer model. The proposed method, following the classical nonparametric regression framework, uses a single step procedure which is easily implemented and computationally efficient. Through an application example of motor engine analysis to predict acceleration performance and gear shift events, we demonstrate our approach and compare it to using the Gaussian process modeling approach."
840,"Computer modeling and simulation are the cornerstones of product design and development in the automotive industry. Computer-aided engineering tools have improved to the extent that virtual testing may lead to significant reduction in prototype building and testing of vehicle designs. In order to make this a reality, we need to assess our confidence in the predictive capabilities of simulation models. As a first step in this direction, this paper deals with developing a metric to compare time histories that are outputs of simulation models to time histories from experimental tests with emphasis on vehicle safety applications. We focus on quantifying discrepancy between time histories as the latter constitute the predominant form of responses of interest in vehicle safety considerations. First we evaluate popular measures used to quantify discrepancy between time histories in fields such as statistics, computational mechanics, signal processing, and data mining. Then we propose a structured combination of some of these measures and define a comprehensive metric that encapsulates the important aspects of time history comparison. The new metric classifies error components associated with three physically meaningful characteristics (phase, magnitude and topology), and utilizes norms, cross-correlation measures and algorithms such as dynamic time warping to quantify discrepancies. Two case studies demonstrate that the proposed metric seems to be more consistent than existing metrics. It is also shown how the metric can be used in conjunction with ratings from subject matter experts to build regression-based validation models."
841,"The optimization of Reel-Cutting Planning Problem “RCPP” is concerned with finding the best selection of a strategic reel set and the corresponding tactical cutting lengths “sheets set”, from a wide feasible space, to be used in producing a set of blanks. The problem is classified as a weighted multi objectives two-staged guillotine two-dimensional Cutting Stock Problem “CSP” of Dyckoff’s type 2/V/D/M. This paper presents a tailored metaheuristic solution using grouping genetic algorithms (GGAs). Overlapped Chromosome representation is newly developed for optimization. Developed also an especial database to create the cutting patterns of resulting solution. The objective in this application is to minimize the total cost, which includes the total material cost required to achieve a lot size, the cost of selecting reels set size, and the cost of cutting lengths from these reels “sheet set size”. An industrial case study is considered. The attained optimum cutting plan provides an overall improvement of 4.3% over the current professional cutting plan. The reduction in the reel set size is from five to an optimum of three reels. The developed procedure also provides reduction of trim loss of 4% over the current plan. The developed approach also proved to be faster than the currently used techniques."
842,"This paper discusses a perspective of hierarchical layout design optimization for highly packaged equipments and demonstrates an implementation of an optimization algorithm with a simplified case study. First, the Pareto optimality of subsystem-level shape design against the optimality of system-level shape design is extracted through two-level hierarchical formulation of layout design problems. Then, a computational design algorithm is developed for a class of two-dimensional layout design problems of rectangles, some of which are the results of similar problems defined in its sub-levels. The algorithm represents the layout topology with sequence-pair and the shape of each module or component with the aspect ratio, and optimizes them with genetic algorithms. The Pareto optimality of sub-levels is handled with the functionality of multi-objective optimization of genetic algorithms, in which a set of Pareto are simultaneously generated. Top-level and sub-level layout problems are coordinated through exchange of preferable ranges of shapes and layout. A case study is explored under the developed algorithm. The promises and limitations of the proposed framework is briefly discussed for defining the future works."
843,"A novel approach using a genetic algorithm is presented for extracting globally satisfycing (Pareto optimal) solutions from a morphological chart where the evaluation and combination of “means to sub-functions” is modeled as a combinatorial multi-objective optimization problem. A fast and robust genetic algorithm is developed to solve the resulting optimization problem. Customized crossover and mutation operators specifically tailored to solve the combinatorial optimization problem are discussed. A proof-of-concept simulation on a practical design problem is presented. The described genetic algorithm incorporates features to prevent redundant evaluation of identical solutions and a method for handling of the compatibility matrix (feasible/infeasible combinations) and addressing desirable/undesirable combinations. The proposed approach is limited by its reliance on the quantifiable metrics for evaluating the objectives and the existence of a mathematical representation of the combined solutions. The optimization framework is designed to be a scalable and flexible procedure which can be easily modified to accommodate a wide variety of design methods that are based on the morphological chart."
845,"Research applications involving design tool development for multiple phase material design are at an early stage of development. The computational requirements of advanced numerical tools for simulating material behavior such as the finite element method (FEM) and the molecular dynamics method (MD) can prohibit direct integration of these tools in a design optimization procedure where multiple iterations are required. The complexity of multiphase material behavior at multiple scales restricts the development of a comprehensive meta-model that can be used to replace the multiscale analysis. One, therefore, requires a design approach that can incorporate multiple simulations (multi-physics) of varying fidelity such as FEM and MD in an iterative model management framework that can significantly reduce design cycle times. In this research a material design tool based on a variable fidelity model management framework is presented. In the variable fidelity material design tool, complex “high fidelity” FEM analyses are performed only to guide the analytic “low-fidelity” model toward the optimal material design. The tool is applied to obtain the optimal distribution of a second phase, consisting of silicon carbide (SiC) fibers, in a silicon-nitride (Si3 N4 ) matrix to obtain continuous fiber SiC-Si3 N4  ceramic composites (CFCCs) with optimal fracture toughness. Using the variable fidelity material design tool in application to one test problem, a reduction in design cycle time around 80 percent is achieved as compared to using a conventional design optimization approach that exclusively calls the high fidelity FEM."
846,"Concept designs synthesized using conventional topology optimization methods are typically not easily manufacturaed, in that multiple finishing processes are required to construct the component. A manufacturing technique that requires only minimal effort is extrusion. Extrusion is a manufacturing process used to create objects of a fixed cross-sectional profile. Extrusion often minimizes the need for secondary machining, although not necessarily of the same dimensional accuracy as machined parts. The result of using this process is lower costs for the manufacture of the final product. In this paper, a non-gradient hybrid cellular automaton (HCA) algorithm is developed to synthesize constant cross section structures that are subjected to nonlinear transient loading. Examples are presented to demonstrate the efficiency of the proposed methodology in synthesizing these structures. The methodology is first demonstrated for elastic-static modeling. The novelty of the proposed method is the ability to generate constant cross section topologies for plastic-dynamic problems since the issue of complex gradients can be avoided using the HCA method."
847,"Component selection in engineering design is a process in which an assembly of pre-defined component types is given and a choice of specific components is desired that satisfies a set of design requirements and constraints. Although algorithmic approaches to component selection have been researched for specific mechanical engineering problems such as bearing selection, a generalized technique has not been developed. This paper proposes a universal tool to automate the process of component selection by incorporating a tree search. Our technique evaluates the worth of candidate solutions in terms of the customer needs satisfaction and the compatibility between interconnected components. The tree search technique used in this research is not only quick and efficient but also guarantees an optimal solution."
848,"Neural networks are increasingly becoming a useful and popular choice for process modeling. The success of neural networks in effectively modeling a certain problem depends on the topology of the neural network. Generating topologies manually relies on previous neural network experience and is tedious and difficult. Hence there is a rising need for a method that generates neural network topologies for different problems automatically. Current methods such as growing, pruning and using genetic algorithms for this task are very complicated and do not explore all the possible topologies. This paper presents a novel method of automatically generating neural networks using a graph grammar. The approach involves representing the neural network as a graph and defining graph transformation rules to generate the topologies. The approach is simple, efficient and has the ability to create topologies of varying complexity. Two example problems are presented to demonstrate the power of our approach."
849,"Advanced internal combustion engine technologies have increased the number of accessible variables of an engine and our ability to control them. The optimal values of these variables are designated during engine calibration by means of a static correlation between the controllable variables and the corresponding steady-state engine operating points. While the engine is running, these correlations are being interpolated to provide values of the controllable variables for each operating point. These values are controlled by the electronic control unit to achieve desirable engine performance, for example in fuel economy, pollutant emissions, and engine acceleration. The state-of-the-art engine calibration cannot guarantee continuously optimal engine operation for the entire operating domain, especially in transient cases encountered in driving styles of different drivers. This paper presents the theoretical basis and algorithmic implementation for allowing the engine to learn the optimal set values of accessible variables in real time while running a vehicle. Through this new approach, the engine progressively perceives the driver’s driving style and eventually learns to operate in a manner that optimizes specified performance indices. The effectiveness of the approach is demonstrated through simulation of a spark ignition engine, which learns to optimize fuel economy with respect to spark ignition timing, while it is running a vehicle."
850,"Faithfully obtaining design specifications from customer requirements is essential for successful designs. The natural lingual, inexact, incomplete and vague attributes of customer requirements make it very difficult to map customer requirements to design specifications. In general design process, the design specifications are determined by designers based on their experience and intuition, and often a certain target value is set for a specification. However, it is on one hand very difficult, on the other hand unreasonable, so a suitable limit range rather than a certain value is preferred at the beginning of design, especially at the concept design process. In this paper, a simplified systematic approach of transforming customer requirements to design specifications is proposed. First, a two-stepped clustering approach for grouping customer requirements and design specifications based on HOQ matrix is presented, by which the mapping is limited to within each group. To further simplify the inference mapping rules of customer requirements and design specifications, the minimal condition inference mapping rules for each design specification are extracted based on rough set theory. In the end, a suitable value range is determined for a specification by applying the fuzzy rule matrix."
851,"Functionality is one of the key aspects of artifact models for design. A function of a device, however, can be captured in different ways in different domains or by different model-authors. Much research on functions has been conducted in the areas of engineering design, functional representation and philosophy, although there are several definitions and notions of functions. We view conceptualization of function is multiplicative in nature: different functions can be captured simultaneously from an objective behavior of an artifact under different teleological contexts of users/designers, or from different viewpoints (perspectives) of a model-author. Such differences become problematic for sharing functional knowledge among engineers. In this article, we attempt to clarify the differences of such perspectives for capturing functions on the basis of the ontological engineering. On the basis of a generalized model of the standard input-output model in the well-known systematic design methodology, we show descriptive categorization of some upper-level types (classes) of functions with references to some definitions of functions in the literature. Such upper-level ontological categories of functions are intended to be used as a reference ontology for functional knowledge interoperability. One of the two usages here is to convert functional models between different functional taxonomies. A functional term in a taxonomy is (ideally) categorized into a generic type defined in the reference ontology. It is widely recognized in the literature that such an upper-level ontology helps automatic “mapping discovery” which is to find similarities between two ontologies and determine which concepts represent similar notion. The reference ontology of function might have such an effect. Another usage of the reference ontology is to integrate fault knowledge into functional knowledge and automatic transformation of FMEA sheets. The designer can describe an integrated model of both functional knowledge and fault knowledge. Based on ontology mappings, automatic transformations of FMEA sheets can be realized. In this article, we discuss the detail of the definitions of the upper-level categories of functions ontologically. Then, we give an overview of usages and effects of the upper-level categories as a reference ontology for functional knowledge interoperability."
852,"Assembly planning is one of the NP complete problems, which is even harder to solve for complex products. Furthermore, development of complex products has been becoming a collaborative activity among different enterprises at the same or different sites. Collaboration among designers is considered as an effective strategy to tackle the difficulty of assembly planning. According to the strategy, the task of assembly planning of complex products is decomposed into several simpler subtasks related to portions of the whole product, and the subtasks are then assigned to the designers in collaboration. In this paper, assembly unit partition is addressed, which means the decomposition of a complex product into smaller assembly units. Assembly unit partition is more complex than subassembly extraction (or identification) because more assembly constraints are taken into consideration. Firstly, different and necessary assembly constraints are analyzed in detail and their related evaluation indices are given for assembly unit partition. Then, the assembly unit model and the decision model of assembly unit partition are proposed. A method of assembly unit partition based on the decision model and Fuzzy Analytical Hierarchy Process (FAHP) method is clarified. The valid assembly units can be determined by analyzing the decision values of assembly relations between two parts and the given conditions. Finally, the validity and effectiveness of the method is verified with an example."
853,"Shorter product life cycles, growing product complexity and the need for a large number of product variants have made Product Data Management (PDM) increasingly important for many manufacturing industries. Since, many industries in India either have implemented PDM or are considering implementation, it is the right time to study the real impact of PDM in Indian industries. This impact study was made by the responses obtained through a survey questionnaire. The main objective of the survey is to study the impact of PDM implementation on productivity. The variables of productivity measurement in terms of Key Performance Areas have been identified using Performance Objective-Productivity model. The questionnaire has been designed and administered and the responses received have been analysed using SPSS software. General descriptive statistics including mean and standard deviation along with paired t-test and Pearson’s correlation studies have been employed for analysing the importance of the measures of productivity. A user driven PDM evaluation framework and methodology based on the two stage Quality Function Deployment (QFD) technique has been developed to select the most suitable PDM product for a specific industry. A simple model to measure the Return on Investment for PDM implemented industries based on Port and Mackrell’s [24] work has been developed. Thus, methodologies have been developed and demonstrated with case studies to overcome the issues identified in the survey."
854,"This paper presents a system framework to support the collaborative top-down assembly design. As the first step, the framework is devised to enable distributed designers to conduct collaborative layout design, 3D skeleton design and detailed design of a product in a top-down manner in a distributed environment. To effectively support the collaborative top-down assembly design, a multi-resolution and distributed product assembly model is proposed as a product representation in the framework. With the help of the framework the designers could conduct those design activities of top-down assembly design that need plenty of collaboration such as the collaborative determination of assembly relationship and coupled structural parameters. A variation propagation mechanism is also developed to guarantee the consistency of the distributed assembly model. A preliminary system prototype based on replicated client-server architecture is implemented."
855,"In most instances it is necessary to partition a product realization process into a set of design activities. Design decisions are an important type of design activity. In order to ensure that information flows among different design activities are achieved without difficulty, uniform representation of the information is necessary. A design activity template and a solution template are presented in this paper to support the information flow in a design process. Activity template provides an option to describe the design problem so that engineers know exactly what kind of design decision is preferable. Solution template provides an option to describe the design solution. The paper also introduces an approach to manage and deliver design freedom from one engineering team to the other. In mechanical design, it is essential to give engineers a feasible design space so that they can choose the design solutions that best satisfy the design problem. A chip design example is used to illustrate the different concepts presented in this paper for collaborative decision making."
856,"The design of large scale complex engineering systems requires interaction and communication between multiple disciplines and decentralized subsystems. One common fundamental assumption in decentralized design is that the individual subsystems only exchange design variable information and do not share objective functions or gradients. This is because the decentralized subsystems can either not share this information due to geographical constraints or choose not to share it due to corporate secrecy issues. Game theory has been used to model the interactions between distributed design subsystems and predict convergence and equilibrium solutions. These game theoretic models assume that designers make perfectly rational decisions by selecting solutions from their "
857,"Laptop computers are designed in a variety of shapes and sizes in order to satisfy diverse consumer preferences. Each design is optimized to attract consumers with a particular set of preferences for design tradeoffs. Gaining a better understanding of these tradeoffs and preferences is beneficial to both laptop designers and to consumers. This paper introduces an engineering model for laptop computer design and a demand model derived from a main-effects choice-based conjoint survey. Several demand model specifications are compared, including linear-in-parameters and discrete part-worth specifications for aggregate multinomial logit and mixed logit models. An integrated optimization scheme combines the engineering model with each demand model form for profit maximization. The solutions of different optimal laptop designs and market share predictions resulting from the unique characteristics of each demand model specification are examined and compared."
858,"Recent research has extended prior efforts to integrate firm-level objectives into engineering design optimization models by further enlarging the scope to investigate the effects of regulation on the design decisions of profit-seeking firms in competition. In particular, one study examined the effects of environmental policy on vehicle design decisions by integrating quantitative models of engineering performance, market demand, production cost and regulatory penalties in a joint optimization framework using game theory to model the effects of competition on design and pricing. Model complexity and the solution methods used to solve for market equilibria in prior research have led to a limitation where the prior approach is too computationally intensive to allow extensive parametric studies on the effects of policy changes on design. To address this issue, we present an alternative game-theoretic approach utilizing necessary and sufficient conditions with Nash conditions to find market equilibria in an oligopoly of automakers, and we use this approach to examine the resulting optimal design responses under various regulation scenarios."
859,"The measurement and understanding of user aesthetic preference for form is a critical element to the product development process and has been a design challenge for many years. In this article preference is represented in a utility function directly related to the engineering representation for the automobile headlight. A method is proposed to solicit and measure customer preferences for shape of the automobile headlight using a choice task on a main-effects conjoint survey design to discover and design the most preferred shape."
860,"This paper presents a methodology, Conjoint-HoQ, which is meant to provide an improvement over the current House of Quality (HoQ) tool. The improvement comes in the form of "
861,"Research from behavioral psychology and experimental economics asserts that individuals construct preferences on a case-by-case basis when called to make a decision. A common, implicit assumption in engineering design is that user preferences exist a priori. Thus, preference elicitation methods used in design decision making can lead to preference inconsistencies across elicitation scenarios. This paper offers a framework for understanding preference inconsistencies, within and across individual users. We give examples of three components of this new framework: comparative, internal, and external inconsistencies across users. The examples demonstrate the impact of inconsistent preference construction on common engineering and marketing design methods, including discrete choice analysis, modeling stated vs. revealed preferences, and the Kano method and thus QFD. Exploring and explaining preference inconsistencies produces new understandings of the relationship between user and product."
862,"This paper introduces the theoretical design and control of reconfigurable parallel kinematic machine tools. First, the general concept of reconfigurable parallel kinematic machine (RPKM) and its growing demand are introduced. Second, the design of reconfigurable parallel kinematic machines is discussed and the geometric modeling of such structures is presented and explained. The potential applications of this type of machine are described. Finally, a case study for one of the proposed structures is conducted, including kinematic/dynamic modeling and control, some results and simulation are demonstrated."
863,"Adaptable design is a new design paradigm to create designs and products that can be easily changed to satisfy different requirements. Adaptable design aims at identifying the designs and products considering functionality, manufacturing efforts, customization and environment friendliness. This research focuses on adaptable design considering product adaptability. In this work, product adaptability is evaluated by three measures including extendibility of functions, upgradeability of modules, and customizability of components. Various design candidates created in adaptable design are evaluated by different life-cycle evaluation measures including product adaptability of design, part and assembly costs of manufacturing, and operationability by customers. Since different evaluation measures are modeled in different units, the grey relational analysis method is employed to integrate the different evaluation measures for prioritizing different design candidates. A case study is given to demonstrate the effectiveness of the introduced adaptable design approach."
864,"The demands on today’s products have become increasingly complex as customers expect enhanced performance across a variety of diverse and changing system operating conditions. Reconfigurable systems are capable of undergoing changes in order to meet new objectives, function effectively in varying operating environments, and deliver value in dynamic market conditions. Research in the design of such responsive and changeable systems, however, currently faces impediments in effective and clear discourse due to ambiguity in terminology. Definitions of the terms flexibility and reconfigurability, two related concepts in reconfigurable system design, are explored based on their original lexical meanings and current understanding in design literature. Design techniques that incorporate flexibility both in the design (form) and performance (function) space are presented. Based upon this literature survey, a classification scheme for flexibility is proposed, and its application to reconfigurable system design is explored. This paper also presents recent methodologies for reconfigurable system design and poses important research questions that remain to be investigated."
865,"Box’s Complex method for direct search has shown promise when applied to simulation based optimization. In direct search methods, like Box’s Complex method, the search starts with a set of points, where each point is a solution to the optimization problem. In the Complex method the number of points must be at least one plus the number of variables. However, in order to avoid premature termination and increase the likelihood of finding the global optimum more points are often used at the expense of the required number of evaluations. The idea in this paper is to gradually remove points during the optimization in order to achieve an adaptive Complex method for more efficient design optimization. The proposed method shows encouraging results when compared to the Complex method with fix number of points and a quasi-Newton method."
866,"Computer analysis and simulation based design optimization requires more computationally efficient global optimization tools. In this work, a new global optimization algorithm based on design experiments, region elimination and response surface model, namely Approximated Unimodal Region Elimination Method (AUREM), is introduced. The approach divides the field of interest into several unimodal regions using design experiment data; identify and rank the regions that most likely contain the global minimum; form a response surface model with additional design experiment data over the most promising region; identify its minimum, remove this processed region, and move to the next most promising region. By avoiding redundant searches, the approach identifies the global optimum with reduced number of objective function evaluations and computation effort. The new algorithm was tested using a variety of benchmark global optimization problems and compared with several widely used global optimization algorithms. The experiments results present comparable search accuracy and superior computation efficiency, making the new algorithm an ideal tool for computer analysis and simulation black-box based global design optimization."
867,"This paper considers the problem of achieving improvements through adaptive experimentation. To limit the focus we consider only design spaces with discrete two-level factors. We prove that, in a Bayesian framework, one factor at a time experimentation is an optimally efficient response to step by step accrual of sample information. We derive Bayesian predictive distributions for experimentation outcomes given natural conjugate priors. Using an example based on fatigue life of weld repaired castings, we show how to use our results."
868,"The current work discusses a novel global optimization method called the Multi-Agent Normal Sampling Technique (MANST). MANST is based on systematic sampling of points around agents; each agent in MANST represents a candidate solution of the problem. All agents compete with each other for a larger share of available resources. The performance of all agents is periodically evaluated and a specific number of agents who show no promising achievements are deleted; new agents are generated in the proximity of those promising agents. This process continues until the agents converge to the global optimum. MANST is a standalone global optimization technique. It is benchmarked with six well-known test cases and the results are then compared with those obtained from Matlab™ 7.1 GA Toolbox. The test results showed that MANST outperformed Matlab™ 7.1 GA Toolbox for the benchmark problems in terms of accuracy, number of function evaluations, and CPU time."
869,"This paper presents a new multi-objective optimization method, which is inspired from the idea of non-dominated sorting genetic algorithm (NSGA) and genetic quantum algorithm (GQA). The GQA has been tested on well known test beds in single objective optimization and compared with the genetic algorithm (GA) in the lead author’s previous work [22]. This paper aims to apply the idea of GQA to multi-objective optimization (MOO). The developed method is called non-dominated sorting genetic quantum algorithm (NSGQA). The developed method is tested with benchmark problems collected from literature, which have characteristics representing various aspects of a MOO problem. Test results show that NSGQA has better performance on most benchmark problems than currently popular MOO methods such as the NSGA. The integration of GQA with MOO, and the systematic comparison with other MOO methods on benchmark problems, should be of general interest to researchers on MOO and to practitioners using MOO methods in design."
870,"Shock from impact loading may risk the lives of the occupants of a military vehicle and damage the sensitive electronic components within it. A finite element model (FEM) for a space-frame based military vehicle is presented in this paper. An approach is developed to optimize the design of the joints within the space frame structure to reduce the mass of the vehicle while maintaining its structural integrity. The process starts by creating a parametric FEM of the vehicle. The optimization variables are the lengths of joint branches. The effect of joint location within the space frame is also explored. The problem is subject to geometry and stress constraints. Results show that a mass reduction can be achieved without adversely affecting integrity of the vehicle."
871,"A bicycle frame is optimized for the lightest weight by using genetic algorithms in this study. Stresses of five rods in the bicycle frame less than the material yielding strength with consideration of the factor of safety are the constraints. A two-dimensional model of the frame is created. Equilibrium equations are derived and loads acting on rods are determined. A known function is used to verify feasibility of the program generated. Effects of the mutation rate, the crossover rate and the number of generation on the mean and the standard deviation of the fitness value are studied. The optimal solutions with the outer diameters and the inner diameters of the front frame rods to be 0.040 m and 0.038 m, respectively, the outer diameters and the inner diameters of the rear frame rods to be 0.024 m and 0.021m, respectively, and the weight of the bicycle frame to be 0.896 kg are recommended for the bicycle frame."
872,"A particularly challenging problem in CAD/CAE is the handling of small geometric details during finite element analysis (FEA). The presence of such details can significantly increase the computational complexity of FEA, while hindering its automation. Therefore, designers typically resort to "
873,"This paper addresses the critical issue of effectiveness, efficiency, and reliability in simulation-based design optimization under surrogate model uncertainty. Specifically, it presents a novel method to build surrogate models iteratively with sufficient fidelity for accurately capturing global optimal design solutions at a minimal cost. The salient feature of the proposed method lies in its unique preference of focusing necessarily high fidelity at potential global optimal regions of surrogate models. The proposed method is the synergic integration of the multiple preference point method, which updates surrogate model at current local optimal points predicted with data-mining techniques in genetic algorithm setup, and the maximum variance point method, which updates surrogate model at the point associated with the maximum prediction variance. Through illustrative comparison studies on thirty different optimization scenarios derived from 15 different test functions, the proposed method demonstrates the tangible reliability advancement. The experimental results indicate that the proposed method can be a reliable updating method in surrogate-model-based design optimization for efficiently locating the global optimal point/points in various kinds of optimization scenarios featured by single/multiple global optimal point/points that may exist at the corners of design space, inside design space, or on the boundaries of design space."
874,"This paper describes a multilevel, multistage approach to system of systems design optimization where a system design is linked with system allocation along the multistage decision making horizon. The approach is composed of two parts: pseudo-hierarchical formulation (i.e., how to model the "
875,"This paper has introduced the finite element analysis (FEA) into the ergonomic design to evaluate the human feelings numerically and objectively, and then into the optimization design of beverage containers considering human factors. In the design of the end of can (the lid of can), experiments and the FEA of indenting vertically the fingertip pulp by a probe and the tab of end have been done to observe force responses and to study feelings in the fingertip. A numerical simulation of finger lifting the tab for opening the can has also been performed, and discomfort in the fingertip has been evaluated numerically to present the finger-accessibility of the tab. The comparison of finger-accessibility between two kinds of tab ring shape designs showed that the tab that may have a larger contact area with the finger is better. In the design of beverage bottles served hot drinks, the FEA of tactile sensation of heat has been performed to evaluate numerically the touch feeling of the finger when holding the hot bottle. The numerical simulations of embossing process have also been performed to evaluate the formability of various rib-shape designs. The optimum design has then been done considering the hot touch feeling as well as the metal sheet formability."
876,"In the design of artifacts that interact with people, the spatial dimensions of the user population are often used to size and engineer the artifact. The variability in body dimensions (called “anthropometry”) is used to indicate how much adjustability or how many sizes are required to accommodate the intended user population. However, anthropometry is not the only predictor of these kinds of interactions. For example, two vehicle drivers with similar body dimensions might have very different preferred locations for the seat. The variability not predicted by body dimensions can be considered “preference”. Well-conceived models considering all sources of variability can can facilitate the application of design automation tools such as optimization and robust design methodologies, resulting in products that are safer, cost effective, and more accessible to broader populations (including people with disabilities). In contrast, poor models and those that fail to include a preference component can produce misleading results that under- or over-approximate accommodation and prescribe inappropriate amounts of adjustability. This paper reviews common methods of designing for human variability, demonstrating the use and strengths and weaknesses of each. This is done in the context of a simple, univariate case study to determine the appropriate allocation of adjustability to achieve a desired accommodation level."
877,"This paper has investigated effects of the bottle opening size on drinking feelings in order to improve the comfort level of consumers when drinking directly from the opening of aluminum bottle. A survey over 120 subjects has been performed based on a drinking test using three kinds of bottles with opening diameters of 28, 33 and 38 mm, respectively. Two questionnaires have been conducted. Statistical analysis results of Questionnaire 1 have shown that 33-mm opening is best for adult consumers with no matter the type of contents, gender and the mouth size. The factor analysis results of Questionnaire 2 based on Kansei Engineering have shown that drinking feeling is affected by two common factors, which considered as the flow from the bottle to the mouth and the flow adjustability. Moreover, the fluid-dynamics analysis model has been developed to simulate the bottled liquid in a drinking action consisting of survey results and experimental observations of consumers’ drinking actions. Numerical simulations have been performed to understand how consumers control the flow during the drinking actions. It is found that the consumers usually try to realize the ideal and preferable condition by adjusting the inclination angle of the bottle."
878,"Coverage toolpath planning is very critical to deposition quality in layered manufacturing especially for metal deposition processes. The correct choice of toolpath patterns will make it possible to build a fully dense and functional metal part. The major consideration when selecting a toolpath pattern is the complete coverage of the to-be-deposited geometry which means no voids should happen. This paper presents the research on the toolpath coverage efficiency and the strategies to predict the possibility of the occurrence of deposition voids so that the appropriate toolpath pattern can be applied to avoid deposition voids. The contour-parallel offsetting pattern and the adaptive zigzag toolpath pattern will be applied as the alternate options and the final adaptive deposition coverage toolpath will be the combination of these two basic patterns depending on the prediction results of the occurrence of the deposition voids. The experiment has demonstrated that the adaptive toolpath pattern can greatly improve the reliability of the coverage path planning in deposition processes."
879,"The design of any MEMS component is subject to stringent manufacturing constraints. The knowledge about these constraints seems to be available to designers who have experienced the details of MEMS fabrication. In this paper, we put forth the idea of automatically generating a fabrication sequence for surface micro-machined MEMS components using the knowledge stored in grammar rules. As an analogy to CAD tools used in mechanical systems, we envision creating a tool which has the Pro-Engineer approach of determining fabrication sequences for a machine tool based on the final part shape. This tool could be an integral addition to the current MEMS design software so that the designers can freely draft devices and then allow an automated process to determine the fabrication sequence. In this paper we give a brief introduction about the graph grammars. Data from already designed MEMS components is extracted in the form of rules to create an expert system. We have also included an example of generation of the fabrication sequence for several MEMS components."
880,"Rapid advancement of 3D sensing techniques has lead to dense and accurate point cloud of an object to be readily available. The growing use of such scanned point sets in product design, analysis and manufacturing necessitates research on direct processing of point set surfaces. In this paper, we present an approach that enables the direct layered manufacturing of point set surfaces. This new approach is based on adaptive slicing of moving least squares (MLS) surfaces. Salient features of this new approach include: 1) it bypasses the laborious surface reconstruction and avoids model conversion induced accuracy loss; 2) the resulting layer thickness and layer contours are adaptive to local curvature and thus it leads to better surface quality and more efficient fabrication; 3) the MLS surface naturally smoothes the point cloud and allows up-sampling and down-sampling, and thus it is robust even for noisy or sparse point sets. Experimental results of the slicing algorithm on both synthetic and scanned point sets are presented."
881,"Current reverse engineering approach is an effective way for technology progress of developing countries. Based on analysis of existing reverse engineering technology, a new concept of Product Reverse Engineering (PRE) is proposed and its theoretical framework is discussed first in this paper in order to extend its application from components to the overall product structure and design process. Then a brief introduction is made to the technical system architecture and key techniques for PRE, which include the rapid solid modeling, integrating with existing CAD systems through STEP file, assembly modeling for conceptual structure, and reverse design process reconstruction. Finally, a prototype system PRE-DARFAD is developed with initial verification by a fixture design example."
882,
883,"A method to generate a quintic NURBS curve which passes through the given points is described. In this case, there are four more equations than there are positions of the control points. Therefore, four gradients which are the first derivative of a NURBS equation are assigned to the given points. In addition to this method, another method to generate a quintic NURBS curve which passes through the given points and which has the first derivative at these given points is described. In this case, a linear system will be underdetermined, determined or overdetermined depending on the number of given points with gradients. A method to modify NURBS curve shape according to the specified radius of curvature distribution to realize an aesthetically pleasing freeform curve is described. The differences between the NURBS curve radius of curvature and the specified radius of curvature is minimized by introducing the least-squares method. A criterion for a fair curve is proposed. Evaluation whether the designed curve is fair or not is accomplished by a comparison of the designed curve to a curve whose radius of curvature is monotone. The radius of curvature is specified by linear, quadratic, and cubic function using the least-squares method. A curve whose radius of curvature is reshaped by one of these algebraic functions is considered as a fair curve. The curvature vector of the curve is used to evaluate the fairness. The comparison of unit curvature vectors is used to evaluate the directional similarity of the curve. The comparison of the curvature is used to evaluate the similarity of the magnitude of curvature vectors. If the directional similarity of the designed curve is close to the fair curve, and also the similarity of the curvature is close to the fair curve, the designed curve can be judged as a fair curve."
884,"Mesh deformation, which is sometimes referred to as mesh morphing in CAE, is useful for providing various shapes of meshes for CAE tools. This paper proposes a new framework for interactively and consistently deforming assembly models of sheet structure for mechanical parts. This framework is based on a surface-based deformation, which calculates the vertex positions so that the mean curvature normal is preserved at each vertex in a least squares sense. While existing surface-based deformation techniques cannot simultaneously deform assembly mesh models, our method allows us to smoothly deform disconnected meshes by propagating the rotations and translations through disconnected vertices. In addition, we extend our deformation technique to handle non-manifold conditions, because shell structure models may include non-manifold edges. We have applied our method to assembly mesh models of automobile parts. Our experimental results have shown that our method requires almost the same pre-processing time as existing methods and can deform practical assembly models interactively."
885,"Core (filler) materials are key components of the sandwich panel and box-beams that are used in the design of lightweight structures. They perform a variety of elastic-range functions such as transferring and supporting working stresses and energy and collapse management. There is an increasing demand, however, for post-yield performance characteristics such as buckling control, impact toughness, and maintenance of component strength after damage. Low density is also an important consideration, as overall component mass is critical in most applications. These cellular solids need to perform well under normal working stress conditions, yet still resist damage from simple and unavoidable low velocity impacts. A new design approach is suggested by biological systems that have evolved for toughness and damage tolerance (bones, trees, plants, corals, etc.). These systems share the relatively low density cellular arrangements of common synthetic core materials, but also exhibit variable density gradients within the core. (Figures 1 and 2) This paper describes engineering design methods that are inspired by such biology. The result is that a design’s failure modes can be more effectively “designed-in”, controlling locations and amounts of failure."
886,"For molding and casting processes, geometries that have undercut-free parting directions (UFPDs) are preferred for manufacturing. However, existing approaches either cannot identify all UFPDs or cannot run at interactive speeds (the best exhaustive algorithm, unimplemented, runs at O(n4 ) time theoretically). Our proposed feature-based approach avoids testing the whole Gaussian sphere of potential directions by first efficiently identify all UFPDs for individual features such as extruded and revolved features, thus significantly reducing test space and running time. In this paper, we describe a fast algorithm to find all UFPDs for solids of revolution. The algorithm is based on analyzing the constructing 2D generator profiles, building on our previous results for 2-moldability analysis of polygons. The running time is O(n), where n is the geometric complexity of the 2D generator profile. For parts containing multiple solids of revolution, the set of possible UFPDs can be significantly reduced based upon an analysis of each such feature, efficiently identifying many as non-2-moldable or reducing the search space for exhaustive algorithms that find all UFPDs."
887,"This paper describes a new formulation of solid modeling that addresses the issue of including parts whose geometry is determined from volumetric scans (CT, MRI, PET, etc.) along with parts whose geometry is designed by traditional computer-aided design (CAD) operations. Such issues arise frequently in the design of medical devices or prostheses where fit and/or interference between man-made artifacts and existing anatomy are essential considerations, but the modeling formulation presented is not limited to medical applications and can be applied to any parts whose volume can be actually or virtually scanned. Scanner data typically comprises a grid of intensity values and segmentation must be performed to determine the extent of the part. In current practice, the segmented scanner data is run through a polygonizer to obtain an approximate tessellation of the object’s surface. Even in the best case scenario where the triangles obtained form a closed surface that accurately approximates the surface of the scanned object, such triangulated models can be problematic due to excessive size. We present an alternative approach based on recent advances in segmentation with level set methods. The output of the level set computation is a grid of approximate values for the signed distance from each grid point to the nearest point on the surface of the scanned object. We propose interpolating the grid of signed distance values to obtain an implicit or function-based representation (f-rep) for the object, and we introduce appropriate wavelets to effectively perform the interpolation while also providing a number of other useful properties including data compression, inherently multi-scale modeling, and capabilities for skeletal-based modeling operations."
888,"The Iterative Closest Point (ICP) algorithm and its variants are widely used in matching different patches of 3-Dimensional (3D) scanning data. In this paper, a 4-Dimensional (4D) based approach is proposed to improve the robustness of the ICP algorithm. Considering curvatures of the given geometries as an extra dimension, the existing ICP algorithm can be extended to 4D space. The reason of using this additional information is that it introduces an extra dimension of similarity in the shape matching algorithm, thus improves the effectiveness of the optimization process. Using a variant of the Laplacian smoothing tool, high frequency noise and interferences in the curvature domain are suppressed and the principal geometric features are addressed. By a 4D to 3D orthogonal projection, the matched geometries are projected back to 3D space, where the existing ICP algorithm in 3D is applied as a fine-tuning tool. Numerical implementations on several sets of scanning data demonstrate the robustness of the proposed method. The converging process and the speed of the propose method are investigated as well."
889,"Mechanical designs undergo numerous geometric changes throughout the design process. Performing these changes relies, whenever possible, on the parametric models used to create the initial geometry. However, a number of open issues prevent the current parametric modeling systems to support many practical design situations, which, in turn, forces the geometry to evolve independently of the original parametric model. The fact that every parametric update can be expressed in terms of a sequence of shape deformations implies that the same geometric updates could be obtained, at least in principle, via shape deformation procedures that parameterize the deformation itself. In this paper we propose a new approach to create and edit solid models by introducing a geometric deformation procedure that relies on motion interpolation. We show that the proposed approach induces a parametrization of the deformation that allows direct control and editing of the deformation, is capable of preserving important geometric invariants such as constant cross-sectional properties of the deformed models, and maintains the ability to perform parametric optimization of the associated solid models. We conclude by discussing advantages and limitations of this approach as well as a number of important research directions that we will pursue in the near future."
890,"Sweeps are considered to be one of the basic representation schemes in solid modeling, and have numerous applications in very diverse fields ranging from engineering design and manufacturing to computer graphics. Despite their prevalence, many properties of the general sweeps are not well understood. Furthermore, boundary evaluation algorithms for 3-dimensional solid objects currently exist only for reasonably simple objects and motions. One of the main reasons for this state of affairs is the lack of a generic point membership test for sweeps. In this paper we describe a point membership classification (PMC) for sweeping solids of arbitrary complexity moving according to one parameter "
891,"Recently meshes of engineering objects are easily acquired by 3D laser or high energy X-ray CT scanning systems, and these meshes are widely used in product developments. To effectively use scanned meshes in engineering applications, such as inspection, CAD model reconstruction, and convergent-type CAE, we need to segment meshes and extract desirable regions and their approximating surfaces as preprocessing. Engineering objects are commonly represented as a set of analytic surfaces, such as planes, cylinders, spheres, cones, and tori. Therefore, the mesh surface of engineering objects needs to be approximated as a set of analytic surfaces. Moreover, a mesh surface should be approximated with a minimum number of analytic surfaces and their approximating error should be minimized as a result of segmentation. We call the segmentation that satisfies these two conditions the "
892,"This research addresses the development of validation metrics for vehicle frontal impact simulation. The model validation metrics provide a quantified measurement of the difference between CAE simulation and physical test. They are useful to develop an objective model evaluation procedure for eventually achieving the goal of zero or near zero prototyping. In this research, full frontal crash pulses are chosen as the key items to be compared in the vehicle frontal impact simulation. Both physics- and mathematics-based metrics are investigated. The physics-based metric include a method of using a simplified step function representation and the mathematics-based metrics include methods of wavelet decomposition, corridor violation plus area, and metrics used in a commercial code ADVISER, respectively. They are all correlated to subject matter experts’ rating through optimal weightings. A new metric, considering variabilities from both experts and metrics for frontal crash pulse, is proposed. One example is used to demonstrate its application."
894,"Software to support the solution generation phase of the engineering design process has been developed in academia for decades. Computational synthesis software enables generation of solutions on both conceptual and embodiment level. This paper focuses on the class of parametric design, such as documented in mechanical engineering handbooks. Examples include machine elements such as bearings, springs, fasteners, transmissions, etc. A parametric synthesis tool automates the engineering design process from functional requirements to quantified solutions, for a single machine element. Since the amount of machine elements is vast and software development time should be low, a generic methodology is helpful to speed up this process. This paper discusses such a methodology to develop synthesis tools for the class of parametric designs. It includes an analysis-oriented approach to formalize the design process’ parameters in terms of embodiment, performance and scenario. Mathematical constraint solving techniques are used to generate candidate solutions. Graphical presentation and exploration of the solution space is done with interactive plots. A standardized layout for the graphical user interface is suggested to allow uniform and intuitive use. A demonstrator is developed using the described methodology and several challenges are discussed for improved constraint solving techniques, more advanced visualization and handling problems with higher complexity. Although small in size, parametric design processes are time consuming due to their reoccurring nature. Developing synthesis tools for these designs will allow engineers to save time and improve design quality."
895,"Demand models play a critical role in enterprise-driven design by expressing revenues and costs as functions of product attributes. However, existing demand modeling approaches in the design literature do not sufficiently address the unique issues that arise when complex systems are being considered. Current approaches typically consider customer preferences for only quantitative product characteristics and do not offer a methodology to incorporate customer preference-data from multiple component/subsystem-specific surveys to make product-level design trade-offs. In this paper, we propose a hierarchical choice modeling approach that addresses the special needs of complex engineering systems. The approach incorporates the use of qualitative attributes and provides a framework for pooling data from multiple sources. Heterogeneity in the market and in customer-preferences is explicitly considered in the choice model to accurately reflect choice behavior. Ordered logistic regression is introduced to model survey-ratings and is shown to be free of the deficiencies associated with competing techniques, and a Nested Logit-based approach is proposed to estimate a system-level demand model by pooling data from multiple component/subsystem-specific surveys. The design of the automotive vehicle occupant package is used to demonstrate the proposed approach and the impact of both packaging design decisions and customer demographics upon vehicle choice are investigated. The focus of this paper is on demonstrating the demand (choice) modeling aspects of the approach rather than on the vehicle package design."
896,"This case study investigates the design process followed by a small to medium scale enterprise (SME) that primarily depends on special expertise in the form of a few key individuals, who design products mainly based on past experience, augmented by trial and error. This is an inefficient, time consuming, and expensive way of designing products and evaluating their performance. This led to development of a specialized and affordable design enabler that facilitates engineering analysis, noticeably absent in SME’s current design process. Further, the design enabler forms the foundation for extending the scope to include rule-based systems, optimization and case based reasoning that would assist designers in efficient product development."
897,"Product family design is a cost-effective way to achieve mass customization by allowing highly differentiated products to be developed from a common platform while targeting individual products to distinct market segments. Recent trends seek to apply and extend principles from product family design to new service development. In this paper, we extend concepts from platform-based product family design to create a novel methodology for module-based service family design. The new methodology helps identify a service platform along with variant and unique modules in a service family by integrating service-based process analysis, ontologies, and data mining. A function-process matrix and a service process model are investigated to define the relationships between the service functions and the service processes offered as part of a service. An ontology is used to represent the relationships between functional hierarchies in a service. Fuzzy clustering is employed to partition service processes into subsets for identifying modules in a given service family. The clustering result identifies the platform and its modules using a platform level membership function. We apply the proposed methodology to determine a new platform using a case study involving a family of banking services."
898,"Although many knowledge management techniques based on text expression have been developed, they are not necessarily sufficient for managing engineering design knowledge. In this paper, we propose quantity dimension indexing of design knowledge as a fundamental method for design knowledge management. Physical quantities describing physical phenomena can be represented as vectors in a seven-dimensional space where the orthogonal axes are the seven base units of the SI (The International System of Units). Because of the generality, objectivity and universality of the SI, this space covers all physical quantities that appear in the past, present and future design knowledge and design problems, and the same quantities are represented as the same vectors regardless of the differences in people, products, domains, organizations, nations and languages. We assume that the similarities of physical phenomena lead to similarities in the dimensions of quantities describing the phenomena, and propose to use this seven-dimensional vector for estimating the similarity of design knowledge from the viewpoint of physical phenomena. Based on this basic idea, we mathematically define similarity between two quantities using quantity dimensions. We prepared design knowledge examples and retrieval keys and conducted design knowledge retrieval and design knowledge similarity estimation by quantity dimension indexing and confirmed that we obtained adequate results without using a concept dictionary or thesaurus elaborated in advance, which are indispensable in the text approach."
899,"Product Platform and Product Family Design is reshaping the way that many companies develop products. But how well are the CAD and PLM technologies keeping pace with this advancement? This paper presents data from a three month ethnographic study of an expert automotive body engineer. His assignment is to modify the design of an existing body structural member for use in the next-generation vehicle. The modification was necessitated by manufacturability issues. Observations and subsequent interviews revealed that manipulation time, model reuse and representation of part interfaces (such as welds) presented challenges to the body engineer and collaborating analysis engineers. Despite re-use of a physical part, the engineer had to create a new CAD model. The redesign involved breaking the original part into two pieces. The engineer sketched initial design concepts on paper because manipulation time in the CAD system was so lengthy. After determining the design concept, the engineer created a new CAD model, including new weld locations, and passed it along to analysis engineers for stiffness and crashworthiness FEA testing. Hand-offs between design and analysis engineers were challenged by the PLM system. The paper ends by making recommendations for improving CAD and PLM tools."
900,"We propose a method for metamodeling non-deterministic computer intensive simulations for use in robust design. Generalized linear models for mean responses and heteroscadastic response variances are iteratively estimated in an integrated manner. Estimators that may be used for predicting the mean and variance models are introduced and metamodels of variance are developed. The usefulness of this metamodeling approach in efficient uncertainty analyses of non-deterministic, computationally-intensive simulation models for robust design methods is illustrated with the example of the design of a linear cellular alloy heat exchanger with randomly distributed cracks in the cell walls."
901,"Design processes for multiscale, multifunctional systems are inherently complex due to the interactions between scales, functional requirements, and the resulting design decisions. While complex design processes that consider all interactions lead to better designs; simpler design processes where some interactions are ignored are faster and resource efficient. In order to determine the right level of simplification of design processes, designers are faced with the following questions: "
902,"Sequential sampling refers to a set of design of experiment (DOE) methods where the next sample point is determined by information from previous experiments. This paper introduces a qualitative and quantitative sequential sampling (Q2S2) technique, in which optimization and user knowledge is used to guide the efficient choice of sample points. This method combines information from multiple fidelity sources including computer simulation models of the product, first principals involved in design, and designer’s qualitative intuitions about the design. Both "
903,"A kriging model can be used as a surrogate to a more computationally expensive model or simulation. It is capable of providing a continuous mathematical relationship that can interpolate a set of observations. One of the major issues with using kriging models is the potentially computationally expensive process of estimating the best model parameters. One of the most common methods used to estimate model parameters is Maximum Likelihood Estimation (MLE). MLE of kriging model parameters requires the use of numerical optimization of a continuous but possibly multi-modal log-likelihood function. This paper presents some enhancements to gradient-based methods to make them more computationally efficient and compares the potential reduction in computational burden. These enhancements include the development of the analytic gradient and Hessian for the log-likelihood equation of a kriging model that uses a Gaussian spatial correlation function. The suggested algorithm is very similar to the Scoring algorithm traditionally used in statistics, a Newton-Raphson gradient-based optimization method."
904,"Response surface approximations (RSA) are a common tool in engineering, often constructed based on finite element (FE) simulations. For some design problems, the FE models can involve a high number of parameters. However it is advantageous to construct the RSA as function of a small number of variables. The purpose of this paper is to demonstrate that a significant reduction in the number of variables needed for an RSA is possible through physical reasoning, dimensional analysis and global sensitivity analysis. This approach is demonstrated for a transient thermal problem, but it is applicable to any FE based surrogate model construction. The thermal problem considered is the design of an integrated thermal protection system (ITPS) for spacecraft reentry where an RSA of the maximum bottom face temperature was needed. The FE model used to evaluate the maximum temperature depended on 15 parameters of interest for the design: 9 thermal material properties and 6 geometric parameters of the ITPS panel. A small number of assumptions simplified the thermal equations allowing easy nondimensionalization, which together with a global sensitivity analysis showed that the maximum temperature mainly depends on only two nondimensional parameters. These were selected to be the design variables of the RSA for maximum temperature. The RSA was still fitted to the original non-simplified FE simulations. Having only two variables allowed a dense design of experiments thus providing a very good quality of fit. Consequently the major error remaining in the RSA is due to the fact that the two nondimensional variables account for only part (albeit the major part) of the dependence on the original 15 variables. This error was checked and good agreement was found. The two-dimensional nature of the RSA allowed graphical representation, which was used for material selection from among hundreds of possible materials for the design optimization of an ITPS panel."
905,"Dimensional Analysis (DA) is a tool often used to relate models and specimens to the actual product or system based on the hypothesis that the two regimes follow the same physical laws and are hence dimensionally equivalent. While this has been a conventional use of the process, we extend the technique to dynamic systems to develop state equations that allow for design studies and optimization. A methodical approach is detailed coupled with an example of a toy water-rocket assembly. A modified methodology (heuristic) is also discussed to condense non-monomial basis systems using simple physical laws and a novel reduction process. Experimental verification is provided to complement the analysis procedure. Efficacy of the process is highlighted in comparison with the conventional close-form approach."
906,"Computational models with variable fidelity have been widely used in engineering design. To alleviate the computational burden, surrogate models are used for optimization without recourse to expensive high-fidelity simulations. In this work, a model fusion technique based on Bayesian Gaussian process modeling is employed to construct cheap, surrogate models to integrate information from both low-fidelity and high-fidelity models, while the interpolation uncertainty of the surrogate model due to the lack of sufficient high-fidelity simulations is quantified. In contrast to space filling, the sequential sampling of a high-fidelity simulation model in our proposed framework is objective-oriented, aiming for improving a design objective. Strategy based on periodical switching criteria is studied which is shown to be effective in guiding the sequential sampling of a high-fidelity model towards improving a design objective as well as reducing the interpolation uncertainty. A design confidence (DC) metric is proposed to serves as the stopping criterion to facilitate design decision making against the interpolation uncertainty. Numerical and engineering examples are provided to demonstrate the benefits of the proposed methodology."
907,"Solution of complex system design problems using distributed, decomposition-based optimization methods requires determination of appropriate problem partitioning and coordination strategies. Previous optimal partitioning techniques have not addressed the coordination issue explicitly. This article presents a formal approach to simultaneous partitioning and coordination strategy decisions that can provide insights on whether a decomposition-based method will be effective for a given problem. Pareto-optimal solutions are generated to quantify tradeoffs between the sizes of subproblems and coordination problems, as measures of the computational costs resulting from different partitioning-coordination strategies. Promising preliminary results with small test problems are presented. The approach is illustrated on an electric water pump design problem."
908,"Real-world engineering design optimization problems often involve systems that have coupled disciplines with uncontrollable variations in their parameters. No approach has yet been reported for the solution of these problems when there are multiple objectives in each discipline, mixed continuous-discrete variables, and when there is a need to account for uncertainty and also uncertainty propagation across disciplines. We present a Multiobjective collaborative Robust Optimization (McRO) approach for this class of problems that have interval uncertainty in their parameters. McRO obtains Multidisciplinary Design Optimization (MDO) solutions which are as best as possible in a multiobjective and multidisciplinary sense. For McRO solutions, the sensitivity of objective and/or constraint functions is kept within an acceptable range. McRO involves a technique for interdisciplinary uncertainty propagation. The approach can be used for robust optimization of MDO problems with multiple objectives, or constraints, or both together at system and subsystem levels. Results from an application of the approach to a numerical and an engineering example are presented. It is concluded that the McRO approach can solve fully coupled MDO problems with interval uncertainty and can obtain solutions that are comparable to an all-at-once robust optimization approach."
909,"In this paper, an integrated optimization, controller design and reduced order finite element modeling based approach is presented for structural design. The proposed approach involves structure decomposition, subcontroller design, system controller assembly, and multiobjective optimization. The concept of structure decomposition with compatible and incompatible interfaces is presented for a control/optimum system problem, and developed for problems with compatible interfaces involving substructure controller design and multiobjective optimization. The substructure information obtained through finite element analysis is synthesized to reconstruct a reduced order model for the entire structure. Based on SSSC (Substructure Synthesis-Substructure Controller), a controller is designed for each substructure. The global controller is obtained by assembling all subcontrollers designed at the substructure level. A multiobjective optimum formulation is presented based on structure decomposition and controller design. Four objective functions are simultaneously optimized. These include a stability robustness index, structural weight, controller energy, and a controller performance index. Numerical examples are presented to demonstrate the effectiveness of the proposed methodology. Results obtained using the proposed approach are compared with those obtained from optimization of the entire structure."
910,"Decomposition-based strategies, such as analytical target cascading (ATC), are often employed in design optimization of complex systems. Achieving convergence and computational efficiency in the coordination strategy that solves the partitioned problem is a key challenge. A new convergent strategy is proposed for ATC, which coordinates the interactions among subproblems using sequential lineralizations. Linearity of subproblems is maintained using "
911,"Analytical Target Cascading (ATC) is an effective decomposition approach used for engineering design optimization problems that have hierarchical structures. With ATC, the overall system is split into subsystems, which are solved separately and coordinated via target/response consistency constraints. As parallel computing becomes more common, it is desirable to have separable subproblems in ATC so that each subproblem can be solved concurrently to increase computational throughput. In this paper, we first examine existing ATC methods, providing an alternative to existing nested coordination schemes by using the block coordinate descent method (BCD). Then we apply diagonal quadratic approximation (DQA) by linearizing the cross term of the augmented Lagrangian function to create separable subproblems. Local and global convergence proofs are described for this method. To further reduce overall computational cost, we introduce the truncated DQA (TDQA) method that limits the number of inner loop iterations of DQA. These two new methods are empirically compared to existing methods using test problems from the literature. Results show that computational cost of nested loop methods is reduced by using BCD and generally the computational cost of the truncated methods, TDQA and ALAD, are superior to other nested loop methods with lower overall computational cost than the best previously reported results."
912,"This work presents design concepts to synthesize composite materials with special dynamic properties, namely, materials that soften at high frequencies. Such dynamic properties are achieved through the use of a two-phase material that has inclusions of a viscoelastic material of negative elastic modulus in a typical matrix phase that has a positive elastic modulus. A possible realization of the negative stiffness inclusion phase is presented. A numerical homogenization technique is used to compute the average viscoelastic properties of the composite. The method and the properties of a composite material designed with it are demonstrated through an example."
913,"Multilevel design is a subset of engineering design in which design problems are defined and analyzed at various levels of model complexity or resolution. Due to the potential for propagated uncertainty in a multilevel design process, design goals for maximizing system robustness to uncertainty in noise and control factors are included in the Blast resistant panels (BRP) design process. Blast resistant panels (BRPs) are sandwich structures consisting of two solid panels surrounding a honeycomb core. Under impulse loading, BRPs experience less deflection than similarly loaded solid panels of equal mass due to core crushing. In order to manage complexity in BRP concurrent product and materials design, a multilevel design approach is proposed. Additionally, in order to collect and store BRP design information in a modular and reusable format, a template-based design approach is implemented in BRP multilevel design. In this paper, a generic multilevel design template based on existing design methods (the compromise Decision Support Problem and the Inductive Design Exploration Method) is presented. The multilevel design template is then particularized and applied to BRP preliminary design, highlighting the advantages of a templatebased approach to multilevel design."
914,"It has been reported that a carbon nanotube (CNT) is one of the strongest materials with their high failure stress and strain. Moreover, the nanotube has many favorable features, such as high toughness, great flexibility, low density, and so on. This discovery has opened new opportunities in various engineering applications, for example, a nanocomposite material design. However, recent studies have found a substantial discrepancy between computational and experimental material property predictions, in part due to defects in the fabricated nanotubes. It is found that the nanotubes are highly defective in many different formations (e.g., vacancy, dislocation, chemical, and topological defects). Recent parametric studies with vacancy defects have found that the vacancy defects substantially affect mechanical properties of the nanotubes. Given random existence of the nanotube defects, the material properties of the nanotubes can be better understood through statistical modeling of the defects. This paper presents predictive CNT models, which enable to estimate mechanical properties of the CNTs and the nanocomposites under various sources of uncertainties. As the first step, the density and location of vacancy defects will be randomly modeled to predict mechanical properties. It has been reported that the Eigenvector Dimension Reduction (EDR) method performs probability analysis efficiently and accurately. In this paper, Molecular Dynamics (MD) simulation with a modified Morse potential model is integrated with the EDR method to predict the mechanical properties of the CNTs. To demonstrate the feasibility of the predicted model, probabilistic behavior of mechanical properties (e.g., failure stress, failure strain, and toughness) is compared with the precedent experiment results."
915,"Designing advanced multifunctional materials and products in an integrated fashion starting from the conceptual stage provides designers with increased flexibility to achieve system performance goals that were not previously achievable. Today however, product designers commonly select more or less advanced materials from selection charts or catalogs, rather than designing them along with the product from the conceptual stage on. In order to increase a designer’s flexibility in the conceptual stage and render conceptual materials design more systematic, hence less ad-hoc and intuitive, the main contribution is the development of a systematic approach to the integrated design of material and product concepts from a systems perspective. This systematic approach is focused on developing multilevel function structures, including the material levels. Based on functional analysis, abstraction and synthesis, multiscale phenomena and associated governing solution principles are mapped to functional relationships. Hence, multilevel function structures are embodied into principal solution alternatives based on comprehensive identification and integration of phenomena and associated governing solution principles occurring at multiples levels and time and length scales. In this paper, the function-based approach to integrated design of material and product concepts is illustrated through the systematic design of reactive material containment system concepts. Having developed an overall reactive material containment system function structure, a more detailed function structure on the materials level is created. For dominating functional relationships at the materials level, governing solution principles are identified on multiple scales. The most promising solution principles are then classified in morphological charts. Combining solution principles in a systematic fashion including the materials level, product and material system concepts are identified. The most promising system concepts, in other words the principal solution alternatives that narrow the gap to desired system performance goals, are selected and illustrated in concept selection charts. A selected material and product system concept is then characterized in terms of its specific properties, which are to be tailored to the functional requirements and performance goals in subsequent embodiment design processes. By developing concepts of the product and material as an integrated system, materials design becomes more systematic and hence less ad-hoc and intuitive. At the same time, designers are enabled to realize new functionality and achieve system performance goals that were not previously achievable."
916,"Modeling with free form features has become the standard in Computer-Aided Design (CAD). With the increasing complexity of free form CAD models, features offer a high-level approach to modeling shapes. However, in most commercial modeling packages, only a static set of free form features is available. Researchers have tried to solve this problem by coming up with methods for user-driven free form feature definition, but failed to connect their methods to a means to instantiate these user-driven free form features on a target surface. Reversely, researchers have proposed tools for modeling with free form features, but these methods are time-intensive in that they are as of yet unsuitable for pre-defined features. This paper presents a new method for user-driven feature definition, as well as a method to instantiate these user-defined features on a target surface. We propose the concept of a dual environment, in which the definition of a feature is maintained simultaneously with its instance on a target surface, allowing the user to modify the definition of an already instantiated feature. This dual environment enables dynamic feature modeling, in which the user is able to change the definition of instantiated features on-the-fly. Furthermore, the proposed instantiation method is independent from the type of shape representation of the target surface and thereby increases the applicability of the method. The paper includes an extensive application example and discusses the results and shortcomings of the proposed methods."
917,"Creating unavailable geometric models from existing parts plays an important role in the process of reverse engineering, for which the accuracy and fitting time of the created models are important factors. This paper proposes the use of Tabu Search (TS) technique in the optimal fitting of NURBS (Non Uniform Rational B-Spline) surfaces to laser-scanned point clouds of free-form surfaces for existing parts. The fitting process involves the initial estimation of the NURBS surface control points using least-squares approximation, followed by optimization of NURBS weights to minimize fitting error. Optimization is performed using a hybrid coding scheme, namely; Modified Continuous Reactive Tabu Search (M-C-RTS), in which a combinatorial optimization component, based on Reactive Tabu Search (RTS), co-operates with Sequential Quadratic Programming (SQP), as a local minimizer. The developed fitting algorithm was applied to a number of simulated free-form surfaces in addition to a laser-scanned PC mouse. Implementation was carried out using MATLAB software and the results were compared to those obtained using Genetic Algorithms (GAs) in an earlier publication. The results show the superiority of the proposed algorithm to the GA-based method with respect to the number of objective function evaluations (about 50% reduction). In addition to this time saving achievement, and surprisingly, M-C-RTS proved to be capable of finding better solutions than GAs."
918,"This paper presents an approach to match correspondences on 3D meshes, which is an important step for the design automation of customized freeform objects. For a given template model with a set of anchor points defined (knots of semantic features), we identify the corresponding points on the target model by minimizing the sum of differences by a series of transformation regardless of their differences in postures, scales and/or positions. The basic idea of our algorithm is to transform the target model to the template model iteratively. Once the correspondences between the surface points on the target model and the template are determined, we have essentially found the semantic features on the target model. We achieve this goal by four major transformations: 1) "
919,"This paper discusses a novel method for designing imprint rolls for the fabrication of fluid pathways. Roller imprint processes have applications in diverse areas including fuel cell manufacturing and microfluidic device fabrication. Robust design methods are required for developing imprint rolls with optimal features. In the method discussed in this paper, the rolls are designed procedurally with the fluid pathway design given as input. The pathways are decomposed into repeating features (or tiles), and the rolls are designed by first modeling a small set of unique tiles and then combining them to model the entire roll. The tiling strategy decreases the complexity of the model, and reduces the time taken for designing the rolls. The modular nature of the tiles also improves the efficiency of post-processing operations like feature identification and optimization, and the generation of toolpaths for machining the roll."
920,"This paper presents a new assembly model named open assembly model plus (OAM+) to support legacy systems engineering (LSE). LSE is a collection of technologies for prolonging the life of old mechanical systems. Rapid Re-Engineering System (RRES), a subsystem of LSE is geared towards the fast production of redesigned parts customized to the manufacturing capability available. RRES requires the extraction of initial part geometry, parameters, interfacing constraints, kinematic constraints, and technical function. These specifications need to be imprinted on the CAD model before iterative redesign. A CAD data model is needed that can carry all the functional constraints. A detailed comparison of all the available assembly model shows that none of them can provide all these requirements in one place. Assembly feature based, object oriented assembly model OAM+ is developed to meet these requirements in one model. OAM+ can be used to perform kinematic analysis, force analysis and can exchange feature data using N-Rep feature definition language between different modules of RRES. OAM+ is based on part and assembly features in N-Rep."
921,"Product design optimizations usually require the optimization of not only all performance characteristics, but also the robustness of certain performance characteristics. Obtaining optimum design solutions is far from easy, since this requires evaluations of numerous related characteristics that usually have complicated and conflicting interrelationships. Some of these characteristics can include variations of one type or another, such as manufacturing process variations, variations pertaining to the environments where the product is used, variations in how long-term use affects certain product characteristics, and so on. The difficulty of obtaining optimum design solutions is thus compounded by the need to carry out specific optimizations that provide sufficient robustness to safely accommodate anticipated ranges of variations. This paper expands the hierarchical multiobjective optimization method based on simplification and decomposition of characteristics so that optimizations can be concurrently conducted for both performance characteristics and maximization of robustness against characteristic variances. A principal cause of variations in performance characteristics is variations in the contact conditions of joints, and the utility of the proposed robust product design optimization method is demonstrated by applying it to machine-tool models that include joints."
922,"Minimizing brake squeal is one of the most important issues in the development of high performance braking systems. Recent advances in numerical analysis, such as finite element analysis, have enabled sophisticated analysis of brake squeal phenomena, but current design methods based on such numerical analyses still fall short in terms of providing concrete performance measures for minimizing brake squeal in high performance design drafts at the conceptual design phase. This paper proposes an optimal design method for disc brake systems that specifically aims to reduce brake squeal by appropriately modifying the shapes of the brake system components. First, the relationships between the occurrence of brake squeal and the geometry and characteristics of various components is clarified, using a simplified analysis model. Next, a new design performance measure is proposed for evaluating brake squeal performance and an optimization problem is then formulated using this performance measure as an objective function. The optimization problem is solved using Genetic Algorithms. Finally, a design example is presented to examine the features of the optimal solutions and confirm that the proposed method can yield useful design information for the development of high performance braking systems that minimize brake squeal."
923,"Designing a drive train for an industrial robot is a demanding task where a set of design variables need to be determined so that optimal performance is obtained for a wide range of different duty cycles. The paper presents a method where singular value decomposition (SVD) is used to reduce the design variable set. The application is a six degree of freedom serial manipulator, with nine drive train parameters for each axis and the objective is to minimize the cycle time on 122 representative design cycles without decreasing the expected lifetime of the robot. The optimization is based on a simulation model of the robot and conducted on a reduced set of the initial duty cycles and with the design variables suggested by the SVD analysis. The obtained design reduces the cycle time with 1.6% on the original design cycles without decreasing the life time of the robot."
924,"In this paper, an approach for modular design of industrial robots is presented. The approach is to introduce an objectoriented simulation model of the robot and combine this with a discrete optimization algorithm. The simulation model of the industrial robot is developed in Modelica, an object oriented modeling and simulation language, and simulated in the Dymola tool. The optimization algorithm used is a modification of the Complex method that has been developed in Matlab and connected to the simulation program. The optimization problem includes selecting components such as gearboxes and motors from a component catalogue and the objective function considers minimization of cost with constraints on gear box lifetime. Furthermore, the correctness of the model has been verified by comparison with an in-house simulation code with high accuracy."
925,"Today’s companies are pressured to develop platform-based product families to increase variety while keeping production costs low. Determining why a platform works, and alternatively why it does not, is an important step in the successful implementation of product families and product platforms in any industry. Internal and competitive benchmarking is essential to obtain knowledge of how successful product families are implemented, thus avoiding potential pitfalls of a poor product platform design strategy. While the two fields of product family design and benchmarking have been growing rapidly lately, we have found few tools that combine the two for product "
926,"Platform-based product development depends on many factors, including technology, cost, competition, and life cycle considerations, and many companies would benefit from knowing more about the nature of their product families and how they impact platform-based product development. We assert that the development of a product platform and its derivative family of products is also impacted by the homogenous/heterogeneous nature of the products being developed, which has received little attention in the engineering literature. The current study introduces an original metric for assessing the homogeneity/heterogeneity in a given family: the Homogeneity versus Heterogeneity Ratio (HHR), which works at two levels of abstraction, namely, family and function. This study focuses on the platform leveraging strategy and takes an interest in two other aspects of platform development: the specification of the family and the necessary differentiation. To support platform design, the HHR"
927,"Many companies that struggle with product variety and configuration management issues turn to a module-based design approach. Although this approach is well-known to be efficient for managing variety of a product family, current methods do not enable designers to handle both modularity and variety within a product family. The Design Structure Matrix (DSM) has been widely used to identify modules within a product, but its use to identify modules across a family of products has been limited. In this context we propose two tools based on an extension of the basic DSM to manage variety of an entire product family. The Variety Design Structure Matrix, DSMV , handles variety of the product family and 3D Design Structure Matrix, DSM3D , enables visual analysis of across the entire product family. These two tools, combined into a single approach, enable analysis of the product family at many levels — family product, module, and interfaces — to better specify modules and interfaces across all of the products in the family. A case study involving a family of three single-use cameras is used to demonstrate the application of these new DSMs and accompanying cross-module and cross-interface analyses. This new approach can be applied during detailed studies as well as in the early stages of the design process."
928,"Quality Function Deployment (QFD) was initially developed to aid in designing a quality product by interconnecting customer needs in a market segment with technical requirements. Although it assists in improving product quality, it does not have a function to examine technical requirements across the major market segments serviced by a company’s product lines and to aid in developing product platform concepts. In this paper, we present a product platform development method using QFD that aids in developing platform concepts as well as improving the understanding of product family design. This method includes platform planning and platform concept exploration. Platform planning describes the extent to which a variety of products share common components, and platform concepts then are explored, which are the arrangement of common components. This paper uses an electric razor example to illustrate the proposed method."
929,"This research addresses the issues to identify the optimal product design based on individual customer requirements in one-of-a-kind production (OKP). In this work, a function decomposition approach is introduced for modeling the variations of design functions, configurations, and parameters in generic OKP product families. Requirements of individual customers are modeled at two different levels: function level and technical level. Customized OKP products are created from the generic OKP product families based on customer requirements. The optimal product design is identified from feasible design candidates through optimization. An industrial case study is given to demonstrate the effectiveness of the introduced approach."
930,"The formulation of a product family requires extensive knowledge about the product market space and also the technical limitations of a company’s engineering design and manufacturing processes. We present a methodology to significantly reduce the computational time required to achieve an optimal product portfolio by eliminating the need for an exhaustive search of all possible product concepts. This is achieved through a data mining decision tree technique that generates a set of product concepts that are subsequently validated in the engineering design level using multi-level optimization techniques. The final optimal product portfolio evaluates products based on the following three criteria: 1) The ability to satisfy customer’s price and performance expectations (based on predictive model) defined here as the feasibility criterion. 2) The feasible set of products/variants validated at the engineering level must generate positive profit that we define as the optimality criterion. 3) The optimal set of products/variants should be a manageable size as defined by the enterprise decisions makers and should therefore not exceed the product portfolio limit. The strength of our work is to reveal the tremendous savings in time and resources that exist when data mining predictive techniques are applied to the formulation of an optimal product portfolio. Using data mining tree generation techniques, a customer response data set of 40,000 individual product preferences is narrowed down to 46 product family concepts and then validated through the multilevel engineering design response of feasible architectures. A cell phone example is presented and an optimal product portfolio solution is achieved that maximizes company profit, while concurrently satisfying customer product performance expectations."
931,"The design and development of effective product lines is a challenge in modern industry. Companies must balance diverse product families that satisfy wide ranging customer demands with practical business needs such as combining manufacturing processes and using similar materials, for example. In this paper, the issue of consolidating an existing product family is addressed. Specifically, the Hypothetical Equivalents and Inequivalents Method (HEIM) is utilized in order to select an optimal product family configuration. In previous uses, HEIM has been shown to assist a decision maker in selecting one concept from a set when concept attributes conflict with each other. In this extension of HEIM, the optimization problem’s constraints are formulated using two different value functions, and common solutions are identified in order to select an optimal family of staplers. The result is then compared with the result found using a multi-attribute utility theory (MAUT) based approach. While each method has its advantages and disadvantages, and MAUT provides a necessary first step for product family consolidation and selection, a robust solution is achieved through HEIM."
932,"One critical aim of product family design is to offer distinct variants that attract a variety of market segments while maximizing the number of common parts to reduce manufacturing cost. Several indices have been developed for measuring the degree of commonality in existing product lines to compare product families or assess improvement of a redesign. In the product family optimization literature, commonality metrics are used to define the multi-objective tradeoff between commonality and individual variant performance. These "
933,"A core challenge in product family optimization is to develop a single-stage approach that can optimally select the set of variables to be shared in the platform(s) while simultaneously designing the platform(s) and variants within an algorithm that is efficient and scalable. However, solving the joint product family platform selection and design problem involves significant complexity and computational cost, so most prior methods have narrowed the scope by treating the platform as fixed or have relied on stochastic algorithms or heuristic two-stage approaches that may sacrifice optimality. In this paper, we propose a single-stage approach for optimizing the joint problem using gradient-based methods. The combinatorial platform-selection variables are relaxed to the continuous space by applying the commonality index and consistency relaxation function introduced in a companion paper. In order to improve scalability properties, we exploit the structure of the product family problem and decompose the joint product family optimization problem into a two-level optimization problem using analytical target cascading so that the system-level problem determines the optimal platform configuration while each subsystem optimizes a single product in the family. Finally, we demonstrate the approach through optimization of a family of ten bathroom scales; Results indicate encouraging success with scalability and computational expense."
934,"While many approaches have been proposed to optimize the product family design for measures of cost, revenue and performance, many of these approaches fail to incorporate the complexity of the manufacturing issues into family design decision-making. One of these issues is assembly sequencing. This paper presents a simulation study by which the impact of assembly sequencing on the product family design outcomes is investigated. Overall, the results indicate that when the product family design takes into account the assembly sequencing decisions, the outcomes at the shop floor level improve. The results have implications for companies that are looking into increasing their revenue without increasing their investment in the shop floor."
935,"Product families are groups of related products that take advantage of part commonalities at various levels to streamline delivery of maximal product variety with minimal cost impact and as short as possible lead-times. This paper proposes a new integrated product design method for build-to-order production system based products, using the product family concept, which considers product performance, delivery lead-time and inventory cost. The development and discussion of this method uses a switchgear design problem as a concrete and practical design case. A build-to-order production system has been applied to switchgear manufacturing due to its small-scale production and a variety of customer requirements. However, if the risk of maintaining unsold inventory can be decreased, manufacturers can justify holding an amount of versatile inventory. In this paper, inventory production system is applied to the switchgear production problem to shorten the delivery lead-time. The switchgear design and production problem is formulated using three objective functions, which are subassembly procurement lead-time, inventory cost and area occupied by various switchgear configurations. Moreover, to assist inventory cost evaluations, a simulation procedure for the inventory system is proposed. The proposed method is used to obtain a Pareto optimal solution set of the three objective functions. Finally, an example switchgear design problem is solved to illustrate that optimal use of component commonalities across different modules can significantly reduce inventory costs, while also shortening product delivery lead-times."
936,"In accordance with the product families, process platforms have been recognized as a promising tool for companies to configure optimal, yet similar, production processes for producing different products. This paper tackles process platform formation from large volumes of production data available in companies’ production systems. A data mining methodology based on text mining and tree matching is developed for the formation of process platforms. A case study of high variety production of vibration motors for mobile phones is reported to prove the feasibility and potential of forming process platforms using text mining and tree matching."
937,"The focus of this paper is on the design of the engineered system comprising various subsystems that have functional interactions amongst themselves. Depending on the system architecture, subsystems can be classified into one of the two types: scalar and modular subsystems. Each subsystem is defined by various primary and secondary parameters, performance criteria, and compatibility constraints. The objective function is formulated as minimization of total cost of all the subsystems while meeting the required performance criteria. The cost of an individual subsystem is a function of parameters of that particular subsystem. The complete engineered system level problem has been formulated as a non linear programming optimization problem. The application of the proposed methodology is demonstrated using an example of the automotive truck family."
938,"Mass customization is a common trend in industries and platform-based product family strategy is widely used for an efficient mass customization. While commonization of a platform is a viable mean for reducing the customization cost, it also has a risk of losing some market share due to its limitation on differentiating individual products. This trade-off requires a platform to be balanced between commonality and distinctiveness of products. In this paper, we focus on developing a versatile platform that maximizes the use of common components while facilitating differentiations which are highly effective for increasing the market share of a product family. A versatile platform is comprised of versatile components which do not restrict effective differentiations even if it is commonized. To determine a certain component is versatile or not, we considered which specifications are preferred to be differentiated in the market and how much change would be required for the component to differentiate a specification. With these two measures, we define a versatility index representing how versatile a component is. Components with higher versatility values are appropriate to be platformized since they are less likely to be changed for differentiations. Furthermore, identification of non-versatile components may provide a clue for improving architecture of the product. The proposed method is applied to the PC mouse design, which yields reasonable alternatives for platform design."
939,"Small volume and high product-mix contribute greatly to the complexity of job shop operations. In addition, shop floor uncertainty or fluctuation is another issue regularly challenging manufacturing companies, including job delay, urgent job insertion, fixture shortage, missing tool, and even machine breakdown. Targeting the uncertainty, we propose a function block based approach to generating adaptive process plans. Enabled by the function blocks, a so-generated process plan is responsive and tolerant to an unpredictable change. This paper presents in detail how a function block is designed and what it can do during process plan execution. It is expected that this new approach can largely enhance the dynamism of fluctuating job shop operations."
940,"Understanding of uncertainty in the data and models used in design simulations matures during the design process as the design progresses from vague requirements through to its full embodiment and detail. Failure to take account of uncertainty in the information that is used in and generated from simulation processes poses risks to decisions based upon these. This paper presents a classification scheme based on the extent and nature of uncertainty in the correlations between simulation predictions and the evidence for a specific performance criterion. The classification allows development of a confidence scale and associated error functions for characterizing the discrepancy between the correlations of design performance parameters and evidence, in the presence of uncertainty. Together, the confidence scale and error functions may provide a greater understanding of uncertainty and errors in simulation processes. In the context of parametric design, the approach provides a mechanism for building up greater understanding of the simulation performance across a feasible design space. A case study on the design of shrink-fits is used to illustrate the framework for handling uncertainty in a systematic and organized manner. The theoretical and practical limitations and further work will be discussed."
941,"Traditional reliability analysis uses probability distributions to calculate reliability. In many engineering applications, some nondeterministic variables are known within intervals. When both random variables and interval variables are present, a single probability measure, namely, the probability of failure or reliability, is not available in general; but its lower and upper bounds exist. The mixture of distributions and intervals makes reliability analysis more difficult. Our goal is to investigate computational tools to quantify the effects of random and interval inputs on reliability associated with performance characteristics. The proposed reliability analysis framework consists of two components — direct reliability analysis and inverse reliability analysis. The algorithms are based on the First Order Reliability Method and many existing reliability analysis methods. The efficient and robust improved HL-RF method is further developed to accommodate interval variables. To deal with interval variables for black-box functions, nonlinear optimization is used to identify the extreme values of a performance characteristic. The direct reliability analysis provides bounds of a probability of failure; the inverse reliability analysis computes the bounds of the percentile value of a performance characteristic given reliability. One engineering example is provided."
942,"Engineering design under uncertainty has gained considerable attention in recent years. There exist two different types of uncertainties in practical engineering applications: aleatory uncertainty that is classified as objective and irreducible uncertainty with sufficient information on input uncertainty data and epistemic uncertainty that is a subjective and reducible uncertainty that is caused by the lack of knowledge on input uncertainty data. Among several alternative tools to handle uncertainty, evidence theory has proved to be computationally efficient and stable tool for reliability analysis and design optimization under aleatory and/or epistemic uncertainty involved in engineering systems. This paper attempts to give a better understanding of uncertainty in engineering design with a general overview. The overview includes theoretical research, computational development, and performable ability consideration of evidence theory during recent years. At last, perspectives on future research are stated."
943,"A reliability analysis method is presented for time-dependent systems under uncertainty. The system response is modeled as a parameterized random process. A double-loop optimization algorithm is used. The inner loop calculates the maximum response in time using a global-local search method, and transforms a time-dependent problem into a time-independent one. The outer loop calculates multiple most probable points (MPPs) which are commonly encountered in vibration problems. The dominant MPPs with the highest contribution to the probability of failure are identified. A niching genetic algorithm is used because of its ability to simultaneously identify multiple solutions. All potential MPPs are initially identified approximately and their location is efficiently refined using a gradient-based optimizer with local metamodels. Among all MPPs, the significant ones are identified using a correlation analysis. Approximate limit states are built at the identified MPPs, and the system failure probability is estimated using bi-modal bounds. The vibration response of a cantilever plate under random oscillating pressure load and a point load illustrates the proposed method. The finite-element model is used to calculate the response."
944,"An efficient Monte Carlo reliability assessment methodology is presented for engineering systems with multiple failure regions and potentially multiple most probable points. The method can handle implicit, nonlinear limit-state functions, with correlated or non-correlated random variables, which can be described by any probabilistic distribution. It uses a combination of approximate or “accurate-on-demand,” global and local metamodels which serve as indicators to determine the failure and safe regions. Samples close to limit states define transition regions between safe and failure domains. A clustering technique identifies all transition regions which can be in general disjoint, and local metamodels of the actual limit states are generated for each transition region. A Monte Carlo simulation calculates the probability of failure using the global and local metamodels. A robust maximin “space-filling” sampling technique is used to construct the metamodels. Also, a principal component analysis addresses the problem dimensionality making therefore, the proposed method attractive for problems with a large number of random variables. Two numerical examples highlight the accuracy and efficiency of the method."
945,"The issue of this paper is the use of perceptual evaluations of products as a simulation platform for improving decision making in the design process of a new product. In previous work, we proposed bayesian kansei models using "
946,"There are two commonly used reliability analysis methods of analytical methods: linear approximation - First Order Reliability Method (FORM), and quadratic approximation - Second Order Reliability Method (SORM), of the performance functions. The reliability analysis using FORM could be acceptable for mildly nonlinear performance functions, whereas the reliability analysis using SORM is usually necessary for highly nonlinear performance functions of multi-variables. Even though the reliability analysis using SORM may be accurate, it is not desirable to use SORM for probability of failure calculation since SORM requires the second-order sensitivities. Moreover, the SORM-based inverse reliability analysis is very difficult to develop. This paper proposes a method that can be used for multi-dimensional highly nonlinear systems to yield very accurate probability of failure calculation without requiring the second order sensitivities. For this purpose, the univariate dimension reduction method (DRM) is used. A three-step computational process is proposed to carry out the inverse reliability analysis: constraint shift, reliability index (β) update, and the most probable point (MPP) approximation method. Using the three steps, a new DRM-based MPP is obtained, which computes the probability of failure of the performance function more accurately than FORM and more efficiently than SORM."
947,"For the performance measure approach (PMA) of RBDO, a transformation between the input random variables and the standard normal random variables is necessary to carry out the inverse reliability analysis. For reliability analysis, Rosenblatt and Nataf transformations are commonly used. In many industrial RBDO problems, the input random variables are correlated. However, often only limited information such as the marginal distribution and covariance could be practically obtained, and the input joint probability distribution function (PDF) is very difficult to obtain. Thus, in literature, most RBDO methods assume all input random variables are independent. However, in this paper, it is found that the RBDO results can be significantly different when the input variables are correlated. Thus, various transformation methods are investigated for development of a RBDO method for problems with correlated input variables. It is found that Rosenblatt transformation is impractical for problems with correlated input variables due to difficulty of constructing a joint PDF from the marginal distributions and covariance. On the other hand, Nataf transformation can construct the joint CDF using the marginal distributions and covariance, and thus applicable to problems with correlated random input variables. The joint CDF is Nataf model, which is called a Gaussian copula in the copula family. Since the Gaussian copula can describe a wide range of the correlation coefficient, Nataf transformation can be widely used for various types of correlated input variables. In this paper, Nataf transformation is used to develop a RBDO method for design problems with correlated random input variables. Numerical examples are used to demonstrate the proposed method. Also, it is shown that the correlated random input variables significantly affect the RBDO results."
948,"Whereas the robust design concept has been well established in the probability theory, it has not been developed in the possibility theory. For problems where accurate statistical information for input data is not available, a possibility-based (or fuzzy set) robust design concept is proposed in this paper by investigating the similarity between the membership function of the fuzzy variable and the cumulative distribution function of the random variable. Based on the probability-possibility consistency principle, a random variable that corresponds to the fuzzy variable is introduced in this paper in order to define the robust design concept for the fuzzy variable. For the system with input fuzzy variables, the robustness measure of the output performance is computed using the performance measure integration (PMI) method, while the integration points are obtained from the inverse possibility analysis by using the maximal possibility search method with interpolation (MPS). For the system with mixed random and fuzzy input variables, the robustness measure of the output performance is computed using PMI method, with the integration points obtained from the inverse mixed analysis by using the maximal failure search method (MFS). A new mixed (random and fuzzy) variable robust design optimization (MVRDO) method is proposed and several numerical examples are used to verify the robust design concept in the possibility theory and the MVRDO formulation."
949,"The performance of a product that is being designed is affected by variations in material, manufacturing process, use, and environmental variables. As a consequence of uncertainties in these factors, some items may fail. Failure is taken very generally, but we assume that it is a random event that occurs at most once in the lifetime of an item. The designer wants the probability of failure to be less than a given threshold. In this paper, we consider three approaches for modeling the uncertainty in whether or not the failure probability meets this threshold: a classical approach, a precise Bayesian approach, and a robust Bayesian (or imprecise probability) approach. In some scenarios, the designer may have some initial beliefs about the failure probability. The designer also has the opportunity to obtain more information about product performance (e.g. from either experiments with actual items or runs of a simulation program that provides an acceptable surrogate for actual performance). The different approaches for forming and updating the designer’s beliefs about the failure probability are illustrated and compared under different assumptions of available information. The goal is to gain insight into the relative strengths and weaknesses of the approaches. Examples are presented for illustrating the conclusions."
950,"In design optimization problems under uncertainty, two conflicting issues are generally of interest to the designer: feasibility and optimality. In this research, we adopt the philosophy that design, especially under uncertainty, is a decision making process, where the associated tradeoffs can be conveniently understood using multiobjective optimization. The importance of constraint feasibility and the associated tradeoffs, especially in the presence of equality constraints, is examined in this paper. We propose a three-step decision making framework that facilitates effective decision making under uncertainty: (1) formulating a multiobjective problem that effectively models the tradeoffs under uncertainty, (2) generating design alternatives by solving the proposed multiobjective robust design formulation, and (3) choosing a final design using filtering and constraint uncertainty visualization schemes. The proposed framework can be used to systematically explore the design space from a constraint tradeoff perspective. A tolerance synthesis example is used to illustrate the proposed decision making process."
951,"The objective of this paper is to provide a method of safely estimating reliability based on small samples. First, it is shown that the commonly used estimators of the parameters of the normal distribution function are biased, and they tend to lead to unconservative estimates of reliability. Then, two ways of making this estimation conservative are proposed: (1) adding constraints when a distribution is fitted to the data to bias it to be conservative, and (2) using the bootstrap method to estimate the bias needed for a given level of conservativeness. The relationship between the accuracy and the conservativeness of the estimates is explored for a normal distribution. In particular, detailed results are presented for the case when the goal is 95% likelihood to be conservative. The bootstrap approach is found to be more accurate for this level of conservativeness. It is then applied to the reliability analysis of a composite panel under thermal loading. Finally, we explore the influence of sample sizes and target probability of failure on estimates quality, and show that for a constant level of conservativeness, small samples and low probabilities can lead to a high risk of large overestimation while this risk is limited to a very reasonable value for samples above."
952,"Reliability-based design optimization (RBDO) is intrinsically a double-loop procedure since it involves an overall optimization and an iterative reliability assessment at each search point. Due to the double-loop procedure, the computational expense of RBDO is normally very high. Current RBDO research is focused on performance functions having explicit analytical expression and readily available gradients. This paper addresses a more challenging type of RBDO problem in which the performance functions are computation intensive. These computation intensive functions are often considered as a “black-box” and their gradients are not available or not reliable. Based on the reliable design space (RDS) concept proposed earlier by the authors, this paper proposes a Reliable Space Pursuing (RSP) approach, in which RDS is first identified and then gradually refined while optimization is performed. It theoretically avoids the nested optimization and probabilistic assessment loop. This approach can apply to RBDO problems with either analytical or blackbox performance functions. Three well known numerical problems from the literature are used to test and demonstrate the effectiveness of RSP."
953,"In the last decade, considerable advances have been made in Reliability-Based Design Optimization (RBDO). It is assumed in RBDO that statistical information of input uncertainties is completely known (aleatory uncertainty), such as a distribution type and its parameters (e.g., mean, deviation). However, this assumption is not valid in practical engineering applications, since the amount of uncertainty data is restricted mainly due to limited resources (e.g., man-power, expense, time). In practical engineering design, most data sets for system uncertainties are insufficiently sampled from unknown statistical distributions, known as epistemic uncertainty. Existing methods in uncertainty based design optimization have difficulty in handling both aleatory and epistemic uncertainties. To tackle design problems engaging both epistemic and aleatory uncertainties, this paper proposes an integration of RBDO with Bayes Theorem, referred to as Bayesian Reliability-Based Design Optimization (Bayesian RBDO). However, when a design problem involves a large number of epistemic variables, Bayesian RBDO becomes extremely expensive. Thus, this paper presents a more efficient and accurate numerical method for reliability method demanded in the process of Bayesian RBDO. It is found that the Eigenvector Dimension Reduction (EDR) Method is a very efficient and accurate method for reliability analysis, since the method takes a sensitivity-free approach with only 2"
954,"The method of Statistical Sensitivity Analysis (SSA) is playing an increasingly important role in engineering design, especially with the consideration of uncertainty. However, applying SSA to the design of complex engineering systems is not straight forward due to both computational and organizational difficulties. In this paper, a Hierarchical Statistical Sensitivity Analysis (HSSA) method is developed to facilitate the application of SSA to the design of complex systems especially those follow hierarchical modeling structures. A top-down strategy for HSSA is introduced to only invoke the SSA of critical submodels based on the significance of submodel performances. A simplified formulation of the Global Statistical Sensitivity Index (GSSI) is studied to represent the effect of a lower-level submodel input on a higher-level model response by aggregating the submodel SSA results across intermediate levels. A sufficient condition under which the simplified formulation provides an accurate solution is derived. To improve the accuracy of the GSSI formulation for a general situation, a modified formulation is proposed by including an Adjustment Coefficient ("
955,"It is an important step in deign under uncertainty to select an appropriate uncertainty propagation (UP) method considering the characteristics of the engineering systems at hand, the required level of UP associated with the probabilistic design scenario, and the required accuracy and efficiency levels. Many uncertainty propagation methods have been developed in various fields, however, there is a lack of good understanding of their relative merits. In this paper, a comparative study on the performances of several UP methods, including a few recent methods that have received growing attention, is performed. The full factorial numerical integration (FFNI), the univariate dimension reduction method (UDR), and the polynomial chaos expansion (PCE) are implemented and applied to several test problems with different settings of the performance nonlinearity, distribution types of input random variables, and the magnitude of input uncertainty. The performances of those methods are compared in moment estimation, tail probability calculation, and the probability density function (PDF) construction. It is found that the FFNI with the moment matching quadrature rule shows good accuracy but the computational cost becomes prohibitive as the number of input random variables increases. The accuracy and efficiency of the UDR method for moment estimations appear to be superior when there is no significant interaction effect in the performance function. Both FFNI and UDR are very robust against the non-normality of input variables. The PCE is implemented in combination with FFNI for coefficients estimation. The PCE method is shown to be a useful approach when a complete PDF description is desired. Inverse Rosenblatt transformation is used to treat non-normal inputs of PCE, however, it is shown that the transformation may result in the degradation of accuracy of PCE. It is also shown that in black-box type of system the performance and convergence of PCE highly depend on the method adopted to estimate its coefficients."
956,"Researchers desire to evaluate system reliability uniquely and efficiently. Despite years of research, little progress has been made on system reliability analysis. Up to now, bound methods for system reliability prediction have been dominant. For system reliability bounds, the probabilities of the second or higher order joint events are assumed to be known exactly although there is no numerical method to evaluate them effectively. Two primary challenges in system reliability analysis are how to evaluate the probabilities of the second or higher order joint events and how to uniquely obtain the system reliability so that the system reliability can be used for Reliability-Based Design Optimization (RBDO). This paper proposes the Complementary Interaction Method (CIM) to define system reliability in terms of the probabilities of the component events, "
957,"This paper presents an innovative approach for quality engineering using the Eigenvector Dimension Reduction (EDR) Method. Currently industry relies heavily upon the use of the Taguchi method and Signal to Noise (S/N) ratios as quality indices. However, some disadvantages of the Taguchi method exist such as, its reliance upon samples occurring at specified levels, results to be valid at only the current design point, and its expensiveness to maintain a certain level of confidence. Recently, it has been shown that the EDR method can accurately provide an analysis of variance, similar to that of the Taguchi method, but is not hindered by the aforementioned drawbacks of the Taguchi method. This is evident because the EDR method is based upon fundamental statistics, where the statistical information for each design parameter is used to estimate the uncertainty propagation through engineering systems. Therefore, the EDR method provides much more extensive capabilities than the Taguchi method, such as the ability to estimate not only mean and standard deviation of the response, but also the skewness and kurtosis. The uniqueness of the EDR method is its ability to generate the probability density function (PDF) of system performances. This capability, known as the probabilistic “what-if” study, provides a visual representation of the effects of the design parameters (e.g., its mean and variance) upon the response. In addition, the probabilistic “what-if” study can be applied across multiple design parameters, allowing the analysis of interactions among control factors. Furthermore, the implementation of the probabilistic “what-if” study provides a basis for performing robust design optimization. Because of these advantages, it is apparent that the EDR method provides an alternative platform of quality engineering to the Taguchi method. For easy execution by field engineers, the proposed platform for quality engineering using the EDR method, known as Quick Quality Quantification (Q3 ), will be developed as a Microsoft EXCEL add-in."
958,"This paper attempts to integrate a derivative-free probability analysis method to Reliability-Based Robust Design Optimization (RBRDO). The Eigenvector Dimension Reduction (EDR) method is used for the probability analysis method. It has been demonstrated that the EDR method is more accurate and efficient than the Second-Order Reliability Method (SORM) for reliability and quality assessment. Moreover, it can simultaneously evaluate both reliability and quality without any extra expense. Three practical engineering problems (vehicle side impact, layered bonding plates, and lower control arm) are used to demonstrate the effectiveness of the EDR method."
960,"The purpose of this paper is to demonstrate the application of Differential Evolution to a realistic design optimization test problem. The present contribution regards the improvements implemented to the original basic algorithm as well as the application of a new algorithm for dealing with the unique challenges associated with real world optimization problems. The selected example is a three-dimensional vehicular structure optimization problem modeled using the commercial Finite Element software ANSYS®  that has a combination of continuous and discrete design variables. The use of traditional gradient-based optimization algorithms is thus not practical. The numerical results presented indicate that the Differential Evolution algorithm is able to find the optimum design for the proposed problem. The algorithm is robust in the sense that it is capable of dealing with the numerical noise involved in the modeling of the system and to manipulate discrete design variables, accordingly."
961,"Novel honeycomb tessellation and material mask overlay methods are proposed in this paper to obtain optimal planar compliant topologies free from checkerboard and point flexure pathologies. A cardinal reason, namely the presence of strain-free rotation regions in rectangular cell based discretization is identified to be a cause in appearance of such singularities. With each hexagonal cell sharing an edge with its neighboring cells, strain-free displacements are not permitted anywhere in the continuum. The new material assignment approach manipulates material within a group of cells as opposed to a single cell thereby reducing the number of variables making optimization efficient. Optimal solutions obtained are free from intermediate material states and can be manufactured directly after design, without requiring any post processing. The proposed procedure is illustrated using two classical examples in 2D compliant mechanisms solved using genetic algorithm."
962,"In this paper, topology optimization is used to study the design of piezoelectric actuator with in-plane motion. Two case studies are reported, the maximization of the in-plane motion toward a pre-defined direction and the maximization of the output force. In addition to volume density as design variable used in the conventional topology optimization, a new design variable, electrode density, is introduced to model the electrode topology on the piezoelectric plate surface. Based on the electrode potential model, the relation between the nodal potential and the electrode density is established. Sensitivity analyses of objective function with respect to volume density and electrode density are derived from the adjoint method. Examples of optimized piezoelectric actuators from the proposed method are presented and discussed."
963,"The use of Evolutionary Computations (EC’s) has become one of the primary methods in the field of automated design synthesis. The overwhelming majority of EC’s in use today use a direct encoding, where an individual is described by its gene string. This means that every engineering domain must create its own encoding scheme, making implementation in new fields difficult and slow. Additionally, direct encoding does not produce symmetry or modularity, unless these attributes are written into the encoding scheme "
964,"This paper describes a virtual reality application that performs fast stress reanalysis coupled with virtual reality and haptics that allows rapid evaluation of multiple designs throughout the product design process. The Interactive Virtual Design Application (IVDA) allows the engineer to interactively explore new design geometry while simultaneously examining the finite element analysis results. In the presence of other parts in the assembly, the new shape can be analyzed and modified, taking into consideration mating part fits. This approach supports concurrent product design and assembly methods prototyping. A “two-step” approach utilizing Taylor series approximations and Pre-conditioned Conjugate Gradient methods is used to perform quick reanalysis during interactive shape modification. The virtual environment provides an immersive three-dimensional workspace. Haptics are used to provide feedback of the stress gradient as the part geometry is changed, thus facilitating the designer’s understanding of the impact of shape change on product performance."
965,"Recent advancements in computing power and speed provide opportunities to revolutionize trade space exploration, particularly for the design of complex systems such as automobiles, aircraft, and spacecraft. In this paper, we introduce three Visual Steering Commands to support trade space exploration and demonstrate their use within a powerful data visualization tool that allows designers to explore multidimensional trade spaces using glyph, 1-D and 2-D histogram, 2-D scatter, scatter matrix, and parallel coordinate plots; linked views; brushing; preference shading and Pareto frontier display. In particular, we define three user-guided samplers that enable designers to explore (1) the entire design space, (2) near a point of interest, or (3) within a region of high preference. We illustrate these three samplers with a vehicle configuration model that evaluates the technical feasibility of new vehicle concepts. Future research is also discussed."
966,"Virtual Reality (VR) technologies provide novel modes of human computer interaction that can be used to support industrial design processes. The integration can be successful if supported by a method to qualify, select and design the VR technologies according to the company’s requirements in order to improve collaboration in extended enterprises and timesaving. The aim of the present work is the definition of a method to translate the company’s expectations into heuristic values that allow the benchmarking of VR systems. The method has been tested on a real test case."
967,"Functional analysis of systems is a common engineering application during different stages of design. Conceptual designers as well as post-development designers use the process to gather useful information about the system that is under consideration. The functional basis and component taxonomy are collective approaches to describe these systems in unique languages. Since many designers naturally think in terms of physical components, it is more difficult for them to grasp fundamental concepts necessary to functionally model a system properly. A new design instrument, component functional templates, has been developed as a means to link the functional basis and component taxonomy together in one coherent visual form that can be used by novice designers as an invaluable skill-building tool. Principal components analysis (PCA) is used to extract historical data from many consumer products whose design information has been stored in an online repository produced by the UMR Design Engineering Lab. This paper presents the approach and derivation of the templates, along with valid examples of template groupings that result from the analysis. An application of the templates is presented in a case study on the drive train of a bicycle where the templates prove to sufficiently begin the modeling process and provide room for unique manipulation that accurately describes functional requirements of the subsystem."
968,"Currently, new product concepts are evaluated by developing detailed virtual part and assembly models with traditional Computer Aided Design (CAD) tools followed by appropriate analyses (e.g., finite element analysis, computational fluid dynamics, etc.). The creation of these models and analyses are tremendously time consuming. If a number of different conceptual configurations have been determined, it may not be possible to model and analyze each of them. Thus, promising concepts might be eliminated based solely on insufficient time to assess them. In addition, the virtual models and analyses performed are usually of much higher detail and accuracy than what is needed for such early assessment. By eliminating the time-consuming complexity of a CAD environment and incorporating qualitative assessment tools, engineers could spend more time evaluating additional concepts, which were previously abandoned due to time constraints. In this paper, a software framework, the Advanced Systems Design Suite (ASDS), for creating and evaluating conceptual design configurations in an immersive virtual reality environment is presented. The ASDS allows design concepts to be quickly modeled, analyzed, and visualized. It incorporates a PC user interface with an immersive virtual reality environment to ease the creation and assessment of conceptual design prototypes. The development of the modeling and assessment tools are presented along with a test case to demonstrate the usability and effectiveness of the framework."
969,"Critical Parameter Management (CPM) is an emerging area in engineering design owing to the motivation from Design for Six Sigma. Introduced in this paper is the exploration of CPM to support rapid redesign of "
970,"Currently available tools for classical topology optimization of structures have proven valuable in conceptual design. These tools may provide design direction very early in the design cycle. However, the results subsequently need to be interpreted and translated by an engineer into a consistent CAD-model. This research focuses on the topological design synthesis of shell structures, which is being carried out using a design language approach. The aim of this approach is to automatically generate, modify and optimize an abstract representation of the design. This representation is automatically translatable into a CAD-model and will thus lead to an optimization process that offers a valid structural CAD-model as result. Design languages serve in this context as a computable abstraction of design representation and synthesis by use of rule-based information processing mechanisms. These rules (also called design patterns), are applied to generate and modify the topology of the design representation. Design patterns contain the engineers know-how and best-practice. The computerized execution of design patterns in a design compiler yields a powerful topology modification tool. As prototype application, the synthesis of shell structures is presented in this work. The automation mechanisms and the information flow through design synthesis, model generation, design analysis and evaluation are outlined. A discussion on the future application of design patterns for knowledge-based structural optimization is derived from the shown examples."
971,"As an essential part of design, the conceptual design needs computer assistance from its initial design stage of product development. However computer-aided conceptual design is limited by the current function-to-form mapping approaches. A new growth design process model is proposed in this paper in which geometrical solutions grow gradually from scratch to their complete configurations. Two theoretical design principles, Decomposition & Reconstitution (D&R) principle and Cell Division principle, are briefly introduced to provide better understanding of Growth Design model. The concept of Conceptual Structure is used to support the transition process from conceptual design to detailed design in the growth design process. Finally, an application case is introduced to show the effects of the growth design model."
972,"Conceptual design is important but complex. Its success heavily depends on a designer’s individual experience and intuition. Design support tools are in need to assist designers to improve design quality and efficiency. However, to date there are few computational tools that are mature enough to provide effective assistance for design concept generation. One of the major reasons is that design information is inherently incomplete and subjective at the early stage of design. No effective evaluation methods have been devised to assess the connectivity between means (i.e., sub-solutions or function carriers) although it has an impact effect on system performance. In this paper, we propose a fuzzy reference model for conceptual design evaluation as part of our hierarchical co-evolutionary design concept generation based on function-means connectivity. An example of designing a simple mechanical transporter is presented to demonstrate the proposed approach."
973,"This paper outlines a framework for applying a genetic algorithm to the selection of component variants between the conceptual and detailed design stages of product development. A genetic algorithm (GA) is defined for the problem and an example is presented that demonstrates its application and usefulness. Functional modeling techniques are used to formulate the design problem and generate the chromosomes that are evaluated with the algorithm. In the presented example, suitable GA parameters and the break-even point where the GA surpassed an enumerated search of the same solution space were found. Recommend uses of the GA along with limitations of the method and future work are presented as well."
974,"Early conceptual design is one of the most important stages in vehicle product development. At this stage, vehicle design information is limited. In many cases, historical or benchmark vehicle data are used as surrogates in decision making. Some of the legacy components may be reused in the new vehicle design in order to reduce development and manufacturing costs. Nowadays, parametric modeling methods have been developed and employed for conceptual vehicle design. However, how to quickly reuse the previous design and legacy vehicle data in parametric design is still a challenge. A hybrid method for parametric conceptual vehicle design is presented in this paper, which uses both legacy components and parametric surfaces. This method allows easy reuse of historical data together with new parametric surfaces for early vehicle design. It enables mix and match of legacy components with newly designed parametric surfaces in representing a new design. This method provides a systematic way to enforce the commonality and reusability in a vehicle design."
975,"Engineering analysis methods, such as the finite element method, are employed extensively to optimize complex engineering designs, but their success in conceptual product development is rather limited since numerous designs must be analyzed to cover the design space, and unfortunately, modern analysis methods can be tedious and time consuming in such scenarios. We propose here a novel analysis methodology for conceptual design wherein, given the simulation results and performance of one of the designs, one predicts upper and lower bounds on the performance of geometrically similar designs. The methodology rests on sound mathematical principles such as adjoint theory of boundary value problems, and is partly motivated by recent work on shape similarity exploitation in manufacturing wherein the cost of manufacturing a new part is estimated by retrieving the manufacturing costs of geometrically similar parts."
976,"A useful way to generate solutions to engineering design problems is to compare the solutions of design problems similar to the one at hand and validate the solutions to satisfy the new design requirements. This process involves evaluating the similarity between the design problem at hand and the various design problems in the repository. The purpose of this paper is to investigate the meaning and the use of similarity in engineering design. Various similarity theories in literature have been explored. Previous applications of these theories are limited to the retrieval of similar computer-aided design models and process plans. This paper extends the applications of these theories to the various stages of the design process."
977,"Risk is a crucial criterion for decision making among multiple stakeholders negotiating for an agreement in a distributed environment. The challenge here is that risk may have different meanings and implications to different stakeholders, and this creates considerable barriers to effective negotiation and coordination in collaborative design. Our goal is to 1) capture the heterogeneous risk information at intra- and inter- stakeholder levels, 2) represent them using a uniform structure based on a function-failure relationship, and 3) enable the negotiation of the risk information among the multiple stakeholders through this uniform structure. Though a significant number of existing methods for risk analysis and management have been developed, these methods mainly focus on the local domain of a certain single stakeholder, and few have considered the possible influence and variations related to global aspects that is important for negotiation among multiple, distributed stakeholders. This work develops intra-level risk property tables to capture and represent the various risk evaluations from individual members in a single stakeholder; and then inter-level risk property tables are formed based on the synthesis of the various intra-level risk properties into a group representation for the single stakeholder, which is directly used in global negotiation and coordination with other stakeholders. An adjustable approach is used in our work to enable the adjustability of the intra- and inter- level risk evaluations via negotiation. An example problem from a NSF/NEES-sponsored research collaborative network is used to demonstrate the use of this method. The preliminary results show that this method has potential in enabling local risk assessment to support global negotiation and coordination in a distributed, collaborative environment."
978,"The focus of this paper is on studying the convergence properties of the solution process of decentralized or distributed subsystems, where each subsystem has its own design problem, including objective(s), constraints, and design variables. The challenging aspect of this type of problem comes in the coupling of the subsystems, which create complex research and implementation challenges in modeling and solving these types of problems. We focus on the dynamics of these distributed design problems and attempt to further the understanding of the fundamental mechanics behind these processes in order to support the decisions being made by a network of decision makers. In this work, the domain of attraction, or region where convergence to a stable equilibrium point is guaranteed, of a decentralized design process is studied. Two approaches based on concepts from nonlinear control theory are presented: the first determines the domain of attraction for a specified Lyapunov function and the second optimizes for a Lyapunov function which maximizes the domain of attraction. The two techniques are illustrated on a benchmark pressure vessel design problem."
979,"Front-loading problem solving is a strategy that seeks to improve product development performance by shifting the identification and solving of design problems to the earlier phases of product development process. Front-loading reduces the development time by speeding up problem solving process and eliminating the total number of problems solved in a project. Furthermore, it supports intelligent decision making through loading problem solving tasks with required pieces of knowledge. In this work, front-loading is studied from a conceptual view point. Also different approaches for front-loading are investigated and classified. To support the discussions with practical examples, front-loading in studied in the context of engine engineering."
980,"This paper outlines a research instrument developed to analyze the relationship between communication modes, leadership styles, and team composition. The instrument is a survey that captures this information from collaborative design team members. This information can be correlated with team success, and the productive characteristics can be encouraged in future groups. The survey was distributed and analyzed in small numbers, and first round recommendations and student feedback are gathered. The developed instrument gathers background information on the student, group, and project. The project itself was defined in four stages: Problem Definition, Concept Generation, Concept Selection, and Concept Expansion. The students were asked questions about the team leadership style based on the Vroom-Yetton Model. The students were then asked how often they used various communication modes (verbal, textual, and graphical) when communicating Peer-to-Peer, Peer-to-Group, and Group as a Whole. These questions were repeated for each of the design stages. The instrument was structured and refined in order to analyze the behavior of undergraduate design students. It is intended to provide researchers and educators a tool to evaluate and critique collaborative behavior in order to streamline the design process. However, the survey was formulated broadly enough to be used in an industrial setting with small changes in the format to accommodate more experienced designers. The analysis of the original distribution revealed that the survey should be broken down into four parts corresponding to the four design stages and administered longitudinally."
981,"This paper addresses the design of a lower control arm of a sport utility vehicle. Four worst load cases were identified and for each of them the forces and moments at each of the three ends of the arm were obtained as input. There was also some space constraint within which the arm had to be designed. A finite-element based methodology is presented here to design a lower control arm. First Topology optimization is applied to get an overall approximate shape within the design space. This process is similar to finding out a load path for the structure to withstand the loads. This shape is then sized to withstand all the loads using classical shape optimization techniques. Shape vectors, which are the most important inputs for shape optimization, are generated for the different regions of the control arm. This case study illustrates the complete process of topology and shape optimizations to get a final design of the lower control arm."
982,"The National Highway Traffic Safety Administration (NHTSA) recently revised Federal Motor Vehicle Safety Standard (FMVSS) 202, which governs head restraints. The new standard, known as FMVSS 202a, establishes for the first time in the U.S. a requirement for the fore-aft position of the head restraint. The fore-aft distance between the head restraint and headform representing a midsize male occupant must not exceed 55 mm when measured with the seat back angle set to 25 degrees. The goal of the rule change is to reduce the incidence of whiplash-associated disorders caused by rear impacts. Moving the head restraint closer to the head prior to impact decreases the amount of relative motion between the occupants’ heads and torsos and is believed to decrease the risk of soft-tissue neck injury. As manufacturers phase in seats that meet the new criterion, some vehicle models are producing complaints from drivers that the head restraint causes discomfort by interfering with their preferred head position, forcing them to select a more reclined seat back angle than they would prefer. To address this issue, an analysis of driver head locations relative to the seat was conducted using a new optimization-based framework for vehicle interior optimization. The approach uses simulations with thousands of virtual occupants to quantity distributions of postural variables of interest. In this case, the analysis showed that smaller-stature occupants are disproportionately likely to experience head-position interference from a head restraint that is rigidly affixed to the seat back. Using an analysis approach that considers both postural and anthropometric variability, design guidelines for the kinematics of an articulated head restraint are proposed. Such a restraint would provide optimal head restraint positioning across occupant sizes while minimizing interference."
983,"This paper presents an efficient algorithm for developing vehicle structures for crashworthiness, based on the analyses of "
984,"Crashworthiness design is an evolving discipline that combines vehicle crash simulation and design synthesis. The goal is to increase passenger safety subject to manufacturing cost constraints. The crashworthiness design process requires the modeling of the complex interactions involved in a crash event. Current approaches utilize a parameterized optimization approach that requires response surface approximations of the design space. This is due to the expensive nature of numerical crash simulations and the high nonlinearity and noisiness in the design space. These methodologies usually require a significant design effort to determine an initial design. In this research, a non-gradient approach to topology optimization is developed for crashworthiness design. The methodology utilizes the cellular automata paradigm to generate a concept design."
985,"Recently there has been much interest in developing shape display systems, i.e. systems that can generate, based on a virtual representation such as a CAD file, a physical shape that a user can touch. The approach considered here is to automatically assemble a shape from simple, identical building blocks using a robot. This paper describes preliminary research results on generating assembly sequences for this purpose, using a rule-based approach in order to deal with the huge search space due to the large number of components. While many of the details of the rule-based approach are discussed elsewhere, this paper focuses on analyzing and comparing the use of two different methods, the assembly method and the disassembly method in the z-direction, for assembly sequence generation."
986,"This paper proposes a new process planning method to achieve intended product quality. The design for the product quality is achieved by total designing and planning of production process considering both good factors and bad factors that affect product quality in the production process. The representation model of quality related production knowledge that is directly used in the computational design method is proposed. The discrete quality related behavior of product in process is modeled. Input information of the proposed design method consists following information: 1) a product structure; 2) feasible operations of the production process; 3) possible quality related causalities in the production process. The design method calculates the space of quality state automatically by this method. Final output information from this method is a good production process (good operations and good sequence) that achieves the intended quality state of final product. In order to develop this production process planning method for keeping the intended quality, this paper proposes a computational representation model of the quality related information in production process. Based on the production process model, a computational recommendation algorithm of production process that can achieve the intended product quality state is developed. The representation, sharing and reuse of production knowledge and know-how are realized based on the proposed production process model. A prototype system for planning a production process without failures of final product is implemented for the purpose of examination and validation for proposed method. A practical design example of a production process planning for one component of an auto-circuit breaker is demonstrated. The methodology proposed in this research addresses: (1) how the product quality in production process should be modeled; (2) how to design good operations and a good process in production that can achieve the intended product quality; and (3) how to apply the proposed method to a practical planning problem of an actual production process."
987,"This study presents an optimum disassembly process with genetic algorithms. The disassembly sequence matrix including the interference matrix, the special connection relation and the contact matrix of a product are used to study the disassembly process. The genetic algorithms are utilized for the multi-objective function formed by the instability function, tool change function, disassembly direction function and inconvenient function due to the gravity to obtain the optimum disassembly process. The disassembly processes of a compressor are investigated and improved, and a redesigned compressor is recommended."
988,"Cost and product quality are significant attributes in manufacturing processes, such as multistation assembly. We use multiobjective optimization for integrated tolerance allocation and fixture layout design to address their interaction and to quantify tradeoffs among cost, product quality, and assembly process robustness. Design decisions relate to product tolerances, assembly process tolerances, and fixture locating positions. A nested optimization strategy is adopted, and the proposed methodology is demonstrated using a vehicle side frame assembly example. The obtained results provide evidence for the existence of tradeoffs, based on which we can identify critical quality and budget requirements."
989,"This paper outlines the requirements for employing visibility for CNC Rapid Prototyping (RP) and Rapid Manufacturing (RM), e.g., necessary conditions for CNC RP. Visibility, as used here, is with respect to line-of-sight access for the tool to the workpiece for machining access. This method forms the basis for the selection of an axis of rotation for which the workpiece would be completely visible and a step in the automation sequence for this process. A review of prior visibility work in the area of machining is included and specifically those related to CNC RP. Additionally, there is a discussion of a new software implementation of visibility and its methodology for CNC RP. The goal of this implementation is to develop a procedure that will reduce the set of all potential workpiece rotational axes to a subset that will provide full visibility. Results are presented for the software tool along with potential improvements for the future."
990,"In this paper, we propose a new type of mock-up consisting of a transparent solid object, fabricated by stereolithography, with a small video camera embedded inside. By processing the video image of the object surface taken from the inside in real time, operation data of the mock-up (e.g., when interfaces such as buttons and dials are operated by the users) and usage (e.g., which hand and fingers are used to operate the buttons and dials of the mock-up) data can be obtained. Consequently, a simple stereolithography model without sensors or circuits can be a functional mock-up. By analyzing these automatically obtained usage data, operation data and the result of functional simulation, some usability data will be obtained or extracted. Designers can use such data to improve the usability of the product that the mock-up imitates. The feasibility and validity of the concept is confirmed and reported on the basis of three experiments: a basic experiment using a simple mock-up consisting of a button and a dial, and two detailed experiments on usability data acquisition and the simulation of a Japanese-character-input interface using stereolithography mock-ups with a small fish-eye video camera inside."
991,"In this paper we propose a new method to determine the part orientation of a 3D mesh based on Principal Component Analysis (PCA). Although the idea and practice of using PCA to determine part orientation is not new, it is not without practical issues. A major drawback of PCA, when it comes to dealing with meshes comprised of nodes and elements, is that the results are "
992,"This article presents hierarchical particle swarm optimization algorithm to determine optimal part orientation for the complex parts produced by fused deposition modeling. Owing to the importance of efficient prototyping during product development phase, it is necessary to address critical issues involved in fused deposition modeling. The best part orientation is explored by taking into account volumetric error, build cost and orientational efficiency in the form of an aggregate objective. This paper exploits the search efficiency exhibited by hierarchical particle swarm optimization (HPSO), an optimization algorithm working on the basis of swarm intelligence, with the aim to resolve underlying optimal part orientation (OPO) problem. Further, in order to establish efficacy of HPSO a comparative study has been performed with a genetic algorithm based search. The results indicate outperforming behavior of HPSO and thus it is claimed to be a viable and efficient alternative to OPO problem."
993,"Complete CAD modelling by 3D digitizers comprises of full coverage scanning of the part from a number of views in the Reverse Engineering process. Different scanned views are required to be registered into a cohesive coordinate system to generate a practical CAD model. In this paper a high accuracy registration method is introduced in which the preregistered views, obtained by a conventional registration method are used as a base for fine registration. This is based on the relationships of the scanned views in the Laser Scanned model and the scanned features created by the Coordinate Measurement Machine. This method is most suitable for Reverse Engineering of precision parts with accurate and standard machined geometric features and complex but less accurate casting features. Experimental results of the method application on two components are presented."
994,"A new mathematical model for representing the geometric variations of lines is extended to include probabilistic representations of 1-D clearance which arise from multidimensional variations of an axis, a hole and a pin-hole assembly. The model is compatible with the ASME/ANSI/ISO Standards for geometric tolerances. Central to the new model is a Tolerance-Map (T-Map), a hypothetical volume of points that models the 3-D variations in location and orientation for a segment of a line (the axis), which can arise from tolerances on size, position, orientation, and form. Here it is extended to model the increase in yield that occurs when maximum material condition (MMC) is specified. The frequency distribution of 1-D clearance is decomposed into manufacturing bias, i.e. toward certain regions of a Tolerance-Map, and into a geometric bias that can be computed from the geometry of multidimensional T-Maps. Although the probabilistic representation in this paper is focused on geometric bias and manufacturing bias is presumed to be uniform, the method is robust enough to include manufacturing bias in the future. Geometric bias alone shows a greater likelihood of small clearances than large clearances between an assembled pin and hole."
995,"A significant amount of research efforts has been given to explore the mathematical basis for 3D dimensional and geometric tolerance representation, analysis, and synthesis. However, engineering semantics is not maintained in these mathematic models. It is hard to interpret calculated numerical results in a meaningful way. In this paper, a new semantic tolerance modeling scheme based on modal interval is introduced to improve interpretability of tolerance modeling. With logical quantifiers, semantic relations between tolerance specifications and implications of tolerance stacking are embedded in the mathematic model. The model captures the semantics of physical property difference between rigid and flexible materials as well as tolerancing intents such as sequence of specification, measurement, and assembly. Compared to traditional methods, the semantic tolerancing allows us to estimate true variation ranges such that feasible and complete solutions can be obtained."
996,"This paper employs quaternion biarcs to interpolate a set of orientations with angular velocity constraints. The resulting quaternion curve represents a piecewise spherical line-symmetric rational motion with C1  continuity. Since a quaternion arc corresponds to the motion of the planet gear in a special spherical epicyclic gear train, each segment of the quaternion biarcs can be realized with such an epicyclic gear train. Quaternion biarcs may be used to approximate B-spline quaternion curves that represent rational spherical motions that have applications in robot path planning, CAD/CAM, mechanism design, and computer graphics."
997,"Flank milling process is commonly applied in the aeronautical industry. It consists of manufacturing mechanical parts using the side of a machinning tool. This process is relevant to be less time consuming as it delivers better surface quality. However, flank milling can only be applied on ruled surfaces. In this article, we cover flank milling application on planar surfaces, a particular ruled surface type. In recent works we presented how to extract planar surfaces milling directions by using expertise provided through our industrial application. We take this study further, where we propose a validation for the proposed milling directions. This validation requires at first a translation of the problem from 3D to 2D. Then, by applying several proposed algorithms we extract for each direction its L-Zone. An L-Zone is the term we use to identify the unmachined part area using a particular milling direction. By intersecting the different L-Zones we obtain the G-Zone that consists of the total unmachined area. Computing the G-Zone for each planar surface indicates the ability of this surface to be flank milled. The proposed study is part of an effort to automate process planning of aeronautical parts. Automating this particular trade can result in a critical reduction of time, effort and costs in aeronautical industries, mainly due to having small production batch."
998,"An aerodynamic shape optimization method suitable for “inexpensive” centrifugal impellers and diffusers has been developed. The shapes are parameterized using non-uniform rational B-spline curves with special attention being paid to the blade’s edge profiles. A hybrid algorithm combining simulated annealing and a neural network is employed for collaborative optimization. The simulated annealing and neural network take turns in controlling the optimization processes, not only for maximizing the efficiency of global exploration, but also for minimizing the risks of automation failures or of reaching an incorrect optimum. A statistical analysis was also conducted using the neural network to extract design knowledge. By applying the proposed method to a centrifugal impeller and diffuser design problem, we obtained innovative shapes for the leading edge of the impeller and the trailing edge of the diffuser. Important design parameters related to the new shapes were identified through the design space analysis."
999,"This paper discusses a computational method for optimally allocating dimensional tolerances for an automotive pneumatic control valve. Due to the large production volume, costly tight tolerances should be allocated only to the dimensions that have high influence to the quality. Given a parametric geometry of a valve, the problem is posed as a multi-objective optimization with respect to product quality and production cost. The product quality is defined as 1) the deviation from the nominal valve design in the linearity of valve stroke and fluidic force, and 2) the difference in fluidic force with and without cavitation. These quality measures are estimated by using Monte Carlo simulation on a Radial-Basis Function Network (RBFN) trained with computational fluid dynamics (CFD) simulation of the valve operation. The production cost is estimated by the tolerance-cost relationship obtained from the discrete event simulations of valve production process. A multi-objective genetic algorithm is utilized to generate Pareto optimal tolerance allocations with respect to these objectives, and alternative tolerance allocations are proposed considering the trade-offs among multiple objectives."
1000,"Chemical Vapor Deposition (CVD) process is simulated and optimized for the deposition of a thin film of silicon from silane. The key focus is on the rate of deposition and on the quality of the thin film produced. The intended application dictates the level of quality need for the film. Proper control of the governing transport processes results in large area film thickness and composition uniformity. A vertical impinging CVD reactor is considered. The goal is to optimize the CVD system. The effect of important design parameters and operating conditions are studied using numerical simulations. Then Compromise Response Surface Method (CRSM) is used to model the process over a range of susceptor temperature and inlet velocity of the reaction gases. The resulting response surface is used to optimize the CVD system."
1001,"In mass customization, data mining can be used to extract valid, previously unknown, and easily interpretable information from large product databases in order to improve and optimize engineering design and manufacturing process decisions. A product family is a group of related products based on a product platform, facilitating mass customization by providing a variety of products for different market segments cost-effectively. In this paper, we propose a method for identifying a platform along with variant and unique modules in a product family using data mining techniques. Association rule mining is applied to develop rules related to design knowledge based on product function, which can be clustered by their similarity based on functional features. Fuzzy c-means clustering is used to determine initial clusters that represent modules. The clustering result identifies the platform and its modules by a platform level membership function and classification. We apply the proposed method to determine a new platform using a case study involving a power tool family."
1002,"Recent manufacturers have been utilizing product families to diversify and enhance the product performance by simultaneously designing multiple products under commonalization and standardization. Design information of product architecture and family is inevitably more complicated and numerous than that of a single product. Thus, more sophisticated computer-based support system is required for product architecture and family design. This paper proposes a knowledge model for a computer-based system to support reflective process of designing product architecture and product family. This research focuses on three problems which should be overcome when product family are modeled in the computer system; design repository without data redundancy and incorrectness, knowledge acquisition without forcing the additional effort on the designer, and integration of prescriptive models to support early stages of the design process. An ontology that is a foundation of a knowledge model is defined to resolve these problems. An example of designing an air conditioner product family is shown to demonstrate the capability of the system."
1003,"A number of software tools exist to assist the designer during the design process. These include tools for solid modeling of components, programs for simulating complex systems, for generating machining code and so on. However, a closer examination reveals that most of these tools are of use in the later stages in the design process. Even though design activity analyses showed the initial phases of design to have the maximum impact on the successful design of a product, few tools exist to support the needs of the designer during these critical stages. This paper documents the development of a design support tool (DIST) based on an analysis of a collaborative design activity. Then, results of an experiment are shared, which was designed to investigate DIST’s effectiveness during conceptual design by novice designers."
1004,"Automating redesign is an approach for engineering designers to prevent design related manufacturability problems in early product development and thus reduce costly design iterations. A vast amount of work exists, with most research findings seemingly staying within the research community rather than finding its way into use in industrial settings where research issues have often evolved from the concerned applied research. The aim of this paper is to present an approach with industrial implementation potential regarding automating redesign of sheet-metal components in early product development to avoid manufacturing problems due to design flaws and non-optimal designs. Geometry, generated by a knowledge-based engineering (KBE) system, gives input to the case-based reasoning (CBR) governed manufacturing planning. If geometry is found non-manufacturable or enhancement of already manufacturable geometry is possible, the CBR system will suggest redesign actions to resolve the problem. CBR extends the capabilities of the rule-based KBE-system by enabling plan-based evaluation. The approach has the potential for industrial implementation, since KBE is often closely coupled to an industrial CAD-system, hence enabling technology is at the industry. Also, combining KBE and CBR reduces the coding effort compared to coding the whole design support with CBR, as feature recognition is simplified by means of KBE. A case study of development of sheet-metal manufactured parts at a Swedish automotive industry partner presents the method in use. As it is shown that redesign can be automated for sheet-metal parts there is a potential for reducing costly design and manufacturing iterations."
1005,"Intelligent agents are becoming increasingly important in our society in applications as diverse as house cleaning robots, computer-controlled opponents in video games, unmanned aerial combat vehicles, entertainment robots, and autonomous explorers in outer space. However, the broader adoption of intelligent agents is often hindered by their limited adaptability to new tasks; when conditions change slightly, agents may quickly become confused. Additionally, a substantial engineering effort is required to design an agent for each new task. This paper presents an adaptable, general purpose intelligent agent toolkit based on reinforcement learning (RL), an approach with strong mathematical foundations and intriguing biological implications. RL algorithms are powerful because of their generality: agents simply receive a scalar reward value representing success or failure, which greatly simplifies the agent design process. Furthermore, these algorithms can be combined with other techniques (e.g., planning from a learned internal model) to improve learning efficiency. The design and implementation of an open source RL toolkit is presented here as a step towards the goal of general purpose agents. Experimental results show learning performance on several tasks, including two physical control problems."
1006,"Reverse engineering has gained importance over the past few years due to an intense competitive market aiding in the survivability of a company. This paper examines the reverse engineering process and what, how, and why it can assist in making a better design. Two well known reverse engineering methodologies are explored, the first by Otto and Wood and the second by Ingle. Each methodology is compared and contrasted according to the protocols and tools used. Among some of the reverse engineering tools detailed and illustrated are: Black box, Fishbone, Function Structure, Bill of Material, Exploded CAD models, Morphological Matrix, Subtract and Operate Procedure (SOP), House of Quality matrix, and FMEA. Even though both methodologies have highly valued tools, some of the areas in reverse engineering need additional robust tooling. This paper presents new and expanded tooling to augment the existing methods in hopes of furthering the understanding of the product, and process. Tools like Reverse Failure Mode and Effects Analysis (RFMEA), Connectivity graphs, and inter-relation matrix increase the design efficiency, quality, and the understanding of the reverse engineering process. These tools have been employed in two industry projects and one demonstrative purpose for a Design for Manufacture Class. In both of these scenarios, industry and academic, the users found that the augmented tools were useful in capturing and revealing information not previously realized."
1007,"This paper introduces a method for sequentially determining experiments in a “design of experiments” where optimization and user knowledge are used to guide the efficient choice of sample points. Typical approaches to the design of experiments involves determining the sample points all at once prior to any experimentation, or sequentially based on the results of previous sample points. This method combines information from multiple fidelity sources including actual physical experiment, computer simulation models of the product, first principals involved in design and designer’s qualitative intuitions about the design. Both quantitative and qualitative information from different sources are merged together to arrive at new sampling strategy. This is accomplished by introducing the concept of confidence, C, which is represented as a field that is a function of the decision variables, x, and the performance parameter, f. The advantages of the approach are demonstrated using different example cases."
1008,"In order to reduce the time and resources devoted to design-space exploration during simulation-based design and optimization, the use of surrogate models, or metamodels, has been proposed in the literature. Key to the success of metamodeling efforts are the experimental design techniques used to generate the combinations of input variables at which the computer experiments are conducted. Several adaptive sampling techniques have been proposed to tailor the experimental designs to the specific application at hand, using the already-acquired data to guide further exploration of the input space, instead of using a fixed sampling scheme defined a priori. Though mixed results have been reported, it has been argued that adaptive sampling techniques can be more efficient, yielding better surrogate models with less sampling points. In this paper, we address the problem of adaptive sampling for single and multi-response metamodels, with a focus on Multi-stage Multi-response Bayesian Surrogate Models (MMBSM). We compare distance-optimal latin hypercube sampling, an entropy-based criterion and the maximum cross-validation variance criterion, originally proposed for one-dimensional output spaces and implemented in this paper for multi-dimensional output spaces. Our results indicate that, both for single and multi-response surrogate models, the entropy-based adaptive sampling approach leads to models that are more robust to the initial experimental design and at least as accurate (or better) when compared with other sampling techniques using the same number of sampling points."
1009,"The high computational cost of population based optimization methods, such as multi-objective genetic algorithms, has been preventing applications of these methods to realistic engineering design problems. The main challenge is to devise methods that can significantly reduce the number of computationally intensive simulation (objective/constraint functions) calls. We present a new multi-objective design optimization approach in that kriging-based metamodeling is embedded within a multi-objective genetic algorithm. The approach is called Kriging assisted Multi-Objective Genetic Algorithm, or K-MOGA. The key difference between K-MOGA and a conventional MOGA is that in K-MOGA some of the design points or individuals are evaluated by kriging metamodels, which are computationally inexpensive, instead of the simulation. The decision as to whether the simulation or their kriging metamodels to be used for evaluating an individual is based on checking a simple condition. That is, it is determined whether by using the kriging metamodels for an individual the non-dominated set in the current generation is changed. If this set is changed, then the simulation is used for evaluating the individual; otherwise, the corresponding kriging metamodels are used. Seven numerical and engineering examples with different degrees of difficulty are used to illustrate applicability of the proposed K-MOGA. The results show that on the average, K-MOGA converges to the Pareto frontier with about 50% fewer number of simulation calls compared to a conventional MOGA."
1010,"Computation-intensive design problems are becoming increasingly common in manufacturing industries. The computation burden is often caused by expensive analysis and simulation processes in order to reach a comparable level of accuracy as physical testing data. To address such a challenge, approximation or metamodeling techniques are often used. Metamodeling techniques have been developed from many different disciplines including statistics, mathematics, computer science, and various engineering disciplines. These metamodels are initially developed as “surrogates” of the expensive simulation process in order to improve the overall computation efficiency. They are then found to be a valuable tool to support a wide scope of activities in modern engineering design, especially design optimization. This work reviews the state-of-the-art metamodel-based techniques from a practitioner’s perspective according to the role of metamodeling in supporting design optimization, including model approximation, design space exploration, problem formulation, and solving various types of optimization problems. Challenges and future development of metamodeling in support of engineering design is also analyzed and discussed."
1012,"This paper describes a new approach for reducing the number of the fitness and constraint function evaluations required by a genetic algorithm (GA) for optimization problems with mixed continuous and discrete design variables. The proposed modification improves the efficiency of the memory constructed in terms of the continuous variables. The work presents the algorithmic implementation of the proposed memory scheme and demonstrates the efficiency of the proposed multivariate approximation procedure for the weight optimization of a segmented open cross section composite beam subjected to axial tension load. Results are generated to demonstrate the advantages of the proposed improvements to a standard genetic algorithm."
1013,"This paper addresses the critical issue of fidelity in simulation-based design optimization using preference-based surrogate models. Specifically, it presents an integrated clustering-based updating procedure in a genetic algorithm setup to iteratively improve the efficacy of Kriging models. A potential drawback of using preference-based surrogate models in simulation based design is that the surrogates may misrepresent the true optima if the model building schemes fail to capture the critical points of interest with enough fidelity or clarity. This work addresses this vulnerability and presents an efficient clustering-technique integrated surrogate model updating procedure that can capture the buried, transient, yet inherent data pattern in the evolution progression of design candidates within a genetic algorithm setup, and screen out distinct optimal points for subsequent sequential model validation and updating. The results show that the successful finding of the true optimal design through cost-effective surrogate-based optimization depends not only on the selection of sampling schemes such as sample rate and distribution in the initial surrogate model build-up, but also on an efficient and reliable updating procedure that can prevent suboptimal decisions."
1014,"This paper deals with the preliminary design of large complex mechanical products. It gives a new way to lead tradeoffs between architectural constraints and mechanical performances. While topology optimization and structural optimization have been widely developed in the last two decades [1], they are still not adapted to take into account the problem of volume allocation in the preliminary design of large and complex mechanical systems as automotive vehicles and aircrafts. New approximate criteria to assess the compliance of architectural constraints and important mechanical performances like contributions to noise are proposed. These criteria are assembled within a sole performance metamodel that embeds most of decision issues usually discussed in the preliminary design stage and which allows to encompass the strict traditional volume allocation process. Through the use of Pareto techniques, we show that it is convenient and relevant to explore the design space by allowing great variation of the design morphology. The trade-offs are made possible since the stage of volume allocation and automotive architects can be helped by tools for leading quantitative negotiations in preliminary design."
1015,"Designers have for some time had powerful tools that allow optimization of geometric design using, for example, parametric design techniques. However, these optimization strategies utilize an incomplete model in the design process. An assumption often made throughout the design process is that the material properties are homogeneous. However, it is recognized that engineered materials may have significant non-homogeneity, which often manifests itself in an undesirable manner (e.g. in the heat affected zone). In an ideal world, the non-homogeneity could be used to improve a design. A designer would have full control over the heterogeneous design of a product, being enabled to optimize both the material properties and geometry concurrently. Recent developments in microstructure sensitive design (MSD) have now made the problem of optimizing over both geometry and material structure accessible. The inverse problem of designing material to achieve desired properties is being tackled. The latest techniques allow a designer to search the property closures of materials for the optimum theoretical microstructures. Furthermore, work has started in terms of linking the space of theoretical microstructures with manufacturing techniques that are currently available for manipulation of the microstructure of polycrystalline materials. This paper demonstrates how the latest available MSD tools may be applied to the design process. A curved rod is optimized to maintain certain critical physical properties, while meeting certain geometrical criteria. The result is a local reduction in radius of 20% without significant reduction in properties. While the manufacturability of this particular case study is not specifically addressed, the potential future directions for such design tools are discussed and some practicalities in terms of bridging the gap between theoretical microstructures and available materials are reviewed."
1016,"In this paper, a parametric structural shape and topology optimization method is presented. To solving structure optimization problems, the level-set method has become a powerful design tool and been widely used in many fields. Combined with the Radial Basis Functions (RBF), which is a popular tool in function approximation, the method of level-set can be represented in a parametric way with a set of advantages comparing with the conventional discrete means. Some numerical examples are presented to illustrate its advantages."
1017,"Finite Element Analysis (FEA) is an important step for the design of structures or components formed by heterogeneous objects such as multi-materials, Functionally Graded Materials (FGMs), etc. The main objective of the FEA-based design of heterogeneous objects is to simultaneously optimize both geometry and material distribution over the design domain (e.g., Homogenization Design Method). However, the accuracy of the FEA-based design wholly depends on the quality of the finite element models. Therefore, there exists an increasing need for generating finite element models adaptive to both geometric complexity and material distribution. This paper introduces a method for FEA-based design of heterogeneous objects. At the design stage, a heterogeneous solid model is first created by referring to the libraries of primary materials and composition functions that are already available in the field of material science. The heterogeneous solid model is then discretized into an object model onto which appropriate material properties are mapped. Discretization converts continuous material variations inside an object into stepwise variations. Next, the object model is adaptively meshed and converted into a finite element model. The meshing algorithm first creates nodes on the iso-material curves (or surfaces) of heterogeneous solid models. Triangular (or tetrahedral) meshes are then generated inside each iso-material region formed by iso-material curves (or surfaces). FEA using commercial software is finally performed to estimate stress levels. This FEA-based design cycle is repeated until a satisfactory solution is obtained. If the design objective is satisfactory, the object model is fed to the fabrication system where a process planning is performed to create instructions for LM machines. An example (FGM pressure vessel) is shown to illustrate the entire FEA-based design cycle."
1018,"Mechanical properties of materials in small-scale applications, such as thin coatings, are often different from those of bulk materials due to the difference in the manufacturing process. Indentation has been a convenient tool to study the mechanical properties in such applications. In this paper, a numerical technique is proposed that can identify the mechanical properties by minimizing the difference between the results from indentation experiments and those from finite element analysis. First, two response surfaces are constructed for loading and unloading curves from the indentation experiment of a gold film on the silicon substrate. Unessential coefficients of the response surface are then removed based on the test statistics. Different from the traditional methods of identification, the tip geometry of the indenter is included because its uncertainty significantly affects the results. In order to validate the accuracy and stability of the method, the sensitivity of the identified material properties with respect to each coefficient is analyzed."
1019,"Topology optimization methods have been shown to have extensive application in the design of microsystems. However, their utility in practical situations is restricted to predominantly planar configurations due to the limitations of most microfabrication techniques in realizing structures with arbitrary topologies in the direction perpendicular to the substrate. This study addresses the problem of synthesizing optimal topologies in the out-of-plane direction while obeying the constraints imposed by surface micromachining. A new formulation that achieves this by defining a design space that implicitly obeys the manufacturing constraints with a continuous design parameterization is presented in this paper. This is in contrast to including manufacturing cost in the objective function or constraints. The resulting solutions of the new formulation obtained with gradient-based optimization directly provide the photolithographic mask layouts. Two examples that illustrate the approach for the case of stiff structures are included."
1020,"Prismatic cellular or honeycomb materials exhibit favorable properties for multifunctional applications such as ultra-light load bearing combined with active cooling. Since these properties are strongly dependent on the underlying cellular structure, design methods are needed for tailoring cellular topologies with customized multifunctional properties that may be unattainable with standard cell designs. Topology optimization methods are available for synthesizing the form of a cellular structure—including the size, shape, and connectivity of cell walls and the number, shape, and arrangement of cell openings—rather than specifying these features "
1021,"We present an approach for the integrated design of materials, products, and design processes. The approach is based on the use of reusable interaction patterns to model design processes, and the consideration of design process decisions using the value of information metrics. The approach is presented using a multifunctional energetic structural materials (MESM) design example. The design objectives in the example include sufficient strength and energy release capabilities. The design is carried out by using simulation models at different scales that model different aspects of the system. Preliminary results from the application of the approach to the MESM design problem are discussed. In this paper, we show that the integrated design of materials and products can be carried out more efficiently by considering the design of design processes."
1022,"In this paper, we explore the benefits of materials design in a product design process. We also compare the methods of material selection and materials design by demonstrating two examples—the design of a cantilever beam for minimum weight and the design of a fan blade for minimum weight. The design of the cantilever beam is carried out using Ashby’s material selection method as well as a proposed method for materials design. The design of the fan blade and its material is completed using computational tools. Our goal in this paper is to demonstrate the benefits of materials design over material selection methods and to illustrate the flexibility inherent in materials design processes. We are more interested in revealing the possibilities of materials design, rather than the specific results from the example problems. The investigation of materials design presented in this paper moves us one step closer towards the realization of a systematic, inductive method for the concurrent design of products and materials."
1023,"Knowledge Management systems have been applied to several areas in the manufacturing industry. These applications have been predominately text based, however, much of manufacturing’s information is related to geometric features. In this paper we describe a representation and content-based search method such that given a geometric specification, the knowledge of previous designs having similar feature requirements is returned. The technique uses curvature based shape index segmentation to construct an attributed graph-like descriptor for 2.5D free-form surface features, such as those found on stamped components. Efficient edit distance methods are applied for search. Experiments on a database of freeform features indicate feasible runtimes with favourable results."
1024,"Parametric modeling systems are fundamentally changing the design process practiced in the industry today. Practically all commercial CAD systems combine established solid modeling techniques with constraint solving and heuristic algorithms to create, edit and manipulate solid models, while enforcing the requirement that every such solid model must maintain the validity of the prescribed geometric constraints. However, a number of fundamental (open) problems limit the functionality and performance of these parametric modeling systems. For example, the allowable parametric changes are "
1025,"Due to the complexity, reuse of freeform features represented by B-spline is still an open issue. Based on Poisson equation, a novel approach to the reuse of freeform features represented by uniform rational B-spline (URBS) is proposed in this paper. In order to effectively support reuse, a new representation of freeform features is put forwarded, which is based on a new property about the odd URBS and consists of the principal control points, geometry context and basic surface of the feature. Based on the new representation, reuse of freeform features is achieved by updating the principal control points on the pasted area of the target surface using Poisson equation. The approach is implemented and some examples are given."
1026,"We propose a novel approach to shape optimization that combines and retains the advantages of the earlier optimization techniques. The shapes in the design space are represented implicitly as level sets of a higher-dimensional function that is constructed using B-splines (to allow free-form deformations), and parameterized primitives combined with R-functions (to support desired parametric changes). Our approach to shape design and optimization offers great flexibility because it provides explicit parametric control of geometry and topology within a large space of freeform shapes. The resulting method is also general in that it subsumes most other types of shape optimization as special cases. We describe an implementation of the proposed technique with attractive numerical properties. The effectiveness of the method is demonstrated by several numerical examples."
1027,"Topology synthesis of low-mobility parallel mechanisms is an important direction of mechanism research. At present, various systematic methods for topology synthesis have been proposed, and large numbers of new mechanisms that satisfy the motion requirements have been synthesized through step-by-step deducing. However, some fundamental problems are ignored. In this paper, some significant synthesis methods are compared from the aspects of the description of output character, limb structure synthesis and geometrical relationship between limbs. The commonly existent problems are analyzed, including the strict description of the output character of the moving platform and the instantaneous mechanisms in the process of topology synthesis. The limitation of existent methods is also indicated. Moreover, a classifying method for low-mobility parallel mechanisms from the viewpoint of topology synthesis is proposed. This classification has guiding effect for synthesis, analysis and application of parallel mechanisms."
1028,"The kinematic and dynamic analysis of an spatial multi-loop mechanism especially parallel mechanism is significant but always complex. Based on the topological structure of mechanisms, this paper proposes the concept of coupling degree of mechanism systematically, and applies it to the criterion of basic kinematic chains(BKCs) and other problems. The relation between topology, kinematics and dynamics of parallel mechanisms is established, and then it is achieved to quantitatively describe the analysis complexity of a parallel mechanism and to obtain its simplest solving path, according to its topological structure. The preliminary method for unified modeling of the topology, kinematics and dynamics of parallel mechanisms is proposed, using BKC as the basic analysis unit. Some suggestions for optimization and selective preference of parallel mechanisms are also presented."
1029,"In this paper, we investigate the current state-of-the-art in packing algorithms. The focus of this survey is on the different types of encoding schemes and associated placement techniques used to represent the layout of a set of objects. The encoding schemes are investigated with respect to their suitability to different types of packing problems, specific scenarios where a given representation may outperform others and their limitations. The different types of placement algorithms that can be used with a given encoding are described. Some common desirable characteristics that an encoding scheme should follow are also discussed. Finally a qualitative comparison of the various encoding schemes is presented to help in selecting a specific representation based on a set of criteria."
1030,"This work focuses on incorporating component shape design into a vehicle configuration design or layout process. A concurrent design process consisting of performing layout design and simultaneous shape morphing of some select components is adopted to replace the traditional sequential design approach. The objective is to improve design efficiency and reduce design cost. Two important issues in the packing optimization with shape morphing problem are identified and studied: the morphing and the optimization. A parameterization-based morphing method and a mesh-based morphing method are implemented, and their advantages and disadvantages are discussed. To efficiently solve this complex problem, it is proposed to decompose it into a bi-level formulation: system level and component level. At the system level, the given functional objectives of the layout design problem are optimized with respect to component positions and orientations. At the component level, the shape of select components is morphed to minimize the overlap with other objects and the enclosure. By iterating between these two levels, the original problem is solved. This bi-level approach is intended to overcome the complexity of performing the placement simultaneously with the shape morphing."
1031,"In this work, we explore simultaneous design and material selection by posing it as an optimization problem. The underlying principles for our approach are Ashby’s material selection procedure and structural optimization. For the simplicity and ease of initial implementation of the general procedure, truss structures under static load are considered in this work in view of maximum stiffness, minimum weight/cost and safety against failure. Along the lines of Ashby’s material indices, a new design index is derived for trusses. This helps in choosing the most suitable material for any design of a truss. Using this, both the design space and material database are searched simultaneously using optimization algorithms. The important feature of our approach is that the formulated optimization problem is continuous even though the material selection is an inherently discrete problem."
1032,"Compliant mechanisms generated by traditional topology optimization methods have linear output response, and it is difficult for traditional methods to implement mechanisms having non-linear output responses, such as nonlinear deformation or path. To design a compliant mechanism having a specified nonlinear output path, a two-stage design method based on topology and shape optimization is constructed here. In the first stage, topology optimization generates an initial and conceptual compliant mechanism based on ordinary design conditions, with “additional” constraints that are used to control the output path at the second stage. In the second stage, an initial model for the shape optimization is created, based on the result of the topology optimization, and the additional constraints are replaced by spring elements. The shape optimization is then executed, to generate a detailed shape of the compliant mechanism having the desired output path. In this stage, parameters that represent the outer shape of the compliant mechanism and the properties of spring elements are used as design variables in the shape optimization. In addition to configuration of the specified output path, executing the shape optimization after the topology optimization also makes it possible to consider the stress concentration and large displacement effects. This is an advantage offered by the proposed method, since it is difficult for traditional methods to consider these aspects, due to inherent limitations of topology optimization."
1033,"This paper presents an algorithmic, physics-based, design synthesis method aimed at facilitating synthesis through automated generation of a range of feasible and optimally directed design alternatives. The method assists designers in the exploration of performance limits and trade-offs for synthesis tasks as well as reducing design time through rapid, computational generation. The method introduced combines a multicriteria generate-and-test search algorithm, called Burst, with a Connected Node System (CNS) design representation and provides automatic links to multiphysics simulation for quantitative evaluation of design performance throughout the synthesis process. The CNS-Burst method is applied to two benchmark synthesis tasks in the domain of MEMS to validate the method. The solutions generated meet the modeled design requirements and the variety of designs generated offers designers the possibility of selecting devices according to their preferences among performance trade-offs. The potential for extension to larger, more complex MEMS design synthesis and optimization tasks is discussed."
1034,"Appraising vehicle package design concepts using seating bucks — physical prototypes representing vehicle package, is an integral part of the vehicle package design process. Building such bucks is costly and may impose substantial burden on the vehicle design cycle time. Further, static seating bucks lack the flexibility to accommodate design iterations during the gradual progression of a vehicle program. A “Computer controlled seating buck”, as described in this paper, is a quick and inexpensive alternative to the traditional seating bucks with the desired degree of fidelity. It is particularly useful to perform package and ergonomic studies in the early stages of a vehicle program, long before the data is available to build a traditional seating buck. Such a seating buck has been developed to accommodate Ford vehicle package design needs. This paper presents the functional requirements, the high level conceptual design of how these requirements are realized, and the methods to verify, improve and sustain the dimensional accuracy and capability of the new computer controlled seating buck."
1035,"This paper presents an efficient method to generate packaging space that can be used to create molded foam. This method first reduces the number of facets presenting the product and then detects heights of supporting lines from an evenly spaced mesh grid on the bottom space of a pre-defined bounding box. The height of each supporting line is further relaxed and smoothed to form the final supporting surface. Based on the supporting surface, an STL format is created so it can be fabricated using RP machines. Three examples are presented to demonstrate this method."
1036,"In this paper, topology optimization problems with two types of body force are considered: gravitational force and centrifugal force. For structural design under both external and gravitational forces, a total mean compliance formulation is used to produce the stiffest structure. For rotational structural design with high angular velocity, one additional design criteria, kinetic energy, is included in the formulation. Sensitivity analyses of the total mean compliance and kinetic energy are derived. Finally, design examples are presented and compared to show the effects of body forces on the optimized results."
1037,"The analytical target cascading (ATC) methodology for optimizing hierarchical systems has demonstrated convergence properties for continuous, convex formulations. However, many practical problems involve both continuous and discrete design variables, resulting in mixed integer nonlinear programming (MINLP) formulations. While current ATC methods have been used to solve such MINLP formulations in practice, convergence properties have yet to be formally addressed, and optimality is uncertain. This paper describes properties of ATC for working with MINLP formulations and poses a solution method applying branch and bound as an outer loop to the ATC hierarchy in order to generate optimal solutions. The approach is practical for large hierarchically decomposed problems with relatively few discrete variables."
1038,"The Mahalanobis Taguchi System (MTS) is a diagnosis and forecasting method for multivariate data. Mahalanobis Distance (MD) is a measure based on correlations between the variables and different patterns that can be identified and analyzed with respect to a base or reference group. The MTS is of interest because of its reported accuracy in forecasting from small, correlated data sets. This is the type of data that is encountered with consumer vehicle ratings. MTS enables a reduction in dimensionality and the ability to develop a scale based on MD values. MTS identifies a set of useful variables from the complete data set with equivalent correlation and considerably less time and data. This paper presents the application of the MTS, its applicability in identifying a reduced set of useful variables in multidimensional systems."
1039,"The two-phase method for model-based decomposition (Chen et al. 2005a) has two major functional components: dependency analysis and partitioning analysis. The functions of these two components are enhanced and generalized in this paper in order to improve the method’s capability. On the one hand, the non-binary dependency analysis is developed such that the two-phase method can handle both binary and non-binary dependency information of the model. The essence of this development is to properly select a resemblance coefficient for the quantification of couplings among the model’s elements. On the other hand, as the past version of partitioning analysis takes the enumerative approach to search decomposition solutions, the heuristic partitioning analysis is developed as an alterative to search a reasonably good solution in a shorter time. The working principle of the heuristic approach is to analyze the coupling structure of the model such that the "
1040,"In order to obtain superior design solutions, the largest possible number of design alternatives, often expressed as discrete design variables, should first of all be considered, and the best design solution should then be selected from this wide set of alternative designs. Also, product designs should be initiated from the earliest possible stages, such as the conceptual and fundamental design stages, when discrete rather than continuous design variables have primacy. Although the use of discrete design variables is fundamentally important, this has implications in terms of computational demands and the accuracy of the optimized solution. This paper proposes an optimization method for product designs incorporating discrete design variables, in which hierarchical product optimization methodologies are constructed based on decomposition of characteristics and/or extraction of simpler characteristics. The optimizations are started at the lowest levels of the hierarchical optimization structure, and proceed to the higher levels. The discrete design variables are efficiently selected and optimized as smaller sub-optimization problems at the lowest hierarchical levels, while the optimum solutions for the entire problem are obtained by conventional mathematical programming methods. Practical optimization procedures for machine product optimization problems having several types of discrete design variables are constructed, and some applied examples demonstrate their effectiveness."
1041,"This paper discusses the detailed design and development of a web based parallel multi-disciplinary optimization (PMDO) framework in the distributed computing environment. This system consists of the "
1042,"This paper presents an optimization ontology and its implementation into a prototype computational knowledge base tool dubbed ONTOP  (ONT ology for OP timization). Salient features of ONTOP include a knowledge base which incorporates both standardized optimization terminology, formal method definitions, and often unrecorded optimization details, such as any idealizations and assumptions that may be made when creating an optimization model, as well as the model developer’s rationale and justification behind these idealizations and assumptions. ONTOP  was developed using Protégé, a Java-based, free open-source ontology development environment created by Stanford University. Two engineering design optimization case studies are presented. The first case study consists of the optimization of a structural beam element and demonstrates ONTOP ’s ability to address the variations in an optimal solution that may arise when different techniques and approaches are used. A second case study, a more complex design problem which deals with the optimization of an impeller of a pediatric left ventricular heart assist device, demonstrates the wealth of knowledge ONTOP is able to capture. Together, these test beds help illustrate the potential value of an ontology in representing application-specific knowledge while facilitating both the sharing and exchanging of this knowledge in engineering design optimization."
1043,"Decomposition-based product design optimization under system of systems paradigm is linked with resource (i.e., product) allocation. A two-stage, system of systems approach to linking resource allocation (e.g., vehicle routing problem (VRP)) and system design optimization (e.g., vehicle design problem (VDP)) is presented. The problem inherently contains discrete variables from VRP, thus a practical formulation is presented to overcome convergence difficulty associated with shared discrete variables in a decomposed setting. Two examples, composed of four and eight air routes respectively with the introduction of a new aircraft to the existing fleet, are presented to demonstrate the effectiveness of the proposed approach. A new type of aircraft is designed and allocated to the currently existing VRP to meet the demand, while the direct operating cost of an airline is minimized."
1044,"Design optimization is performed by presenting a systematic method to obtain the optimal operating conditions of a Proton Exchange Membrane (PEM) fuel cell system targeted towards a vehicular application. The fuel cell stack model is a modified version of the semi-empirical model introduced by researchers at the Royal Military College of Canada and one that is widely used by industry. Empirical data obtained from tests of PEM fuel cell stacks are used to determine the empirical parameters of the fuel cell performance model. Based on this stack model, a fuel cell system model is built in MATLAB. Included in the system model are heat transfer and gas flow considerations and the associated Balance of Plant (BOP) components. The modified ADVISOR vehicle simulation tool is used to integrate the New York City Cycle (NYCC) drive cycle and vehicle model to determine the power requirements and hence the load cycle of the fuel cell system for a low-speed fuel cell hybrid electric vehicle (LSFCHEV). The optimization of the powerplant of this vehicle type is unique. The vehicle model has been developed in the work to describe the characteristics and performance of an electric scooter, a simple low-speed vehicle (LSV). The net output power and system exergetic efficiency of the system are maximized for various system operating conditions using the weighted objective function based on the load cycle requirement. The method is based on the coupling of the fuel cell system model with three optimization algorithms (a) sequential quadratic programming (SQP); (b) simulated annealing (SA); and (c) genetic algorithm (GA). The results of the optimization provide useful information that will be used in future study on control algorithms for LSFCHEVs. This study facilitates research on more complex fuel cell system modeling and optimization, and provides a basis for experimentation to verify the fuel cell system model."
1045,"NASA’s future space exploration systems will include a highly complex Integrated Systems Health Management (ISHM) capability, which can detect, predict, isolate and respond to system and component failures in order to improve safety and maintainability. An ISHM system, as a whole, consists of several subsystems that monitor different components of a space mission. Due to the complex and multidisciplinary nature of designing ISHM, there seems to be a lack of formal methodologies to design an optimal (or near-optimal) ISHM for a given system of systems. In this research, we propose a new methodology to design and optimize ISHM as a distributed system with multiple interacting disciplines as well as multiple conflicting design objectives (i.e. Figures Of Merit or FOMs). This specialized multidisciplinary design approach can be used to optimize the effectiveness of ISHM systems for future NASA missions. We assume a hierarchical design protocol, where each subsystem communicates with other subsystems only in a top-down tree structure. At the top level, the overall performance of the mission consists of system-level variables, parameters, objectives, and constraints that are shared throughout the system and by all subsystems. Each subsystem will then comprise of these shared values in addition to those values that are specific to subsystems. As a specific case study, we take the example of designing an ISHM capability for X-34 reusable launch vehicle in two levels. The proposed approach, referred to as ISHM Multidisciplinary and Multiobjective System Analysis & Optimization (or ISHM MMSA&O), has a hierarchical structure to pass up or down shared values between the two levels with system-level and subsystem-level optimization routines."
1046,"Customization and market uncertainty require increased functional and physical bandwidth in product platforms. This paper presents a platform design process in response to such future uncertainty. The process consists of seven iterative steps and is applied to an automotive body-in-white (BIW) where 10 out of 21 components are identified as potential candidates for embedding flexibility. The method shows how to systematically pinpoint and value flexible elements in platforms. This allows increased product family profit despite uncertain variant demand and specification changes. We show how embedding flexibility suppresses change propagation and lowers switch costs, despite an increase of 34% in initial investment for equipment and tooling. Monte Carlo simulation results for 12 future scenarios reveal the value of embedding flexibility."
1048,"Many companies constantly struggle to find cost-effective solutions to satisfy the diverse demands of their customers. In this paper, we report on two recent industry-focused conferences that emphasized platform design, development, and deployment as a means to increase variety, shorten lead-times, and reduce development and production costs. The first conference, "
1049,"As companies are pressured to decrease product development costs concurrently with increasing product variety, the need to develop products based upon common components and platforms is growing. Determining why a platform worked, or alternatively why it did not, is an important step in the successful implementation of product families and product platforms in any industry. Unfortunately, published literature on platform identification and product family analysis using product dissection and reverse engineering methods is surprisingly sparse. This paper introduces two platform identification methodologies that use different combinations of tools that can be readily applied based on information obtained directly from product dissection. The first methodology uses only the Bills-of-Materials and Design Structure Matrices while the second utilizes function diagrams, Function-Component Matrices, Product-Vector Matrices, and Design Structure Matrices to perform a more in-depth analysis of the set of products. Both methodologies are used to identify the platform elements in a set of five single-use cameras available in the market. The proposed methodologies identify the film advance and shutter actuation platform elements of the cameras, which include seven distinct components. The results are discussed in detail along with limitations of these two methodologies."
1050,"The competitiveness in today’s market forces many companies to rethink the way they design (and redesign) products. Instead of developing one product at a time, many manufacturing companies are developing families of products to provide enough variety for the marketplace while keeping costs relatively low. Although the benefits of commonality are widely known, many companies are still not taking full advantage of it when developing new products or redesigning existing ones. One reason is the lack of appropriate methods and useful metrics to assess a product family based on commonality and diversity. Although many component-based commonality metrics have been proposed in the literature, they do not (1) help resolve the tradeoff between commonality and diversity in a product family and (2) capture enough information to be completely useful during product family design and redesign. In this paper, we propose the Comprehensive Metric for Commonality (CMC) to evaluate the design of a product family on a 0–1 scale based on the components in each product, their size, geometry, material, manufacturing process, assembly, costs, and the allowed diversity in a family. To demonstrate the usefulness of this metric for product family benchmarking and redesign, the CMC is compared to six other component-based commonality indices. A CMC-based method is also proposed and applied to a family of staplers to (1) assess the level of commonality in the product family and (2) give recommendations for redesigning the product family."
1052,"This paper presents the status of an ongoing project to develop a comprehensive suite of test problems suitable for comparing methods for scale-based product platform design. Despite a growing body of work in the area, there is no adequate set of testbed example problems for product platform design and benchmarking. A lack of consensus as to exactly what scale-based platform design entails has also hampered comparison of methods. In order to make a comprehensive test suite, we first need to define what different capabilities of platform design methods should be tested. To further this end, a classification scheme for example problems for scale-based platform design is presented. This simple taxonomy classifies example problems on the basis of two criteria: selection of platform architecture and incorporation of market demand. A brief review of examples from the literature shows that the existing examples are useful to test only a few of the capabilities of platform design methods. A new extension of an existing example, the design of a family of universal electric motors, is presented to test capabilities not covered by the existing set. This extended example is the first in our suite of examples."
1053,"Many engineering systems are required to operate under changing operating conditions. A special class of systems called adaptive systems have been proposed in the literature to achieve high performance under changing environments. Adaptive systems acquire this powerful feature by allowing their design configuration to change with operating conditions. In the optimization of the adaptive systems, designers are often required to select (i) adaptive and (ii) non-adaptive (or fixed) design variables of the design configuration. Generally, the selection of these variables, and the optimization of adaptive systems are performed sequentially, thus leaving a likelihood of a sub-optimal design. In this paper, we propose the Selection-Integrated Optimization (SIO) methodology that integrates the two key processes: (1) the selection of the adaptive and fixed design variables, and (2) the optimization of the adaptive system, thereby leading to an optimum design. A major challenge to integrating these two key processes is the selection of the number of fixed and adaptive design variables, which is discrete in nature. We propose the Variable-Segregating Mapping-Function (VSMF) that overcomes this roadblock by progressively approximating the discreteness in the design variable selection process. This simple yet effective approach allows the SIO methodology to integrate the selection and optimization processes, and help avoid one significant source of sub-optimality from typical optimization formulations. The SIO methodology finds its applications in a variety of other engineering fields as well, such as product family optimization. However, in this paper, we limit the scope of our discussion to adaptive system optimization. The effectiveness of the SIO methodology is demonstrated by optimally designing a new air-conditioning system called Active Building Envelope (ABE) System."
1054,"This paper describes a method for improving commonality in a highly customized low volume product line whose members were originally developed one-at-a-time to meet specific customer requirements. Rather than focusing on redesign of the entire product line, which can often be cost prohibitive, the method is part of a strategy to redesign a limited set of component parts that have the highest potential for cost savings. The method involves a four-step process: (1) determine an optimal component solution for each member artifact of an existing market segment grid, (2) test the feasibility of using each optimal component as a platform for the other artifacts, (3) formulate an optimization problem around the feasibility statistics whose solution is a product platform portfolio, and (4) solve the optimization problem for the minimum number of platforms that can adequately span the existing market segment grid. The proposed method is applied to an example involving the redesign of actuator mounting yokes for an existing set of valves that are used in nuclear power plants. The method shows promise for determining a product platform mix that maximizes commonality yet meets performance requirements."
1055,"Contemporary product designers seek to create products that are not only robust for the current marketplace but also flexible for future changes, adaptations, and evolutions. This type of product flexibility is distinctive from mass customization, product architecture of singular products, and product families. The intent is to design products that intrinsically enable future changes even though such changes may not be known or planned in the current product offering. To accommodate product flexibility of this type, research advancements are needed in terms of fundamental design principles and evaluation methods for predicting and improving the flexibility of a product. This paper presents advancements in both areas. We first present the systematic enhancement of a flexibility assessment tool referred to as CMEA, Change Modes and Effects Analysis. CMEA provides the basic ability to assess the flexibility of a product, with analogous features to the well-known Failure Modes and Effects Analysis. Our enhancements extend the method to provide for intuitive and more repeatable measures of flexibility. We then use the enhanced CMEA to investigate a variety of consumer products with the goal of inductively deriving product flexibility principles. Concrete applications are shown for these principles from the domain of power yard tools, such as hedge trimmers, weed trimmers, and leaf blowers. Also, the applications are used to demonstrate the value of the CMEA enhancements."
1056,"Setting design specifications (targets) is a critical task in the early stages of a design process. Flexible targets can accommodate uncertainty and changes in design by postponing design commitments and preserving design freedom. In this work, a new and efficient method for obtaining a ranged set of design specifications that meets the overall design goal while incorporating heterogeneous design capability information is developed. Our proposed method involves two important aspects. First, a quantization algorithm based on rough set theory is used to decompose a design attribute space into subregions based on how well they meet the overall design goal. Second, a new design flexibility measure is used as a metric to select the most desired “target region” based on both the size of the region and the design capability information retrieved from potential design concepts. Our approach captures heterogeneous design capability information in the design attribute space and enhances the ability to adapt to evolving design knowledge as well as unexpected changes. The proposed method is much more efficient than conventional optimization algorithms for solving such problems. The proposed method is demonstrated by a numerical example and the design of a domestic blender."
1057,"At a time when product differentiation is a major indicator of success in the global market, each company is looking to offer competitive and highly differentiated products. This differentiation issue is restricted by the design of platform-based products that share modules and/or components. It is not easy to differentiate products in a market that is often overwhelmed by numerous options. A platform-based approach can be risky because competition in the global market can become an internal competition among similar products within the family if there is not enough differentiation in the family. Thus, the goal for the product platform is to share elements for common functions and to differentiate each product in the family by satisfying different targeted needs. To assess commonality in the family, numerous indices have been proposed in the literature. Nevertheless, existing indices focus on commonality and reflect an increase in value when commonality increases but do not positively reflect an increase in the value as a result of diversity; hence, the "
1058,"To help guarantee profit and stability in today’s global market, companies must focus on the differentiation of their products. Successfully differentiated products will attract customers, generate revenue and benefit the brand image, whereas a banal product can lose money and leave a bad impression in the market. Many large companies have recently lost significant market share in part due to poor product differentiation. This paper introduces four indices to assess this differentiation at two levels—family and market—based on product function and function attributes. At the family level, the Product Differentiation Index (PDI) assesses the differentiation between a product and other products in the rest of the family and also the differentiation within the family. At the market level, the Family Differentiation Index (FDI), Family Coverage Index (FCI), and Family Un-coverage Index (FUI) assess the differentiation, the coverage, and the un-coverage of a family with another, and/or with the rest of the market, respectively. These indices help designers and marketers evaluate the positioning of their products and support product family planning. A case study involving two competitive single-use camera families is presented."
1059,"Modular design issues are receiving increased attention by companies interested in reducing costs from carrying large numbers of components while at the same time increasing product quality and providing customers with greater product variety. Existing research has mainly focused on optimizing product platforms and product offerings, with little attention being given to the interfaces between modules. This research presents an investigation into how module interfaces are best represented in a CAD/PDM environment. The representation decisions are identified and advantages and limitations for each option are presented. Representation decisions revolve around issues such as the use of higher abstraction models, the use of ports, and referencing interface components in interface definitions. We conclude that higher abstraction models are necessary, ports should be represented explicitly, and interface hardware should not be included directly with interfaces. The research considers a large number of components from representative products offered by a home appliance manufacturer."
1060,"Many companies are using product platform concepts to gain economies of scale and to identify new market opportunities. Though the area of product platforming continues to be actively investigated by both industry and academia, there is no comprehensive classification scheme that can provide a clear picture of the existing problems and possible future research directions. Hence, in the present paper, we introduce a broad taxonomy that classifies product platform problems based on the product development stages. This can serve as a basis to: (1) Extract and categorize problems from research literature; (2) Identify potential extensions and/or new problems that have not been addressed in the literature; and (3) Identify existing problem sets and/or develop new problem sets for benchmarking purposes. We introduce a Conditions and Assumptions Code (CAC) scheme and use it in the identification of benchmark problems as well as in analyzing two classes of evaluation methods adopted for the platform problems: metrics-based and optimization-based. Thus, we have not only categorized existing problems but also identified possible future research problems in each of the categories. This categorization serves as a navigation tool to understand the progress made in this field so far and to identify new research directions."
1061,NULL
1062,"Different from the traditional machining processes, the quality of parts produced by the metal deposition process is much more dependent upon the choice of deposition paths. Due to the nature of the metal deposition processes, various tool path patterns will result in different shapes in the metal deposition process with about the same input geometry. This paper presents the research conducted on the effect of various scanning patterns and strategies for the deposition results. Triangle and rectangle patterns are selected as basic 2-D “cells” to plan the scanning path. Several criteria, like minimum angle, minimum length of edge, etc. are defined to categorize the different “cell” shapes. Based on deposition results, the suitable patterns are determined for each type. The previously defined patterns are applied for each cell in order to achieve the optimal quality. The experiment has demonstrated that the pattern and strategy selection has improved the deposition quality significantly."
1063,"A new curve matching method is proposed to generate non-self-intersecting and non-twisted ruled layers for application in diverse fields such as layered manufacturing, offsetting and multi-axis CNC machining. The method establishes point-to-point correspondence represented by a set of ruling lines between two directrices of the ruled surface. The directrices are given as non-self-intersecting, closed, at least C1 continuous, planar, B-spline curves. To match the points on the directrices, a heuristic optimization method developed with the objective is to maximize the sum of the inner products of the unit normals at the end points of the ruling lines and minimize the sum of the lengths of connecting ruling lines. The generated ruling lines can be used as cutter location data for multi-axis NC machining of ruled surfaces. Moreover, by subdividing the ruling lines into equal number of segments, one can construct a series of intermediate piecewise linear curves that represent the metamorphosis between the directrices. Implementation and examples are also presented."
1064,"Contour-parallel path, also called offset path, is a commonly used deposition strategy exploited by many deposition processes. This offset path has been studied vastly due to the potential of its applications. The majority of contour-parallel path research is found in robotics, CNC machining, and metal deposition. Even though curve offsetting has been extensively studied, there is virtually no algorithm that can produce a complete connected deposition path and can fill arbitrary shape cross sections entirely without gaps. This gap problem is similar to the uncut-region problem in CNC pocket machining. There are only few investigations on this uncut-region or gap problem even though the problem has long been recognized. A new strategy to divide regions and to plan the spiral-like deposition paths without gap based on contour-parallel paths is discussed in this paper. To prove the correctness and the usefulness of the proposed method, simulations and experiments are also discussed."
1065,"We present a GPU-accelerated algorithm for computing a fast approximation of the volume of supports required for layered manufacturing in a given build direction, one criterion often used to choose a direction that requires less time and material. In a sequence of rendering passes that project the part in the given build direction, we use depth peeling to identify faces bounding supports. We exploit programmable graphics hardware to compute the total height of all supports at each projected pixel location, scale the values by pixel area, and finally sum over all pixels to find the total volume of supports. For sample parts tested, our algorithm achieves over 99% accuracy and running times ranging from .2 seconds, for a part with 1,252 facets and depth complexity 2, to 1.86 seconds, for a part with 419,798 facets and depth complexity 9."
1066,"Shot peening process is largely used for surface treatment of metallic components with the aim of increasing surface toughness and extending fatigue life. The fatigue strength of the component can be improved by inducing compressive residual stress in the surface and subsurface layers by the shot peening process. Numerical simulation of the shot peening process is an important tool that is used to aid in understanding the effects of the process parameters on intended goal of attaining the optimum residual stress profile and maximum process gain. In this paper an elasto-plastic finite element model is used for the shot peening process. The process parameters that are varied in this analysis are: the shot diameter, shot speed, number of shots at a given time (coverage) and target material. The analysis is carried out for two different materials, namely, steel and aluminum. An Explicit finite element code (ABAQUS) is used to perform this task. These parameters have different effects on the resulting residual profile and the results of the study showed that by adjusting these parameters, the most effective residual stress profile could be obtained."
1067,"Reliability-based design optimization is much more computationally expensive than deterministic design optimization. To alleviate the computational demand, the First Order Reliability Method (FORM) is usually used in reliability-based design. Since FORM requires a nonlinear transformation from non-normal random variables to normal random variables, the nonlinearity of a constraint function may increase. As a result, the transformation may lead to a large error in reliability calculation. In order to improve accuracy, a new reliability-based design method with Saddlepoint Approximation is proposed in this work. The strategy of sequential optimization and reliability assessment is employed where the reliability analysis is decoupled from deterministic optimization. The accurate First Order Saddlepoint method is used for reliability analysis in the original random space without any transformation, and the chance of increasing nonlinearity of a constraint function is therefore eliminated. The overall reliability-based design is conducted in a sequence of cycles of deterministic optimization and reliability analysis. In each cycle, the percentile value of the constraint function corresponding to the required reliability is calculated with the Saddlepoint Approximation at the optimal point of the deterministic optimization. Then the reliability analysis results are used to formulate a new deterministic optimization model for the next cycle. The solution process converges within a few cycles. The demonstrative examples show that the proposed method is more accurate and efficient than the reliability-based design with FORM."
1068,"Both aleatory and epistemic uncertainties exist in engineering applications. Aleatory uncertainty (objective or stochastic uncertainty) describes the inherent variation associated with a physical system or environment. Epistemic uncertainty, on the other hand, is derived from some level of ignorance or incomplete information about a physical system or environment. Aleatory uncertainty associated with parameters is usually modeled by probability theory and has been widely researched and applied by industry, academia, and government. The study of epistemic uncertainty in engineering has recently started. The feasibility of the unified uncertainty analysis that deals with both types of uncertainties is investigated in this paper. The input parameters with aleatory uncertainty are modeled with probability distributions by probability theory, and the input parameters with epistemic uncertainty are modeled with basic probability assignment by evidence theory. The effect of the mixture of both aleatory and epistemic uncertainties on the model output is modeled with belief and plausibility measures (or the lower and upper probability bounds). It is shown that the calculation of belief measure or plausibility measure can be converted to the calculation of the minimum or maximum probability of failure over each of the mutually exclusive subsets of the input parameters with epistemic uncertainty. A First Order Reliability Method (FORM) based algorithm is proposed to conduct the unified uncertainty analysis. Two examples are given for the demonstration. Future research directions are derived from the discussions in this paper."
1069,"Significant recent research has focused on the marriage of consumer preferences and engineering design in order to improve profitability. The extant literature has neglected the effects of channel markets which are increasingly prevalent. At the crux of the issue is the fact that channel dominating retailers, like Wal-Mart, have the ability to unilaterally control manufacturer production decisions as gatekeepers to the consumer or market. In this paper, we propose a new methodology that accounts for this power asymmetry. A chance constrained framework is used to model retailer acceptance of possible engineering designs and accounts for the important effect on the profitability of the retailer’s assortment through a latent class estimation of demand from conjoint surveys. Our approach allows the manufacturer to optimize a product design for profitability while reliably ensuring that the product will make it to market by making the retailer more profitable with the addition of the new product. As a demonstrative example, we apply the proposed approach for product design selection in the case of an angle grinder. For this example, we analyze the market and are able to improve expected manufacturer profitability while simultaneously presenting the decision maker with tradeoffs between slotting allowances, market share, and risk of retailer acceptance."
1070,"There is growing acceptance in the design community that two types of uncertainty exist: inherent variability and uncertainty that results from a lack of knowledge, which variously is referred to as imprecision, incertitude, irreducible uncertainty, and epistemic uncertainty. There is much less agreement on the appropriate means for representing and computing with these types of uncertainty. Probability bounds analysis (PBA) is a method that represents uncertainty using upper and lower cumulative probability distributions. These structures, called probability boxes or just p-boxes, capture both variability and imprecision. PBA includes algorithms for efficiently computing with these structures under certain conditions. This paper explores the advantages and limitations of PBA in comparison to traditional decision analysis with sensitivity analysis in the context of environmentally benign design and manufacture. The example of the selection of an oil filter involves multiple objectives and multiple uncertain parameters. These parameters are known with varying levels of uncertainty, and different assumptions about the dependencies between variables are made. As such, the example problem provides a rich context for exploring the applicability of PBA and sensitivity analysis to making engineering decisions under uncertainty. The results reveal specific advantages and limitations of both methods. The appropriate choice of an analysis depends on the exact decision scenario."
1071,"Deterministic optimal designs that are obtained without taking into account uncertainty/variation are usually unreliable. Although reliability-based design optimization accounts for variation, it assumes that statistical information is available in the form of fully defined probabilistic distributions. This is not true for a variety of engineering problems where uncertainty is usually given in terms of interval ranges. In this case, interval analysis or possibility theory can be used instead of probability theory. This paper shows how possibility theory can be used in design and presents a computationally efficient sequential optimization algorithm. After, the fundamentals of possibility theory and fuzzy measures are described, a double-loop, possibility-based design optimization algorithm is presented where all design constraints are expressed possibilistically. The algorithm handles problems with only uncertain or a combination of random and uncertain design variables and parameters. In order to reduce the high computational cost, a sequential algorithm for possibility-based design optimization is presented. It consists of a sequence of cycles composed of a deterministic design optimization followed by a set of worst-case reliability evaluation loops. Two examples demonstrate the accuracy and efficiency of the proposed sequential algorithm."
1072,"Since engineering design requires decision making under uncertainty, the degree to which good decisions can be made depends upon the degree to which the decision maker has expressive and accurate representations of his or her uncertain beliefs. Whereas traditional decision analysis uses precise probability distributions to represent uncertain beliefs, recent research has examined the effects of relaxing this assumption of precision. A specific example of this is the theory of imprecise probability. Imprecise probabilities are more expressive than precise probabilities, but they are also more computationally expensive to propagate through mathematical models. The probability box (p-box) is an alternative representation that is both more expressive than precise probabilities, and less computationally expensive than general imprecise probabilities. In this paper, we introduce a method for propagating p-boxes through black box models. Based on two example models, a new method, called p-box convolution sampling (PCS), is compared with three other p-box propagation methods. It is found that, although PCS is less expensive than the alternatives, it is still relatively expensive and therefore only justifiable when the expected benefits are large. Several directions for further improving the efficiency of PCS are discussed."
1073,"An efficient single-loop approach for series system reliability-based design optimization problems is presented in this paper. The approach enables the optimizer to apportion the system reliability among the failure modes in an optimal way by increasing the reliability of those failure modes whose reliability can be increased at low cost. Furthermore, it identifies the critical failure modes that contribute the most to the overall system reliability. A previously reported methodology uses a sequential optimization and reliability approach. It also uses a linear extrapolation to determine the coordinates of the most probable points of the failure modes as the design changes. As a result, the approach can be slow and may not converge if the location of the most probable failure point changes significantly. This paper proposes an alternative system RBDO approach that overcomes the above difficulties by using a single-loop approach where the searches for the optimum design and for the most probable failure points proceed simultaneously. An easy to implement active set strategy is used. The maximum allowable failure probabilities of the failure modes are considered as design variables. The efficiency and robustness of the method is demonstrated on three design examples involving a beam, an internal combustion engine and a vehicle side impact. The results are compared with deterministic optimization and the conventional component RBDO formulation."
1074,"The aim of this paper is to develop and illustrate an efficient methodology to design blades with robust aerodynamic performance in the presence of manufacturing uncertainties. A novel geometry parametrization technique is developed to represent manufacturing variations due to tolerancing. A Gaussian Stochastic Process Model is trained using DOE techniques in conjunction with a high fidelity CFD solver. Bayesian Monte Carlo Simulation is then employed to obtain the statistics of the performance at each design point. A multiobjective optimizer is used to search the design space for robust designs. The multiobjective formulation allows explicit trade-off between the mean and variance of the performance. A design, selected from the robust design set is compared with a deterministic optimal design. The results demonstrate an effective method to obtain compressor blade designs which have reduced sensitivity to manufacturing variations with significant savings in computational effort."
1075,"This study focuses specifically on the relationship between function and risk in early design by presenting a mathematical mapping from product function to likelihood and consequence risk assessments that can be used in the conceptual design phase. An investigation of a spacecraft orientation subsystem is used to demonstrate the proposed mappings. The risk assessment presented in this paper is a tool that will aid designers by identifying risks as well as reducing the subjectivity of the likelihood and consequence value from a risk element, provide four key risk element properties (design parameter, failure mode, likelihood, and consequence) for numerous risk elements with a simple calculation, and provide a means for inexperienced designers to effectively address risk in the conceptual design phase. The investigation demonstrates that the method presented in this paper is a useful tool for preliminary identification and assessment of product risks."
1076,"Making appropriate environmental policy decisions requires considering various sources of uncertainty. An air pollution example is formulated as a design optimization problem with probabilistic constraints, also referred to as reliability-based design optimization (RBDO). Environmental applications with a large number of constraints and significant model complexity present special challenges. In this paper an efficient active set strategy is integrated with a reliability contour surface approach to solve probabilistic problems with non-normal variable probability distributions. Discrete random parameters, which result in Bayesian probability, are also present and they are incorporated using delta function approximations. Joint constraint reliability that considers satisfying all regulatory constraints is also discussed. A demonstration example of setting the optimal vehicle speed limit while maintaining high reliability for CO and NOx standards of a residential area near two highway systems is included."
1077,"Since no simulation model is perfect, any simulation model for modeling a system’s physical behavior can be refined further. Hence, the question faced by a designer is — "
1078,"In engineering design, information regarding the uncertain variables or parameters is usually in the form of finite samples. Existing methods in optimal design under uncertainty cannot handle this form of incomplete information; they have to either discard some valuable information or postulate existence of additional information. In this article, we present a reliability-based optimization method that is applicable when information of the uncertain variables or parameters is in the form of both finite samples and probability distributions. The method adopts a Bayesian Binomial inference technique to estimate reliability, and uses this estimate to maximize the confidence that the design will meet or exceed a target reliability. The method produces a set of Pareto trade-off designs instead of a single design, reflecting the levels of confidence about a design’s reliability given certain incomplete information. As a demonstration, we apply the method to design an optimal piston-ring/cylinder-liner assembly under surface roughness uncertainty."
1079,"Variations associated with stenting systems, artery properties, and doctor skills necessitate a better understanding of coronary artery stents so as to facilitate the design of stents that are customized to individual patients. This paper presents the development of an integrated computer simulation-based design approach using engineering finite element analysis (FEA) models for capturing stent knowledge, utility theory-based decision models for representing the design preferences, and statistics-based surrogate models for improving process efficiency. Two focuses of the paper are: 1) understanding the significance of engineering analysis and surrogate models in the simulation-based design of medical devices; 2) investigating the modeling implications in the context of stent design. The study reveals that the advanced nonlinear FEA software with analysis capacities on large deformation and contact interaction has offered a platform to execute high fidelity simulations, yet the selection of appropriate analysis models is still subject to the tradeoff between cost of analysis and accuracy of solution; the cost-prohibitive simulations necessitate the employment of surrogate models in subsequent multi-objective design optimization. A detailed comparison between regression models and Kriging models suggests the importance of sampling schemes in successfully implementing Kriging methods."
1080,"The use of metamodels in simulation-based robust design introduces a new source of uncertainty that we term "
1081,"The objective of this research was to develop a function-based method for analyzing the critical sequences of events that must occur for complex space missions to be successful. The resulting methodology, the Function-based Analysis of Critical Events, or FACE, uses functional and event models to identify the changes in functionality of a system as it transitions between critical mission events. Two examples are presented that detail the application of FACE to prior mission failures including the loss of the Columbia orbiter and the failure of the Mars Polar Lander probe. The result of the research is a methodology that allows designers to not only reduce the occurrence of such failures but also analyze the specific functional causes of the failures when they do occur."
1082,"Even though model-based simulations are widely used in engineering design, it remains a challenge to validate models and assess the risks and uncertainties associated with the use of predictive models for design decision making. In most of the existing work, model validation is viewed as verifying the model accuracy, measured by the agreement between computational and experimental results. However, from the design perspective, a good model is considered as the one that can provide the discrimination (good resolution) between design candidates. In this work, a Bayesian approach is presented to assess the uncertainty in model prediction by combining data from both physical experiments and the computer model. Based on the uncertainty quantification of model prediction, some design-oriented model validation metrics are further developed to guide designers for achieving high confidence of using predictive models in making a specific design decision. We demonstrate that the Bayesian approach provides a flexible framework for drawing inferences for predictions in the intended but may be untested design domain, where design settings of physical experiments and the computer model may or may not overlap. The implications of the proposed validation metrics are studied, and their potential roles in a model validation procedure are highlighted."
1083,"A multi-disciplinary modeling and design optimization formulation for uncertainty effects consideration is presented in this paper. The optimization approach considers minimization of uncertainty factors related to the overall system performance while satisfying target requirements specified in the form of constraints. The design problem is decomposed into two analysis systems; performance design and uncertainty effects analysis. Each design system can be partitioned into several subsystems according to the different functions they perform. Performance evaluation is considered by minimizing the variations between specified expected values of performance functions and their target values. Uncertainty effects analysis is defined by minimizing the ratio of the maximum variations caused by uncertainty factors over the expected function values. The entire problem has been treated as a multi-disciplinary design optimization (MDO) for maximum robustness and performance achievement. An electromechanical system is used as an example to demonstrate this optimization methodology."
1084,"Structural analysis and design optimization have recently been extended to consider various uncertainties. If the statistical data for the uncertainties are sufficient to construct the input distribution function, the uncertainties can be treated as random variables and RBDO is used; otherwise, the uncertainties can be treated as fuzzy variables and PBDO is used. However, many structural design problems include both uncertainties with sufficient data and uncertainties with insufficient data. For these problems, RBDO will yield an unreliable design since the distribution functions of uncertainties are not believable. On the other hand, treating the random variables as fuzzy variables and invoking PBDO may yield too conservative design with a higher optimum cost. This paper proposes a new design formulation using the performance measure approach (PMA). For the inverse analysis, this paper proposes a new most probable/possible point (MPPP) search method called maximal failure search (MFS), which is an integration of the enhanced hybrid mean value method (HMV+) and maximal possibility search (MPS) method. Some mathematical and physical examples are used to demonstrate the proposed inverse analysis method and design formulation."
1085,"The objective of reliability-based robust design optimization (RBRDO) is to minimize the product quality loss function subject to probabilistic constraints. Since the quality loss function is usually expressed in terms of the first two statistical moments, mean and variance, many methods have been proposed to accurately and efficiently estimate the moments. Among the methods, the univariate dimension reduction method (DRM), performance moment integration (PMI), and percentile difference method (PDM) are recently proposed methods. In this paper, estimation of statistical moments and their sensitivities are carried out using DRM and compared with results obtained using PMI and PDM. In addition, PMI and DRM are also compared in terms of how accurately and efficiently they estimate the statistical moments and their sensitivities of a performance function. In this comparison, PDM is excluded since PDM could not even accurately estimate the statistical moments of the performance function. Also, robust design optimization using DRM is developed and then compared with the results of RBRDO using PMI and PDM. Several numerical examples are used for the two comparisons. The comparisons show that DRM is efficient when the number of design variables is small and PMI is efficient when the number of design variables is relatively large. For the inverse reliability analysis of reliability-based design, the enriched performance measure approach (PMA+) is used."
1086,"Human Machine Interactions Systems (HMI) are decisive for acceptance and safety of new cockpits in the automotive as well as in the aerospace industries. A new design and simulation platform called INSIDES will be presented where virtual cockpit prototypes are being built based on 3D CAD geometry e.g. from CATIA and integrated with logical interaction data derived from UML specifications. This new development platform enables the continuous validation and check of new interaction concepts by involving usability engineers in the very early stage of the development cycle. Since the simulation work is being done in the context of the entire aircraft cockpit / car interior with all instruments, control commands as well as displays devices a better validation of HMI systems can be achieved."
1087,"Currently industrial design is mainly done by CAD and mock-up is created to evaluate the design. This process is repeated from the rough sketches to the final detailed mock-up until the designer is satisfied. In this process, creating the mock-up, especially detailed mock-up, is quite costly. Hence, there is a need to improve the efficiency of mock-up fabrication. One of the strategies for realizing this is to use virtual models (VM) instead of mock-up. VM is a model which is created on computer by 3D computer graphics, and it allows realistic graphical modeling which can be modified easily, reduces time considerably, and enables dynamic viewing of models from any angle and orientation. Despite this, mock-ups are still required because the process of design evaluation by touching physical models (PM) is still important to designers. To resolve this disadvantage of VM, this paper proposes a new evaluation system for industrial design. With this system, VM and the rapid prototyping mock-up are overlapped in virtual space to produce a tangible VM. This new type of VM functions just like a detailed mock-up but can be created much faster and cheaper. This system employs the concept of Augmented Virtuality (AV), which is mainly based on virtual space and real objects are added to reinforce the virtual space. In this case, haptic information from the rapid prototyping mock-up is added to optical information from the VM. When estimating the 3D position of a real object, optical information is used considerably more than haptic information. Therefore, if there are only a few positional or geometrical differences between the rapid prototyping mock-up and the VM, the differences can be offset by the (incorporated into the) optical information. For this reason, if the designer needs to modify the product shape, only the VM needs to be modified, allowing old mock-ups to be used repeatedly. This means that designers can evaluate a variety of product designs with only one mock-up, thus reducing both time and the costs for creating a mock-up. The operator wears a data glove on his/her hand to construct a virtual hand in the virtual space. With this virtual hand, the operator can also evaluate the user interface (UI) of the product by means of pushing buttons or watching display on the VM. This paper also provides a new method for overlapping the virtual space and real space."
1088,"A significant amount of research has established that product platform planning is effective for the development of multiple products and management strategy for companies in today’s market. However, there are still significant challenges in planning and the realization of product families and platforms. This is particularly true for determining family and platform architectures—imperative assets in companies in order to pursue competitive advantages. It is a challenging task because individual customization of products generally competes with the goal of maximizing platform commonality. To address this challenge, this paper introduces a graphical computer-based modeling environment to support product design teams in configuring modular product families. In the modeling environment, a product family can be decomposed into its products, modules, and functions. Also, interfaces among the product components can be elaborated by defining the relationship types (fundamental, redundant, and operational). Further elaboration can be achieved by defining an appropriate set of module drivers from four different perspectives: financial, customer, design processes, and organizational culture/IT. These features facilitate modeling of a product family at multiple levels of abstraction as capturing design drivers, reasoning and goals. The application of the modeling environment is illustrated with a family of coffee-makers. It is demonstrated how the proposed modeling method offers a comprehensive representation and understanding of product family planning by integrating multiple perspectives on modular architecting. Moreover, a matrix-based analysis option is provided for design teams to view the relationships between the technical functions and the forms, and the design goals and the customer requirements in a preliminary manner."
1089,"This paper describes our initial efforts to develop a 3D visualization tool that is part of an overall effort to create a Concept Generator, an automated conceptual design tool, to aid a designer during the early stages of the design process. The use of CAD software has diversified into various disciplines that have made use of simulation and software modeling tools for reasons that range from improving accuracy in design, reduction in lead times, and simple visualizations. The impacts of CAD software have been beneficial in industry and education. Described in this paper is the use of low-memory VRML models to represent components. These low memory models have been created to achieve several goals that complement the overall objectives of the concept generator. One key goal is that the concept generator be accessible via the web, thus the need for low-memory and low-data models. Additionally, as the concept generator is intended for usage during early conceptual design, the 3D visualization tool allows the creation of models upon which basic manipulations can be performed so that a designer can get an initial feel of the structure that his product is going to take. Our research has enabled us to create a basic visualization tool which, while similar in nature to most other CAD software tools, is unique, in that it represents the link, as a visual interface, between a formulated concept and the designer. The paper presents the research problem, an overview of the architecture of the software tool and some preliminary results on visual representations as an aid to concept generation."
1090,"The automotive industry is very competitive and companies are spending enormous amounts of resources on the development of new cars. The success of a new model is highly correlated to how well the designers and engineers have been able to blend features, functionality, quality and design to bring an attractive car to a certain segment of the market at the right time. Furthermore, as modern manufacturing techniques have enabled most manufacturers to offer standard features in their cars, the design has become a major selling point and one of the key factors for the ‘image’ associated with a company. However, the image, or form impression of a car, stated in natural language, is subtle and difficult to directly relate to concrete design parameters. With few tools to address this issue, designers are left to rely on their experience and sensitivity to current trends in order to meet the customer expectations for a new model. The purpose of the method reported in this paper is to provide a foundation for a design support system, which can help designers visualize and validate the complex relationship between form impressions and design parameters. This was achieved by expressing form impressions in natural language as sets of 10 weighted attributes. 14 design parameters were established to describe the basic shape of a car and data on the form impression for 31 different shapes was collected via a survey designed by the Taguchi method. Factor analysis was performed to extract correlated factors and eliminate the overlap of meaning between attributes. A neural network, able to relate form impressions expressed in these factors to basic proportions of a car, was created, trained and used to generalize design parameters corresponding to any form impression presented to it. Finally, a 3D-model with the desired form impression was automatically created by the CAD-system outlined in this paper. These results show that this method can be used to create a design support system, which has a sensibility to the form impressions various shapes will give."
1091,"Conceptual design is a key early activity in product development. However, limited understanding of the conceptual design process and lack of quantitative information at this stage of design pose difficulties for effective design concept generation and evaluation. In this paper, we propose a hierarchical co-evolutionary approach to supporting design concept generation and evaluation. The approach adopts a zigzag process in which grammar rules guide function decomposition and functions and means co-evolve at each level of decomposition hierarchy. It provides an automatic computational solution to complex conceptual design systems. In this paper, the details of the approach are described, and an example of designing a mechanical personal transporter is presented to show the effectiveness of the proposed approach."
1092,"Road Safety System development is complex task, which requires the collaboration between designers and accidentologists. However, designers and accidentologists do not share the same viewpoints, neither the same models to analyze an accident, nor the same technical language. This makes their communication a complex task in a design process. "
1093,"We present a two-step technique for learning reusable design procedures from observations of a designer in action. This technique is intended for the domain of parametric design problems in which the designer iteratively adjusts the parameters of a design so as to satisfy the design requirements. In the first step of the two-step learning process, decision tree learning is used to infer rules that predict which design parameter the designer will change for any particular state of an evolving design. In the second step, decision tree learning is again used, but this time to learn explicit termination conditions for the rules learned in the first step. The termination conditions are used to predict how large of a parameter change should be made when a rule is applied. The learned rules, and termination conditions, can be used to automatically solve new design problems with a minimum of human intervention. Initial experiments with this technique suggest that it is considerably more efficient than the previous technique which was incapable of learning explicit rule termination conditions. In particular, the rule termination conditions allow a program to automatically solve new design problems with far fewer iterations than required with the previous approach."
1094,"In traditional optimal control and design problems, the control gains and design parameters are usually derived to minimize a cost function reflecting the system performance and control effort. One major challenge of such approaches is the selection of weighting matrices in the cost function, which are usually determined via trial and error and human intuition. While various techniques have been proposed to automate the weight selection process, they either can not address complex design problems or suffer from slow convergence rate and high computational costs. We propose a layered approach based on Q-learning, a reinforcement learning technique, on top of genetic algorithms (GA) to determine the best weightings for optimal control and design problems. The layered approach allows for reuse of knowledge. Knowledge obtained via Q-learning in a design problem can be used to speed up the convergence rate of a similar design problem. Moreover, the layered approach allows for solving optimizations that cannot be solved by GA alone. To test the proposed method, we perform numerical experiments on a sample active-passive hybrid vibration control problem, namely adaptive structures with active-passive hybrid piezoelectric networks (APPN). These numerical experiments show that the proposed Q-learning scheme is a promising approach for."
1095,"Design collaboration is recognized as an effective approach in joint problem solving to achieve success of product development in distributed and heterogeneous environments. Design collaboration involves communication of design information, coordination of design activities, and negotiation of design conflicts between multi-disciplinary teams. To support these critical requirements in collaborative design, methodologies and software systems are needed. This paper shares our experience in the method and software development for a Web-enabled engineering object modeling environment. It presents our methods for interoperable and extensible design information modeling, for intelligent object behaviors embedment in CAD models, and for design information sharing across product lifecycle applications through a common vocabulary. The prototype implementation of the modeling environment provides standardized and localized engineering objects embedded with design semantics and intelligent behaviors for the information needs from multiple engineering software applications. The prototype also provides activity coordination and negotiation facilities through team setting, online visualization, live updating, conflict management, and messaging. Use scenarios are discussed in the paper."
1096,"Most complex systems, including engineering systems such as cars, airplanes, and satellites, are the results of the interactions of many distinct entities working on different parts of the design. Decentralized systems constitute a special class of design under distributed environments. They are characterized as large and complex systems divided into several smaller entities that have autonomy in local optimization and decision-making. A primary issue in "
1097,"This paper reviews recent developments and persisting challenges in facilitating collaborative engineering design. The review has two foci: 1) design information and representations, and 2) engineering workstations. Recent developments regarding these foci are discussed for their contribution to facilitating collaborative design. The paper concludes with directing attention to current challenges and recommendations for research."
1098,"Recently, almost all industrially manufactured consumer goods have a high level of engineering excellence, and product designers face an increasingly difficult task of creating products that will stand out in a competitive marketplace. At present, users tend to base their purchasing decisions on the product’s degree of fitness to their preferences, not the degree of functional fulfillment that the product offers. The development of products that are more attractive to users requires the consideration of human preferences and sensibilities, so-called “Kansei,” as well as the skillful application of these factors to the design sequence. The process of identifying and clarifying Kansei suggests that personal preferences concerning a given product are strongly influenced by both the person’s environment and the circumstances in which the product will be used. Analyzing both of these clarifies the influence that subconscious desires and human nature have on the expression of Kansei. This paper proposes a method for extracting the Kansei of potential customers and applying it to product designs that aim to maximize their human appeal, rather than their technical superiority."
1099,"In this paper, it is illustrated how computational design methods such as design optimization and probabilistic analysis is applied to system simulation models in a web based framework. Special emphasis is given models defined in the Modelica modeling language. An XML-based information system for representation and management of design data for use together with Modelica models as well as other types of models is proposed. This approach introduces a separation between the model of the system and data related to the design of the product. This is important in order to facilitate the use of computational methods in a generic way. A web based framework for integration of simulation models and computational methods is further illustrated. The framework is based on open standards for distributed computing and enables so-called service oriented architecture. Finally, an example is presented, where design optimization and probabilistic analysis is carried out on a Modelica model of an aircraft actuation system using the proposed and implemented tools and methods."
1100,"Synergies and integration in design set a mechatronic system apart from a traditional, multi-disciplinary system. This paper proposes a method for the modularization and evaluation of different mechatronic design concepts in the early stages of product development processes. In order to consider the specific aspects of complex systems, a design metric is presented, which assists the design engineer in finding the best solution concept. For the description and evaluation of a complex mechatronic system, it is essential to decompose the total system into a hierarchical structure of mechatronic sub-modules. The number of levels in the decomposition, as well as the number of mechatronic modules involved, is indicative of the complexity of the design task."
1101,"This research introduces an evolutionary design database model to describe design requirements and design results developed at different design stages from conceptual design to detailed design. In this model, the evolutionary design database is represented by a sequence of worlds corresponding to the design descriptions at different design stages. The design requirements and design results in each world are modeled using a database representation scheme that integrates both geometric descriptions and non-geometric descriptions. In each world, only the differences with its ancestor world are recorded. When the design descriptions in one world are changed, these changes are then propagated to its descendant worlds automatically. Consistency of the design descriptions in descendant worlds is also checked when design descriptions in an ancestor world are changed. Case study is conducted to show the effectiveness of this evolutionary design database model."
1102,"This paper proposes a spatial three degrees of freedom parallel kinematic machine enhanced by a passive leg and a web-based remote control system. First, the geometric model of the three degrees of freedom parallel kinematic machine is addressed; in the mechanism, a fourth kinematic link — a passive link connecting the base center to the moving platform center — is introduced. This last link is used to constrain the motion of the tool (located in the moving platform) to only three degrees of freedom, as well as to enhance the global stiffness of the structure and distribute the torque from machining. With the kinematic model, a web-based remote control approach is then applied. The concept of the web-based remote manipulation approach is introduced and the principles behind the method are explored in detail. Finally, an example of remote manipulation is demonstrated to the proposed 3-DOF structure using web-based remote control concept before conclusions."
1103,"Design in general is about increasing the information of the product/system. Therefore it is natural to investigate the design process from an information theoretical point of view. There are basically two (although related) strains of information theory. The first is the information theory of communication. Another strain is the algorithmic theory of information. In this paper the design process is described as a information transformation process, where an initial set of requirements are transformed to a system specification. Performance and cost are both a functions of complexity and refinement, that can be expressed in information theoretical terms. The information theoretical model is demonstrated on examples. The model has implications for the balance between number of design parameters, and the degree of convergence in design optimization. Furthermore, the relationship between concept refinement and design space expansion can be viewed in information theoretical terms."
1104,"To meet the needs for product variety, many companies are shifting from a mass production mode to mass customization, which demands quick response to the needs of individual customers with high quality and low costs. One of the key elements of mass customization is that the product family can share some modules. The multifunction nature of mechanical components necessitates designers to redesign them each time a component’s function changes. This is the main obstacle to the practical mechanical product family modeling. In this paper, a graph grammar based mechanical product family modeling method is proposed. The other issue that is studied in this paper is tolerancing, which is a critical part of the product design process because it is intimately related to a product’s quality and costs. A functional tolerance specification method, called the mirror method, is proposed to provide guidelines for the component’s datum reference frame construction and generic and uniform functional tolerance specifications."
1105,"Design optimization algorithms have traditionally focused on lowering weight and improving structural performance. Although cost is a vital factor in every emerging design, existing tools lack key features and capabilities in optimizing designs for minimum product cost at acceptable performance levels. This paper presents a novel methodology for developing a decision support tool for designers based on manufacturing cost. The approach focuses on exploiting the advantages offered by combining parametric CAD, Finite element analysis, feature based cost estimation and optimization techniques within a single automated system. This methodology is then applied in optimizing the geometry for minimum manufacturing cost of an engine mounting link from a Rolls-Royce civil aircraft engine."
1106,"The House of Quality is a popular tool that supports information processing and decision making in the engineering design process. While its application is an aid in conceptual aspects of the design process, its use as a quantitative decision support tool in engineering design is potentially flawed. This flaw is a result of assumptions behind the methodology of the House of Quality and is viewed as an important deficiency that can lead to potentially invalid and poor decisions. In this paper this deficiency and its implications are explored both experimentally and empirically. The resulting conclusions are important to future use and improvement of the House of Quality as an engineering design tool."
1107,"Supporting the decision of a group in engineering design is a challenging and complicated problem when issues like consensus, consistency, conflict, and compromise must be taken into account. In this paper, we present two developments extending the Group Hypothetical Equivalents and Inequivalents Method (Group-HEIM) and making it applicable to new classes of group decision problems. The first extension focuses on handling forms of value functions other than the traditional L1 -norm. The second extension focuses on updating the formulation to place unequal importance on the preferences of the group members. Typically, there are some group members whose experience, education, and/or knowledge makes their input more important. The formulation presented in this paper allows team leaders to emphasize the input from certain group members. Illustration and validation of the developments are presented using a vehicle selection problem. Data from twelve engineering design teams is used to demonstrate the application of the method."
1108,"An important aspect of product development is design for manufacturability (DFM) analysis that aims to incorporate manufacturing requirements into early product decision-making. Existing methods in DFM seldom quantify explicitly the tradeoffs between revenues and costs generated by making design choices that may be desirable in the market but costly to manufacture. This paper builds upon previous work coordinating models for engineering design and marketing product line decision-making by incorporating quantitative models of manufacturing investment and production allocation. The result is a methodology that considers engineering design decisions quantitatively in the context of manufacturing and market consequences in order to resolve tradeoffs, not only among performance objectives, but also between market preferences and manufacturing cost."
1109,"Flexible systems maintain a high performance level under changing operating conditions or design requirements. Flexible systems acquire this powerful feature by allowing critical aspects of their design con guration to change during the operating life of the product or system. In the design of such systems, designers are often required to make critical decisions regarding the exible and the non-exible aspects of the design con guration. We propose an optimization based methodology to design exible systems that allows a designer to effectively make such critical decisions. The proposed methodology judiciously generates candidate optimal design versions of the exible system. These design versions are evaluated using multiobjective techniques in terms of the level of exibility and the associated penalty. A highly exible system maintains optimal performance under changing operating conditions, but could result in increased cost and complexity of operation. The proposed methodology provides a systematic approach for incorporating designer preferences and selecting the most desirable design version — a feature absent in several recently proposed exible system design frameworks. The developments of this paper are demonstrated with the help of a exible three-bar-truss design example."
1110,"Collecting design error or failure information in a database (FKDB: Failure Knowledge Database) gives an organization an effective place for designers to study and learn past events so they will not repeat the same mistakes in their own design. When a designer makes an error, however, he had not foreseen the mistake at all. Once made, the error may seem trivial and even predictable, however, at the time of design, the problem and facts that surround the error are completely concealed from the designers mind. The designer, therefore, has no intention to look at past failure information that relate to the error he is repeating at the time of his design. This often makes the FKDB, despite of all the efforts in collecting the information it holds, a mere collection of past failure cases waiting for its passive use; the designer may occasionally look it up for the purpose of general study. A group of people including one of the authors, in the past, developed a conceptual design tool, Creative Design Engine (CDE) that helps the designer by displaying mechanisms, machines, sub-assemblies, and related information that realize functional requirements that the designer wants to accomplish. The tool effectively brings the designer’s consciousness to ideas new to him or something that escaped his mind at the time of conceptual design. We analyzed this tool and laid out the modifications necessary so that it not only displays design solutions and alternative options to the designer but also gives warnings to the designer about design error he is about to make during conceptual design. The application will constantly monitor the designer’s intention to compare it to known failures in the FKDB."
1111,"On March 26, 2004, a six-year-old boy ran into an automatic revolving door when it was about to close. The door caught the boy’s head and killed him. The accident immediately caught the attention of mass media, police, government, and the people. Amidst all the opinions and talks about how dangerous these automatic revolving doors are and how safety measures should be installed, we organized a group of volunteers to analyze the dynamics of the accident to measure the forces, door velocity, acceleration, and if available, the driving current and voltage of the motors. The group not only studied the same door that caused the fatal accident but it also ran the same series of tests on a smaller size power-assisted revolving door, an automatic sliding door, an elevator door, a building shutter, a commuter train door, a bullet train door, an automatic sliding door on an automobile, and its power window. With the safety mechanisms disabled, we measured an impact force of 548kgf on a dummy head of a 3-year old when it was jammed between the revolving door edge and the door frame. The human scull of a child crushes at only 100kgf and our results show that in addition to this large automatic revolving door, the smaller size power-assisted revolving door, the shutter, and manually closing automobile doors generate forces that exceed this limit. These doors inherit the danger of causing fatal accidents."
1112,"In and after about the year 2000, organizations have started to build databases of workers’ accidents, troubles in the production processes, and customer complaints to make positive use of such failure information. For quantifying such organizational applications and clarifying their problems, we developed a new worksheet, “Failure Knowledge Application Evaluation Sheet (FKAES)”, and conducted a survey by having members of the Association for the Study of Failure fill out the worksheet. Our research disclosed the following facts with organizations. They properly feedback failures that require action in the production and inspection processes, however, do not identify those that require action in the planning or development processes as failures because they have organizational causes rather than technical. Large corporations with 1,000 or more employees practice more applications than smaller ones, and some even publicize their failure applications to customers and stockowners."
1113,"In product design and manufacturing, robust design leads to a product that has good quality. Robust design is reviewed in two categories: one is the process and the other is the robustness index. The process means efficient manipulation of the mean response and the variance. The robustness index indicates a measure of insensitiveness with respect to the variation. To improve existing methods, a three-step robust design (TRD) is proposed. The first step is “reduce the variance,” the second is “find multiple candidate designs,” and the third is “select the optimum robust design by using the robustness index,” Furthermore, a new robustness index is introduced in order to accommodate the characteristics of the probability of success in axiomatic design and the Taguchi’s loss function. The new robustness indices are compared with the existing ones. The developed robust design process is verified by examples and the results using the robustness index are compared with those of other indices."
1114,"Process planning is a key product development activity that links design and manufacturing, and is traditionally carried out based on the outcome of the design process. One of the consequences of conducting process planning after design is that the process planning (manufacturability) information needed in the execution of upstream activities is in most cases not available formally. Using informal manufacturability information in the early phases of the product development process can lead to e.g. untrustworthy feasibility study or unnecessary design iterations. As an attempt to solve this problem, a modular procedure for execution of process planning activities is proposed in this paper. It allows for the execution of some of the process planning activities to commence as soon as the details of the order and the requirements for the product are known. The goal is to ensure that formal manufacturability information is available in various stages of the product development process, including to those activities that take place prior to process planning. The new modular process planning procedure has been applied, and it has been found that the design iterations caused by lack of manufacturability information can be avoided. This paper first defines the problem and presents related works. It then introduces the modular process planning procedure, and presents an application case study."
1115,"This research introduces a new approach to model the non-linear relations among different design evaluation measures and to achieve the optimal design considering these different design evaluation measures through multi-objective optimization. In this approach, different design evaluation measures are mapped to comparable design evaluation indices. The non-linear relation between a design evaluation measure and its design evaluation index is identified based on the least-square curve-fitting method. The weighting factors for different design evaluation indices, representing the importance measures of these indices in the multi-objective design optimization, are achieved using the pair-wise comparison method. A case study example of automobile caliper disc brake design considering 4 different design evaluation measures is given to illustrate the effectiveness of the introduced approach."
1116,"Design of modern engineering products requires complexity management. Several methodologies for complex system optimization have been developed in response. Single-level strategies centralize decision-making authority, while multi-level strategies distribute the decision-making process. This article studies the impact of coupling strength on single-level Multidisciplinary Design Optimization formulations, particularly the Multidisciplinary Feasible (MDF) and Individual Disciplinary Feasible (IDF) formulations. The Fixed Point Iteration solution strategy is used to motivate the analysis. A new example problem with variable coupling strength is introduced, involving the design of a turbine blade and a fully analytic mathematical model. The example facilitates a clear illustration of MDF and IDF and provides an insightful comparison between these two formulations. Specifically, it is shown that MDF is sensitive to variations in coupling strength, while IDF is not."
1117,"In this paper, we present the development and application of a Technical Feasibility Model (TFM) used in preliminary design to determine whether or not a set of desired product specifications is technically feasible, and the optimality of those specifications with respect to the Pareto frontier. The TFM is developed by integrating the capabilities of a multidisciplinary design framework, a multi-objective design optimization tool, a Pareto set gap analyzer, metamodeling methods, and mathematical methods for feasibility assessment. This tool is then applied to a three objective example problem and to a five objective passenger vehicle design problem by analyzing benchmarking data from 78 late model sedans."
1118,"This paper deals with development of Genetic Range Genetic Algorithms (GRGAs). In GRGAs, one of the key is to set a new searching range, it needs to be followed after current searching situations, to be focused on local minute search and to be scattered as widely as possible for global search. However, first two strategies have a possibility of early stage convergence, and random scattering cause vain function calls to produce the range which seems no chance to prosper for a number of generations. In this paper, we propose a new method of setting it by using Particle Swarm Optimization (PSO) to overcome dilemma of the conventional method."
1119,"One area in design optimization is component based design where the designer has to choose between many different discrete alternatives. These types of problems have discrete character and in order to admit optimization an interpolation between the alternatives is often performed. However, in this paper a modified version of the non-gradient algorithm the Complex method is developed where no interpolation between alternatives is needed. Furthermore, the optimization algorithm itself is optimized using a performance metric that measures the effectiveness of the algorithm. In this way the optimal performance of the proposed discrete Complex method has been identified. Another important area in design optimization is the case of optimization based on simulations. For such problems no gradient information is available, hence non-gradient methods are therefore a natural choice. The application for this paper is the design of an industrial robot where the system performance is evaluated using comprehensive simulation models. The objective is to maximize performance with constraints on lifetime and cost, and the design variables are discrete choices of gear boxes for the different axes."
1120,"This paper proposes a new design optimization framework by integrating evolutionary search and cumulative function approximation. While evolutionary algorithms are robust even under multi-peaks, rugged natures, etc., their computational cost is inferior to ordinary schemes such as gradient-based methods. While response surface techniques such as quadratic approximation can save computational cost for complicated design problems, the fidelity of solution is affected by density of samples. The new framework simultaneously performs evolutionary search and constructs response surfaces. That is, in its early phase the search is performed over roughly but globally approximated surfaces with the relatively small number of samples, and in its later phase the search is performed intensively around promising regions, which are revealed in the preceded phases, over response surfaces enhanced with additional samples. This framework is expected to be able to robustly find the optimal solution with less sampling. An optimization algorithm is implemented by combining a real-coded genetic algorithm and a Voronoi diagram based cumulative approximation, and it is applied to some numerical examples for discussing its potential and promises."
1121,"In this paper we are going to show that applying the twinkling technique on a naive random search algorithm is frequently more powerful than any algorithm using specific research techniques unless they use information provided by the gradient or the Hessian. In order to illustrate this result we have made the choice of the study of a mechanical system characterized by the non-linear nature of the optimization space. This system is basically an open kinematics chain that represents a robot which has to go through various different trajectories defined by a set of temporally equidistant points. In fact, we are going to show that the genetic algorithm, the simulated annealing algorithm, the particle swarm algorithm, the random search algorithm, need to use comparatively, a huge number of function evaluations in order to perform the same result quality."
1122,"In this paper the Variable Topography Distance Transform method (VTDT) is used to find optimal paths across physical landscapes for pipelines carrying a two-phase geothermal fluid. The method incorporates constraints such as obstacles, land costs, building costs, variable gradients, height, and environmental issues in the route selection process. The method is an expanded form of Distance Transform algorithms that are used in image processing. It offers a way to look at land surfaces as a slope-adjusted 2-D model rather than as a more complex and computationally intensive 3-D model. The VTDT method works with a digital representation, called a Digital Elevation Model (DEM), of the landscape in question. The method is then tested on the route design for pipelines carrying a two-phase geothermal fluid at the Hellisheidi Power Plant in Iceland, which is currently (early 2005) in the design and construction phase."
1123,"The purpose of this work is to develop a novel optimization process for the design of space frames. The main objective is to minimize the space frame volume and consider stress constraints satisfaction. A finite element program is devised to synthesize 3D-space frames and aid in its topology optimization. The program is verified through different elementary problems with known analytical solutions as well as with commercial packages. A Midi-Bus frame is modeled with about 300 members and analyzed for a severe road model condition. The optimization effectively uses the devised Heuristic Gradient Projection (HGP) technique to synthesize the optimum Midi-Bus frame. Results indicate a marked improvement over available designs and remarkably faster convergence over other optimization techniques. This technique can thus be effectively applied to other large 3D space frame synthesis and optimization."
1124,"In this paper, a new methodology to obtain an optimal structure size considering geometries nonlinearity is presented. This method makes use of Heuristic Gradient Projection method in addition to Fuzzy Logic. The Heuristic Gradient Projection (HGP) method, a previously developed method for 3D-frame design and optimization, utilizes mainly bending stress relations in order to simplify the process of iterations. HGP is based on comparing the resulting equivalent stress with the allowable stress value. The proposed Fuzzy Heuristic Gradient Projection (FHGP) approach incorporates both bending stress and axial stress when processing with the allowable stress value. The weighting factors of both axial and bending stresses are found using a Fuzzy Logic controller. Fuzzy logic is incorporated to reach an optimal solution with lesser number of function evaluations. A simple cantilever example, subjected to axial force and bending moment, is presented to illustrate this approach in addition to a 10-member planar frame that is used to prove the efficacy of the new method. FHGP approach generally results in faster convergence."
1125,"Design space exploration during conceptual design is an active research field. Most approaches generate a number of feasible design points (complying with the constraints) and apply graphical post-processing to visualize correlations between variables, the Pareto frontier or a preference structure among the design solutions. The generation of feasible design points is often a statistical (Monte Carlo) generation of potential candidates sampled within initial variable domains, followed by a verification of constraint satisfaction, which may become inefficient if the design problem is highly constrained since a majority of candidates that are generated do not belong to the (small) feasible solution space. In this paper, we propose to perform a preliminary analysis with Constraint Programming techniques that are based on interval arithmetic to dramatically prune the solution space before using statistical (Monte Carlo) methods to generate candidates in the design space. This method requires that the constraints are expressed in an analytical form. A case study involving truss design under uncertainty is presented to demonstrate that the computation time for generating a given number of feasible design points is greatly improved using the proposed method. The integration of both techniques provides a flexible mechanism to take successive design refinements into account within a dynamic process of design under uncertainty."
1126,"A methodology to reach the best CHP plant design is proposed. The standard method to choose the best fit in the process design of a CHP plant is improved considering off-design simulation of the pre-selected schemes. The off-design simulation deals specifically with economic dispatch optimization applied on each pre-selected plant to calculate the operation performance under well-known heat and power loads. The economic dispatch include fuel, uniform series payments related with investment and operation & maintenance costs and evaluates the variable behavior for power and heat along time with a scenario that takes into account transactions with the utility grid as well as auxiliary or back up boilers. It is shown the different result reached in each approach and the new point of view gotten with the use of the methodology proposed."
1127,"This paper presents some approaches to the optimal design of stacked-ply composite flywheels. The laminations of the disk are constructed such that the principal fiber direction is either tangential or radial. Here, optimization problems are formulated to maximize the energy density of the flywheel. This is accomplished by allowing arbitrary, continuous, variation of the orientation of the fibers in the radial plies. The paper compares designs based on minimizing cost functions related to the (i) the maximum stress, (ii) the maximum strain and (iii) the Tsai-Wu failure criteria. It is shown that the optimized designs provide an improvement in the flywheel energy density when compared to a standard stacked-ply design. The results also show that, for a given disk design, the estimate of the energy density can vary greatly depending on the failure criteria employed."
1128,"This paper discusses the roles that a graduate student coach experienced while working with an undergraduate design team in the development of a low, cost, low volume plastic injection modeling machine. Identified roles include: design tool teacher, design reviewer, project manager, and customer. A critique of the roles, including times spent in each role, is provided. This experience created generally higher satisfaction among the students and among the customers than had been previously seen in similar projects. Based upon this experience, it is justified to consider incorporating graduate design students as design coaches in senior design project teams."
1129,"During the last decade, digital prototyping has become a natural part of any industrial project dealing with product development. The reasons for this differ, but the two most obvious is time saving aspects and the amount of cost effectiveness achieved when replacing the physical prototype with the cheaper digital. Time and cost are equally, or even more critical in academic projects. This paper describes the usage of a low cost demonstrator as a mean to reduce both time and cost during a product development project course as well as to guarantee educational quality. The paper also discusses the reason for using demonstrators in an industrial environment. When large product development project courses are given at educational engineering programs, they often strive for imitating a real industrial situation, trying to include all the phases and aspects of product realization. Time is of course critical in both environments, industrial and academic, but for slightly different reasons. A typical industrial project may run over several years while a large educational project’s duration is counted in months. Thus, if the course tutor wants to simulate the whole product development process, within the same project course, there are needs for means that may speed up the project without spoiling the educational message as well as the industrial authenticity."
1130,"An engineer presented with a design challenge often creates a symmetric solution. For instance, consider a table (front-back and left-right symmetry), a car (left and right symmetry), a bridge (front-back and left-right symmetry), or the space shuttle (left-right) symmetry. These examples may not be 100% symmetric, but their overriding features are remarkably similar. The reasons for the design of symmetric structures is not always clear. In some cases, like the table, symmetry may be a tradition. Similarly, the symmetry may be for aesthetic reasons. However in automated design algorithms, especially stochastic techniques, the output is often largely asymmetric, One reason for this is that fitness functions are not rewarded for symmetry. A possible resolution to this is to add a reward function for symmetry. Unfortunately, this approach is computationally intractable as well as arbitrary. In this paper a Genetic Algorithm based method is presented that rewards re-use of parts. The method is applied to a simple, idealized situation as well as to real design case. The results show that in some situations, symmetry naturally emerges from the synthesis, but that it does not provide clear performance advantages over asymmetric configurations."
1131,"Multi-functional design problems are characterized by strong coupling between design variables that are controlled by stakeholders from different disciplines. This coupling necessitates efficient modeling of interactions between multiple designers who want to achieve conflicting objectives but share control over design variables. Various game-theoretic protocols such as cooperative, non-cooperative, and leader/follower have been used to model interactions between designers. Non-cooperative game theory protocols are of particular interest for modeling cooperation in multi-functional design problems. These are the focus of this paper because they more closely reflect the level of information exchange possible in a distributed environment. Two strategies for solving such non-cooperative game theory problems are: a) passing Rational Reaction Sets (RRS) among designers and combining these to find points of intersection and b) exchanging single points in the design space iteratively until the solution converges to a single point. While the first strategy is computationally expensive because it requires each designer to consider all possible outcomes of decisions made by other designers, the second strategy may result in divergence of the solution. In order to overcome these problems, we present an interval-based focalization method for executing decentralized decision-making problems that are common in multi-functional design scenarios. "
1132,"This research investigates the use of quantitative measures of performance to aid the grammatical synthesis of mechanical systems. Such performance measures enable search algorithms to be used to find designs that meet requirements and optimize performance by using automatically generated performance feedback, including behavioral simulation, as a guide. The work builds on a new type of production system, a parallel grammar for mechanical systems based on a Function-Behavior-Structure representation, to generate an extensive variety of designs. Geometric and topological constraints are used to bound the design space, termed the language of the grammar, to ensure the validity of designs generated. The winding mechanism of an electromechanical camera is examined as a case study using the behavioral modeling language Modelica. Behavioral simulations are run for parametric models generated by the parallel grammar and this data is used, in addition to geometric performance metrics, for performance evaluation of generated alternative designs. Multi-objective stochastic search, in the form of a hybrid pattern search developed as part of this research, is used to generate Pareto sets of optimally directed designs of winding mechanisms, showing the design of the camera chosen for the case study to be optimally directed with respect to the design objectives considered. The Pareto sets generated illustrate the range of simulation-driven solutions that can be generated and simulated automatically as well as their performance tradeoffs."
1133,"This paper documents a meta-analysis of 113 data sets from published factorial experiments. The study quantifies regularities observed among factor effects and multi-factor interactions. Such regularities are known to be critical to efficient planning and analysis of experiments and to robust design of engineering systems. Three previously observed properties are analyzed — effect sparsity, hierarchy, and heredity. A new regularity is introduced and shown to be statistically significant. It is shown that a preponderance of active two-factor interaction effects are synergistic, meaning that when main effects are used to increase the system response, the interaction provides an additional increase. The potential implications for robust design are discussed."
1134,"In this paper, we discuss a way to extend a geometric surface feature framework known as Direct Surface Manipulation (DSM) into a volumetric mesh modeling paradigm that can be directly adopted by large-scale CAE applications involving models made of volumetric elements, multiple layers of surface elements or both. By introducing a polynomial-based depth-blending function, we extend the classic DSM mathematics into a volumetric form. The depth-blending function possesses similar user-friendly features as DSM basis functions permitting ease-of-control of the continuity and magnitude of deformation along the depth of deformation. Practical issues concerning the implementation of this technique are discussed in details and implementation results are shown demonstrating the versatility of this volumetric paradigm for direct modeling of complex CAE mesh models. In addition, the notion of a model-independent, volumetric-geometric feature is introduced. Motivated by modeling clay with sweeps and templates, a model-independent, catalog-able volumetric feature can be created. Deformation created by such a feature can be relocated, reoriented, duplicated, mirrored, pasted, and stored independent of the model to which it was originally applied. It can serve as a design template, thereby saving the time and effort to recreate it for repeated uses on different models (frequently seen in CAE-based Design of Experiments study)."
1135,"The work described in this paper seeks to minimize the time spent on manually reducing thin-walled CAD-geometry into surface idealizations. The purpose of the geometrical idealizations is the creation of shell element meshes for FE-calculations. This is motivated by time and thereby cost savings and also to make the results of the calculations available earlier in the product development process allowing the results to guide the designs to a larger extent. Systems for automated geometry idealization and creation of FE-models already exist, but this paper describes a novel approach with the working principle of analyzing how the CAD-specific features of the CAD-file history tree are constituted. This information is used to automatically create the best practice geometrical idealization in the same CAD-model. Evaluation of the performance of the system in an industrial example is also presented."
1136,"In this research we describe a computer-aided approach to geometric tolerance analysis for assemblies and mechanisms. This new tolerance analysis method is based on the “generate-and-test” approach. A series of as-manufactured component models are generated within a NURBS-based solid modeling environment. These models reflect errors in component geometry that are characteristic of the manufacturing processes used to produce the components. The effects of different manufacturing process errors on product function is tested by simulating the assembly of these imperfect-form component models and measuring geometric attributes of the assembly that correspond to product functionality. A tolerance analysis model is constructed by generating-and-testing a sequence of component variants that represent a range of manufacturing process capabilities. The generate-and-test approach to tolerance analysis is demonstrated using a case study that is based on a high-speed stapling mechanism. As-manufactured models that correspond to two different levels of manufacturing precision are generated and assembly between groups of components with different precision levels is simulated. Misalignment angles that correspond to functionality of the stapling mechanism are measured at the end of each simulation. The results of these simulations are used to build a tolerance analysis model and to select a set of geometric form and orientation tolerances for the mechanism components. It is found that this generate-and-test approach yields insight into the interactions between individual surface tolerances that would not be gained using more traditional tolerance analysis methods."
1137,"Rapid and representative reconstruction of geometric shape models from surface measurements has applications in diverse arenas ranging from industrial product design to biomedical organ/tissue modeling. However, despite the large body of work, most shape models have had limited success in bridging the gap between reconstruction, recognition, and analysis due to conflicting requirements. On one hand, large numbers of shape parameters are necessary to obtain meaningful information from noisy sensor data. On the other hand, search and recognition techniques require shape parameterizations/abstractions employing few robust shape descriptors. The extension of such shape models to encompass various analysis modalities (in the form of kinematics, dynamics and FEA) now necessitates the inclusion of the appropriate physics (preferably in parametric form) to support the simulation based refinement process. Thus, in this paper we discuss development of a class of parametric shape abstraction models termed as extended superquadrics. The underlying geometric and computational data structure intimately ties together implicit-, explicit- and parametric- surface representation together with a volumetric solid representation that makes them well suited for shape representation. Furthermore, such models are well suited for transitioning to analysis, as for example, in model-based non rigid structure and motion recovery or for mesh generation and simplified volumetric-FEA applications. However, the development of the concomitant methods and benchmarking is necessary prior to widespread acceptance. We will explore some of these aspects further in this paper supported with case studies of shape abstraction from image data in the biomedical/life-sciences arena whose diversity and irregularities pose difficulties for more traditional models."
1138,"Surfaces, like planes, cylinders or spheres, are basic primitive surfaces not only for mechanical engineering but also for aesthetic design, world of free-form surfaces, where they are essentially used to answer some functional constraints, like assembling and manufacturing ones, or to achieve specific light effects. The early design steps are characterised by the uncertainty in the definition of the precise geometry and most of the time, product constraints are only partially available. Unfortunately, until now, the insertion of primitive surfaces requires precise curve and surface specifications together with trimming operations, thus imposing that the free-form geometry is recreated each time a modification occurs. In this paper we present a method for the insertion of planar surfaces suitable to handle the uncertainty in the first draft of a product. The approach does not provide effective precise primitive surfaces, but it is able to introduce regions resembling such a behaviour in a free-form surface, without requiring trimming operations, so allowing more efficient shape alternative evaluations."
1139,"In this paper, groups of individual features, i.e. a point, a line, and a plane, are called "
1140,"A new math model for geometric tolerances is used to build the frequency distribution for clearance in an assembly of parts, each of which is manufactured to a given set of size and orientation tolerances. The central element of the new math model is the Tolerance-Map®  (T-Map® ); it is the range of points resulting from a one-to-one mapping from all the variational possibilities of a feature, within its tolerance-zone, to a specially designed Euclidean point-space. A functional T-Map represents both the acceptable range of 1-D clearance and the acceptable limits to the 3-D variational possibilities of the target face consistent with it. An accumulation T-Map represents all the accumulated 3-D variational possibilities of the target which arise from allowable manufacturing variations on the individual parts in the assembly. The geometric shapes of the accumulation and functional maps are used to compute a measure of all variational possibilities of manufacture of the parts which will give each value of clearance. The measures are then arranged as a probability density function over the acceptable range of clearance, and a beta distribution is fitted to it. The method is applied to two examples."
1141,"This paper presents a computational method for designing assemblies with a built-in disassembly pathway that maximizes the profit of disassembly while satisfying regulatory requirements for component retrieval. Given component revenues and components to be retrieved, the method simultaneously determines the spatial configurations of components and locator features on the components, such that the product can be disassembled in the most profitable sequence, via a domino-like “self-disassembly” process triggered by the removal of one or a few fasteners. The problem is posed as optimization and a multi-objective genetic algorithm is utilized to search for Pareto-optimal designs in terms of three objectives: 1) the satisfaction of distance specification among components, 2) the efficient use of locator features on components, and 3) the profit of overall disassembly process under the regulatory requirements. A case study with different costs for removing fasteners demonstrates the effectiveness of the method in generating design alternatives under various disassembly scenarios."
1142,"Rounds and fillets are important design features. We introduce a new point-based method for constant radius rounding and filleting. Based on the mathematical definitions of offsetting operations, discrete offsetting operations are introduced. Steps of our approach are discussed and analyzed. The methodology has been implemented and tested. We present the experimental results on accuracy, memory and running time for various input geometries and radius. Based on the test results, the method is very robust for all kinds of geometries."
1143,"This paper focuses on efficient automatic recognition algorithms for turning features. As with other domains, recognition of interacting features is a difficult issue, because feature interaction removes faces and alters the topology of the existing turned features. This paper presents a method for efficiently recognizing both isolated (without interaction with other features) and interacting rotational features from geometrical CAD model of mill/turn parts. Additionally, the method recognizes Transient Turned Features (TTFs) that are defined as maximal axisymmetric material volumes from a non-turning feature that can be removed by turning. A TTS may not share any faces with the finished part. First, the rotational faces on a solid model are explored to extract isolated rotational features and some of the interacting ones. Then portions of the 3D model where no rotational faces can be used to recognize turning features are cut out and processed by a novel algorithm for finding their transient turning features."
1144,"Solving packing problems corresponds to finding the optimal placement of a series of objects in an enclosed space, while satisfying functional requirements. The research presented in this paper proposes a “packing GA” designed especially for the packing problems. It differs from the traditional GA by its encoding method and GA operators, which are tailored for the configuration design problem. In this paper, the detailed implementation and the design principles of the packing GA are presented. To evaluate the effectiveness of the proposed algorithm, a strict definition of acceptable layout is given, so that the performance of the GA can be judged by a more meaningful criterion. The packing GA is tested against two other GAs on an 8 box packing problem. The results show the packing GA has a much better chance to find the global optimum."
1145,"In this paper we present a simple new algorithm to offset multiple, non-overlapping polygons with arbitrary holes that makes use of winding numbers. Our algorithm constructs an intermediate “raw offset curve” as input to the tessellator routines in the OpenGL Utility library (GLU), which calculates the winding number for each connected region. By construction, the invalid loops of our raw offset curve bound areas with non-positive winding numbers and thus can be removed by using the positive winding rule implemented in the GLU tessellator. The proposed algorithm takes "
1146,"Understanding of the shape and size of different features of human body from the scanned data is necessary for automated design and evaluation of product ergonomics. In this paper, a computational framework is presented for "
1147,"Development of tolerance analysis methods that are consistent with the ASME and ISO GD&T (geometric dimensioning and tolerancing) standards is a challenging task. Such methods are the basis for creating computer-aided tools for 3D tolerance analysis and assemblability analysis. These tools, along with the others, make it possible to realize virtual manufacturing, in order to shorten lead-time and reduce cost in the product development process. Current simulation tools for 3D tolerance analysis and assemblability analysis are far from satisfactory because the underlying variation algorithms are not fully consistent with the GD&T standards. Better algorithms are still to be developed. Towards that goal, this paper proposes a complete algorithm for 3D slot features and tab features (frequently used in mechanical products) for 3D simulation-based tolerance analysis. The algorithms developed account for bonus/shift tolerances (i.e. effects from material condition specifications), and tolerance zone interaction when multiple tolerances are specified on the same feature. A case study is conducted to demonstrate the algorithm developed. The result from this work is compared with that from 1D tolerance chart method. The comparison study shows quantitatively why 1D tolerance chart method, which is popular in industry, is not sufficient for tolerance analysis, which is 3D in nature."
1148,"A generative automotive engine CAPP (Computer-Aided-Process Planning) system based on Oracle database was developed. A practical tolerance information communication solution between the CAD and CAPP system was proposed and realized in an independent file schema. A parametrical cylinder head model based on manufacturing feature was created to meet the requirements of the CAPP system feature recognizing. The data structure of the tolerance information in the Pro/ENGINEERING database was presented. A Pro/TOOLKIT program was developed to extract the model’s tolerance information automatically. The tolerance information extracted was then organized with EXPRESS language and imported into the CAPP system database Oracle."
1149,"A method for cutting non-circular holes on a bent thick plate is proposed. Generally, in order to cut holes on large plates, a special-purpose 5-axis machine is needed. However, such a machine is unavailable in most machine shops. This paper provides a description of a method that utilizes a general-purpose 5-axis water-jet machine in place of the special-purpose machine: First, the bent piece is transformed into a flat plate, where the shape of the holes is reconstructed by considering deformation during bending. Then, after 5-axis NC data is generated, the holes on the flat plate are cut using the 5-axis water-jet machine. In the final step, the desired shape of the piece is obtained by bending the plate with its newly-cut holes. Some illustrations are provided in order to show the validity of the proposed method."
1150,"The fundamental principle of MR fluid fan clutch in transmitting torque is analyzed, in the meantime, a shear model of MR clutch is proposed; MR fluid fan clutch having simple, novel structure is designed and made; At the same time, on the basis of experiments, the characteristic of velocity regulating of the clutch is studied in detail. The experimental results indicate that, compared with the shearing rate, the change of magnetic fields has a tremendous influence on the speed regulating characteristic of a fan clutch, and output torque of fan clutch can satisfy demand of engine cooling fan."
1151,"The springback is a significant manufacturing defect in the stamping process. A serious impediment to the use of lighter-weight, higher-strength materials in manufacturing is the relative lack of understanding about how these materials respond to the complex forming process. The springback problem can be reduced by using appropriate designs of die, punch, and blank holder shape together with friction and blank holding force. That is, an optimum stamping process can be determined using a gradient-based optimization to minimize the springback. However, for an effective optimization of the stamping process, development of an efficient analytical design sensitivity analysis method is crucial. In this paper, a continuum-based shape and configuration design sensitivity analysis (DSA) method for the stamping process has been developed. The material derivative concept is used to develop the continuum-based design sensitivity. The design sensitivity equation is solved without iteration at each converged load step in the finite deformation elastoplastic nonlinear analysis with frictional contact, which makes the design sensitivity calculation very efficient. The accuracy and efficiency of the proposed method is illustrated by minimizing springback in an S-rail part, which is often used as an industrial benchmark to verify the numerical procedures employed for stamping processes."
1152,"The automobile seat must satisfy various safety regulations for the passenger’s safety. In many design practices, each component is independently designed by concentrating on a single regulation. However, since multiple regulations can be involved in a seat component, there may be a design confliction among the various safety regulations. Therefore, a new design methodology is required to effectively design an automobile seat. The axiomatic approach is employed to consider multiple regulations. The Independence Axiom is used to define the overall flow of the seat design. Functional requirements (FRs) are defined by safety regulations and components of the seat are classified into groups which yield design parameters (DPs). The classification is carried out to keep the independence in the FR-DP relationship. Components in the DP group are determined by using the orthogonal arrays of the design of experiments (DOE). Numerical analyses are utilized to evaluate the safety levels by using a commercial software system for nonlinear transient finite element analysis."
1153,"The paper describes the design optimization of different refractory components used in the continuous casting process. In the first case, an impact pad of a continuous caster tundish is optimized for its turbulence suppression capability, while the inclusion particle trapping of the design is monitored. The impact pad is used in isolation as the only tundish furniture component. In the second case, the Submerged Entry Nozzle (SEN) of the continuous caster mold is optimized for minimum meniscus turbulent kinetic energy (i.e., stable meniscus). In both cases, the design variables are geometrical in nature. The steady-state flow and thermal patterns in the tundish and mold are obtained using the commercial CFD solver FLUENT. In order to perform optimization, the geometries are parameterized and incorporated into a mathematical optimization problem. FLUENT and its pre-processor GAMBIT are linked to a commercial design optimization tool, LS-OPT, to automatically improve the designs using metamodel approximations. The optimization results show a reduction of 12.5% in the turbulence on the slag layer of the tundish, while for the SEN, the results for one design iteration only are shown, due to the high cost of the function evaluations. The final paper will contain additional results. The SEN base and improved designs are validated using water modeling."
1155,"The methodology presented in this paper is implemented through a tool that integrates the functionality needed to perform accurate CHP market analysis. This tool includes the selection of target market segments and representative buildings, hourly building loads and characteristics, alternative CHP configurations, control rules and equipment management strategies, as well as detailed utility rates, components-based economics and reliability data. Results obtained by using the full capability of this tool are compared with less rigorous screening methods that use average building loads, constant equipment characteristics, and average utility rates. The comparison of results demonstrates that the utilization of the latter methods allows faster market screenings, but generates results that may lead to loss of capital investment, equipment operation and designs that are far from optimal, and erroneous energy policies."
1156,"Inspection is an important stage in the manufacturing process of machined parts. Coordinate measuring machines (CMM) have become more automatic, programmable, and capable of fulfilling the growing demands of inspection. However, fixturing (datum alignment) of parts is still done manually, consuming valuable inspection time. In this paper, we describe an automated datum alignment technique which integrates a vision system with the CMM to avoid part fixturing. The rough position of the part is estimated through image analysis. This initial reference frame drives the CMM through an automatic datum alignment procedure, thereby automatically establishing the reference frame without the use of fixtures. This technique has been demonstrated for two and a half dimensional (2.5D) machined parts with well-defined features that exhibit a stable position on a flat table."
1157,"In order to prevent machine tool feed slide system from transient vibrations during operation, machine tool designers usually adopt some typical design solutions; box-in-box typed feed slides, optimizing moving body for minimum weight and dynamic compliance, and so on. Despite all efforts for optimizing design, a feed drive system may experience severe transient vibrations during high-speed operation if its feed-rate control is not suitable. A rough feed-rate curve having discontinuity in its acceleration profile causes serious vibrations in the feed slides system. This paper presents a feed-rate optimization of a ball screw driven machine tool feed slide system for its minimum vibrations. A ball screw feed drive system was mathematically modeled as a 6-degree-of-freedom lumped parameter model. Then, a feed-rate optimization of the system was carried out for minimum vibrations. The main idea of the feed-rate optimization is to find out the most appropriate smooth acceleration profile having jerk continuity. A genetic algorithm, G.A., was used in this feed rate optimization."
1158,"The effectiveness of using Computer Aided Engineering (CAE) tools to support design decisions is often hindered by the enormous computational demand of complex analysis models, especially when uncertainty is considered. Approximations of analysis models, also known as “metamodels”, are widely used to replace analysis models for optimization under uncertainty. However, due to the inherent nonlinearity in occupant responses during a crash event and relatively large numbers of uncertain variables and responses, naive application of metamodeling techniques can yield misleading results with little or no warning from the algorithms which generate the metamodels. Furthermore, in order to improve the quality of metamodels, a relatively large number of design of experiments (DOE) and comparatively expensive metamodeling techniques, such as Kriging or radial basis function (RBF), are necessary. Thus, sampling-based methods, e.g. Monte Carlo simulations, for obtaining the statistical quantities of system responses during the optimization loop may still be inefficient even for these metamodels. In recent years, analytical uncertainty propagation via metamodels is proposed by Chen et al. 2004, which provides analytical formulation of mean and variance evaluations via a variety of metamodeling techniques to reduce the computational time and improve the convergence behavior of optimization under uncertainty. An occupant restraint system design problem is used as an example to test the applicability of this method."
1159,"In this paper, some new developments to the packing optimization method based on the rubber band analogy are presented. This method solves packing problems by simulating the physical movements of a set of objects wrapped by a rubber band in the case of two-dimensional problems or by a rubber balloon in the case of three-dimensional problems. The objects are subjected to elastic forces applied by the rubber band to their vertices as well as reaction forces when contacts between objects occur. Based on these forces, objects translate or rotate until maximum compactness is reached. To improve the compactness further, the method is enhanced by adding two new operators: volume relaxation and temporary retraction. These two operators allow temporary volume (elastic energy) increase to get potentially better packing results. The method is implemented and applied for three-dimensional arbitrary shape objects."
1160,"Vibration attenuation techniques in cutting tools can save old machines and enhance design flexibility in new manufacturing systems. The finite element method is employed to investigate structural stiffness, damping, and switching methodology under the use of smart material in tool error attenuation. This work discusses the limitations of using lumped mass modeling in toolpost dynamic control. Transient solution for tool tip displacement is obtained when pulse width modulation (PWM) is used for smart material activation during the compensation of the radial disturbing cutting forces. Accordingly a Fuzzy algorithm is developed to control actuator voltage level toward improved dynamic performance. The required minimum number of PWM cycles in each disturbing force period is investigated to diminish tool error. Time delay of applied voltage during error attenuation is also evaluated. Toolpost static force-displacement diagram as required to predict voltage intensities for error reduction is tested under different dynamic operating conditions."
1161,"Fibre Metal Laminates (FML) are a member of the hybrid materials family, consisting of alternating metal layers and layers of fibres embedded in a resin. Improved damage resistance and tolerance result in a significant weight and maintenance cost reduction compared to aluminium. FML also give the aircraft engineer additional design freedom, such as local tailoring of laminate properties. However, experience has shown that FML’s provide the aircraft manufacturer with many challenges as well. With increasing complexity of the structure, requirements from different disciplines within the engineering process will start to interfere, resulting in conflicts. This article discusses the current engineering process of FML fuselage panels as applied at Stork/Fokker Aerospace (FAESP). A case study is presented, clarifying the current design process and the way requirements start to interfere during the engineering process. A new approach based on Knowledge Engineering is discussed, implementing knowledge from engineers from all disciplines in an early stage of the design process. An automated design approach for FML fuselage panels is presented, using the same design parameters as the current approach. Because of the high complexity of the design, requirements start to conflict. Fulfilling all requirements with a traditional engineering approach results in an iterative and time consuming process. Automation of the design process, integrating knowledge and requirements from all disciplines, results in a fast and transparent design approach."
1162,"A flexible information model for systematic development and deployment of product families during all phases of the product realization process is crucial for product-oriented organizations. In this paper we propose a unified information model to capture, share, and organize product design contents, concepts, and contexts across different phases of the product realization process using a web ontology language (OWL) representation. Representing product families by preconceived common ontologies shows promise in promoting component sharing while facilitating search and exploration of design information over various phases and spanning multiple products in a family. Three distinct types of design information, namely, (1) customer needs, (2) product functions, and (3) product components captured during different phases of the product realization process, are considered in this paper to demonstrate the proposed information model. Product vector and function component mapping matrices along with the common ontologies are utilized for designer-initiated information exploration and aggregation. As a demonstration, six products from a family of power tools are represented in OWL DL (Description Logic) format, capturing distinct information needed during the various phases of product realization."
1163,"The product development process is a series of asynchronous process steps in which the geometry, the materials, and the manufacturing processes are defined to meet the performance and cost requirements. During the product design, information about a product is initially sparse and becomes more detailed as the process matures. In this paper, we apply a systems complexity analysis methodology to track the evolution of information complexity for several design process workflows. We used a frame-slot based model to store parametric design information, defined the size and link complexity measures for the design information and tracked the evolution of the knowledge-base complexity throughout the design process. Product design through injection molding is taken as an example to illustrate the utility of our approach and the static and dynamic aspects of the complexity of design information are analyzed."
1164,"This work explains the development of an integrated modeler, which is applied in the design-to-manufacturing stages of manufacturing processes namely machining, sheet metal processing and forging. Its system architecture is broadly divided into four modules namely, Feature Based Design (FBD), Virtual Factory Environment (VFE), Process Based Feature Mapping (PBFM) and Process Planning (PP). Feature based design is used for the design, modeling, synthesis, representation and validation of the components for manufacturing applications. New set of features namely integrated features are pre-defined as feature templates and instanced to get / derive the information required for the design-to-manufacturing stages of the components. VFE defines the factory, which provides the database for operations, machines, cutting tools, work pieces etc. The knowledge base of the developed system maps validated features of the component into operation sets in the first phase of the PBFM. Each operation in the operation sets can be executed using different machines and tools in a factory. All these possible choices are obtained in the second phase of PBFM. Genetic algorithm is used to find the optimal sequence of operations, machines and tools for different criteria in the process planning stage. This paper explains the developed system with case studies."
1165,"The process of constructing computationally benign approximations of expensive computer simulation codes, or metamodeling, is a critical component of several large-scale Multidisciplinary Design Optimization approaches. Such applications typically involve complex models, such as finite elements, computational fluid dynamics, or chemical processes. The decision regarding the most appropriate metamodeling approach usually depends on the type of application. However, several newly-proposed kernel-based metamodeling approaches can provide consistently accurate performance for a wide variety of applications. The authors recently proposed one such novel and effective metamodeling approach — the Extended Radial Basis Function approach — and reported encouraging results. To further understand the advantages and limitations of this new approach, we compare its performance to that of the typical radial basis function approach, and another closely related method — kriging. Several test functions with varying problem dimensions and degrees of nonlinearity are used to compare the accuracies of the metamodels using these metamodeling approaches. We consider several performance criteria, such as metamodel accuracy. effect of sampling technique, effect of problem dimension, and computational complexity. The results suggest that the E-RBF approach is a potentially powerful metamodeling approach for MDO-based applications."
1166,"Metamodels are becoming increasingly popular for representing unknown black box functions. Several metamodel classes exist, including response surfaces and spline-based models, kriging and radial basis function models, and neural networks. For an inexperienced user, selecting an appropriate metamodel is difficult due to a limited understanding of the advantages and disadvantages of each metamodel type. This paper reviews several major metamodeling techniques with respect to their advantages and disadvantages and compares several significant metamodel types for use as a black box metamodeling tool. The results make a strong case for using Non-Uniform Rational B-spline (NURBs) HyPerModels as a generic metamodeling tool."
1167,"This paper presents a new method to construct response surface function and a new hybrid optimization method. For the response surface function, the radial basis function is used for a zeroth-order approximation, while new bases is proposed for the moving least squares method for a first-order approximation. For the new hybrid optimization method, the gradient-based algorithm and pattern search algorithm are integrated for robust and efficient optimization process. These methods are based on: (1) multi-point approximations of the objective and constraint functions; (2) a multi-quadric radial basis function for the zeroth-order function representation or radial basis function plus polynomial based moving least squares approximation for the first-order function approximation; and (3) a pattern search algorithm to impose a descent condition. Several numerical examples are presented to illustrate the accuracy and computational efficiency of the proposed method for both function approximation and design optimization. The examples for function approximation indicate that the multi-quadric radial basis function and the proposed radial basis function plus polynomial based moving least squares method can yield accurate estimates of arbitrary multivariate functions. Results also show that the hybrid method developed provides efficient and convergent solutions to both mathematical and structural optimization problems."
1168,"Probabilistic design in complex design spaces is often a computationally expensive and difficult task because of the highly nonlinear and noisy nature of those spaces. Approximate probabilistic methods, such as, First-Order Second-Moments (FOSM) and Point Estimate Method (PEM) have been developed to alleviate the high computational cost issue. However, both methods have difficulty with non-monotonic spaces and FOSM may have convergence problems if noise on the space makes it difficult to calculate accurate numerical partial derivatives. Use of design and Analysis of Computer Experiments (DACE) methods to build polynomial meta-models is a common approach which both smoothes the design space and significantly improves the computational efficiency. However, this type of model is inherently limited by the properties of the polynomial function and its transformations. Therefore, polynomial meta-models may not accurately represent the portion of the design space that is of interest to the engineer. The objective of this paper is to utilize Gaussian Process (GP) techniques to build an alternative meta-model that retains the properties of smoothness and fast execution but has a much higher level of accuracy. If available, this high quality GP model can then be used for fast probabilistic analysis based on a function that much more closely represents the original design space. Achieving the GP goal of a highly accurate meta-model requires a level of mathematics that is much more complex than the mathematics required for regular linear and quadratic response surfaces. Many difficult mathematical issues encountered in the implementation of the Gaussian Process meta-model are addressed in this paper. Several selected examples demonstrate the accuracy of the GP models and efficiency improvements related to probabilistic design."
1169,"This study presents a compromise approach to augmentation of response surface (RS) designs to achieve the desired level of accuracy. RS are frequently used as surrogate models in multidisciplinary design optimization of complex mechanical systems. Augmentation is necessitated by the high computational expense typically associated with each function evaluation. As a result previous results from lower fidelity models are incorporated into the higher fidelity RS designs. The compromise approach yields higher quality parametric polynomial response surface approximations than traditional augmentation. Based on the D-optimality criterion as a measure of RS design quality, the method simultaneously considers several polynomial models during the RS design, resulting in good quality designs for all models under consideration, as opposed to good quality designs only for lower order models as in the case of traditional augmentation. Several numerical and an engineering example are presented to illustrate the efficacy of the approach."
1170,"A paradigm shift is underway in which the classical materials "
1171,"Through the use of generalized spherical harmonic basis functions a spectral representation is used to model the microstructure of cubic materials. This model is then linked to the macroscopic elastic properties of materials with Cubic Triclinic and Cubic Axial-symmetric symmetry. The influence that elastic anisotropy has on the fatigue response of the material is then quantified. This is accomplished through using the effective elastic stiffness tensor in the computation of the crack extension force, "
1172,"Multi-material structures take advantage of beneficial properties of different materials to achieve an increased level of functionality. In an effort to reduce the weight of vehicle components such as brake disk rotors, which are generally made of cast iron, light materials such as aluminum alloys may be used. These materials, however, may lead to unacceptable temperature levels. Alternatively, functionally graded structures may offer a significant decrease in weight without altering thermal performance. The design of such structures is not trivial and is the focus of this paper. The optimization combines a transient heat transfer finite element code with a genetic algorithm. This approach offers the possibility of finding a global optimum in a discrete design space, although this advantage is balanced by high computational expenses due to many finite element analyses. The goal is to design a brake disk rotor for minimum weight and optimal thermal behavior using two different materials. Knowing that computational time can quickly become prohibitively high, strategies, such as finite element grouping to reduce the number of design variables and local mesh refinement, must be employed to efficiently solve the design problem. This paper discussed the strengths and weaknesses of the proposed design method."
1173,"Simulation Based Engineering Science (SBES) is an evolving interdisciplinary research area rooted in the methods for modeling multiscale, multi-physics events. The objective in SBES is to develop methodologies that are foundational to designing multiscale systems by accounting for phenomena at multiple scales of lengths and time. Some of the key challenges faced in SBES include lack of methods for bridging various time and length scales, management of models and uncertainty associated with them, management of huge amount and variety of information, and methods for efficient decision making based on the available models. Although efforts have been made to address some of these challenges for individual application domains, a domain independent framework for addressing these challenges associated with multiscale problems is not currently available in the literature. In this paper, we make a clear distinction between multiscale modeling and multiscale design. "
1174,"In this paper, we propose an Inductive Design Exploration Method (IDEM) which can be used to design materials and products concurrently and systematically. IDEM facilitates hierarchical materials and product design synthesis, which includes multi-scale material structure and product analysis chains, and uncertainty in models and its propagation through the chains. In this method, we sequentially identify a ranged set of feasible specifications, instead of an optimal point solution in each segment of a hierarchical design process. The feasible spaces are searched from top-level design requirements to product and materials specifications taking into account propagated uncertainty. Strategies for parallelizing computations and achieving a robust solution for uncertainty in models are also addressed. The method is demonstrated with a simple example of designing a clay-filled polyethylene cantilever beam."
1175,"This paper describes a generalized Cahn-Hilliard model for the topology optimization of multi-material structure. Unlike the traditional Cahn-Hilliard model applied to spinodal separation which only has bulk energy and interface energy, the generalized model couples the elastic energy into the total free energy. As a result, the morphology of the small phase domain during phase separation and grain coarsening process is not random islands and zigzag web-like objects but regular truss structure. Although disturbed by elastic energy, the Cahn-Hilliard system still keeps its two most important properties: energy dissipation and mass conservation. Therefore, it is unnecessary to compute the Lagrange multipliers for the volume constraints and make great effort to minimize the elastic energy for the optimization of structural topology. Furthermore, this model also makes the simple interpolation of stiffness tensors reasonable for multi-material structure in real simulation. To resolve these fourth-order nonlinear parabolic Cahn-Hilliard equations coupled with elastic energy, we developed a powerful mutigrid algorithm. Finally, we demonstrate that this new method is effective in optimizing the topology of multi-material structure through several 2-D examples."
1176,"Towards the goal of developing a new methodology for control of vibration in flexible structures, this paper introduces the concept of modal disparity and addresses the topology optimization problem for maximizing the disparity. The modal disparity in a structure is generated by the application of forces that vary the stiffness of the structure and a topology optimization problem determines the best locations for application of these forces. When the forces are switched on and off and, as a result, the structure is switched between two stiffness states, modal disparity results in vibration energy being transferred from a set of uncontrolled modes to a set of controlled modes. This allows the vibration of the structure to be completely attenuated by removing energy from the small set of controlled modes. Simulation results are presented to demonstrate control of vibration in two truss-like structures exploiting modal disparity."
1177,"Formulations for the automatic synthesis of two-dimensional bistable, compliant periodic structures are presented, based on standard methods for topology optimization. The design space is parameterized using non-linear beam elements and a ground structure approach. A performance criterion is suggested, based on characteristics of the load-deformation curve of the compliant structure. A genetic algorithm is used to find candidate solutions. A numerical implementation of this methodology is discussed and illustrated using a simple example."
1178,"This paper presents a new method for designing vehicle structures for crashworthiness using surrogate models and a genetic algorithm. Inspired by the classifier ensemble approaches in pattern recognition, the method estimates the crash performance of a candidate design based on an ensemble of surrogate models constructed from the different sets of samples of finite element analyses. Multiple sub-populations of candidate designs are evolved, in a co-evolutionary fashion, to minimize the different aggregates of the outputs of the surrogate models in the ensemble, as well as the raw output of each surrogate. With the same sample size of finite element analyses, it is expected the method can provide wider ranges potentially high-performance designs than the conventional methods that employ a single surrogate model, by effectively compensating the errors associated with individual surrogate models. Two case studies on simplified and full vehicle models subject to full-overlap frontal crash conditions are presented for demonstration."
1179,"This paper discuses a new topology optimization method using frame elements for the design of mechanical structures at the conceptual design phase. The optimal configurations are determined by maximizing multiple eigen-frequencies in order to obtain the most stable structures for dynamic problems. The optimization problem is formulated using frame elements having ellipsoidal cross-sections, as the simplest case. Construction of the optimization procedure is based on CONLIN and the complementary strain energy concept. Finally, several examples are presented to confirm that the proposed method is useful for the topology optimization method discussed here."
1180,"With the advent of robots in modern-day manufacturing workcells, optimization of robotic workcell layout (RWL) is crucial in ensuring the minimization of the production cycle time. Although RWL share many aspects with the well-known facility layout problem (FLP), there are features which set the RWL apart. However, the common features which they share enable approaches in FLP to be ported over to RWL. One heuristic gaining popularity is genetic algorithm (GA). In this paper, we present a GA approach to optimizing RWL by using the distance covered by the robot arm as a means of gauging the degree of optimization. The approach is constructive: the different stations within the workcell are placed one by one in the development of the layout. The placement method adopted is based on the spiral placement method first broached by Islier (1998). The algorithm was implemented in Visual C++ and a case study assessed its performance."
1181,"A mechanism is a device transmits motion in a predetermined manner in order to accomplish specific objectives. Mechanism design can be divided into three steps: type synthesis, number synthesis and dimensional synthesis, where the number synthesis is also called topological synthesis. In this paper, a new approach for topological synthesis and dimensional synthesis of linkage mechanism design with pin joints is presented. This approach is based on the discrete element approach which always provides clear definitions of number of linkages and joints. In order to extend its applications beyond the compliant mechanism, a novel analysis method based on the principle of minimum potential energy for linkage topology optimization is employed. Unlike the traditional FEM based approaches, this novel analysis method can be applied to multiple joint linkage designs directly. Genetic Algorithm is chosen as the optimizer. Finally, a few design examples from the proposed method are presented."
1182,"In this paper, topology optimization of structure subject to design-dependent loads is studied. The position and direction of the design-dependent loads will change as the shape and topology of structure changes during optimization iteration. A potential function is introduced to locate the surface boundary. Design sensitivity analysis is derived. Examples from the proposed method are presented."
1183,"The field of new product development has a number of difficult challenges with which it must contend: shortened production time, greater market share demand, and geographically dispersed teams. Several software systems have been developed to ease these challenges. A representative cross-section of work in the fields of document management, project management, product lifecycle management, and conceptual and family design is examined, including past and current academic work and commercially available software. The scope and features of these projects are examined and compared on a software taxonomy. The potential application of these systems to product families is discussed throughout."
1184,"As the marketplace is changing so rapidly, it becomes a key issue for companies to best meet customers’ diverse demands by providing a variety of products in a cost-effective and timely manner. In the meantime, an increasing variety of capability and functionality of products has made it more difficult for companies that develop only one product at a time to maintain competitive production costs and reclaim market share. By designing a product family based on a robust product platform, overall production cost can be more competitive than competitors selling one product at a time while delivering highly differentiated products. In order to design cost-effective product families and product platforms, we are developing a production cost estimation framework in which relevant costs are collected, estimated, and analyzed. Since the framework is quite broad, this paper is dedicated to refining the estimation framework in a practical way by developing an activity-based costing (ABC) system in which activity costs are mapped to individual parts in the product family, which is called cost modularization, and the activity costs affected by product family design decisions are reconstructed to make the costs relevant to these decisions. A case study involving a family of power tools is used to demonstrate the proposed use of the ABC system."
1185,"In this paper we propose a framework based on Formal Concept Analysis (FCA) that can be applied systematically to (1) visualize a product family (PF) and (2) improve commonality in the product family. Within this framework, the components of a PF are represented as a complete lattice structure using FCA. A Hasse diagram composed of the lattice structure graphically represents all the products, components, and the relationships between products and components in the PF. The lattice structure is then analyzed to identify prospective components to redesign to improve commonality. We propose two approaches as part of this PF redesign methodology: (1) "
1186,"This paper presents our continued research efforts towards developing a decomposition-based solution approach for "
1187,"We have developed a decomposition-based rapid redesign methodology for large, complex computational redesign problems. While the overall methodology consists of two general steps: "
1188,"Many companies are using product families and platform-based product development to reduce costs and time-to-market while increasing product variety and customization. Multi-objective optimization is increasingly becoming a powerful tool to support product platform and product family design. In this paper, a genetic algorithm-based optimization method for product family design is suggested, and its application is demonstrated using a family of universal electric motors. Using an appropriate representation for the design variables and by adopting a suitable formulation for the genetic algorithm, a one-stage approach for product family design can be realized that requires no "
1189,"Many of today’s manufacturing companies are using platform-based product development to realize families of products with sufficient variety to meet customers’ demands while keeping costs relatively low. The challenge when designing or redesigning a product family is in resolving the tradeoff between product commonality and distinctiveness. Several methodologies have been proposed to redesign existing product families; however, a problem with most of these methods is that they require a considerable amount of information that is not often readily available, and hence their use has been limited. In this research, we propose a methodology to help designers during product family redesign. This methodology is based on the use of a genetic algorithm and commonality indices - metrics to assess the level of commonality within a product family. Unlike most other research in which the redesign of a product family is the result of many human computations, the proposed methodology reduces human intervention and improves accuracy, repeatability, and robustness of the results. Moreover, it is based on data that is relatively easy to acquire. As an example, a family of computer mice is analyzed using the Product Line Commonality Index. Recommendations are given at the "
1190,"Proper utilization of available assembly resources can reduce the development time and cost for platforms and new product family members. This paper presents a method to explicitly consider existing assembly plant configuration and resources during selection of assembly process for new product family members. In order to perform the trade-off studies, first constraints on the assembly sequence are identified and used to generate the feasible assembly sequence space, which is combinatorial in nature and is enumerated using recursive functions. The assembly sequence design spaces and representation of effects of constraints on these spaces to explicitly represent feasible regions, and efficiently enumerate designs within this space are investigated. A method that stops recursive growth based on constraints and minimization of assembly plant modification cost is used for efficient search of the feasible assembly sequence space. The assembly sequence modification cost is estimated by dividing assembly plant modification tasks into smaller activities and determining cost associated with each activity. Application of the Assembly Resource Utilization Design Module (ARUDM) to determine assembly sequence while increasing utilization of existing assembly plants is demonstrated using a coffeemaker family."
1191,"The Product Platform Constructal Theory Method (PPCTM) provides designers with an approach for synthesizing multiple modes of managing customization in the development of product platforms. An assumption underlying PPCTM is that the extent of the market space is known and is fixed. In this paper, we introduce PPCTM-RCM (Robust to Changes in Market) that facilitates designing product platforms when the extent of the market space is expected to change. The PPCTM-RCM is illustrated via example problem, namely, the design of a product platform for a line of customizable pressure vessels. Our focus in this paper is on highlighting features of the method rather than results "
1192,"Repository based applications for portfolio design offer the potential for leveraging archived design data with computational searches. Toward the development of such search tools, we present a representation for product portfolios that is an extension of an existing Group Technology (GT) coding scheme. Relevance to portfolio design is treated with a case study example of a hand held grinder design. Results of this work provide a numerical coding representation that captures function, form, material and manufacturing data for systems. This extends the current GT line work by combining these four types of design data and clarifying the use of the functional basis in a GT code. The results serve as a useful starting point for the development of portfolio design algorithms, such as genetic algorithms, that account for this combination of design information."
1193,"Product platform formation has long been considered as an effective method to meet challenges set forth by mass customization. To cater to the changes in customer need driven functional requirements and technological advancements, product platforms have to be robust for a given planning horizon from the manufacturer’s point of view. To date, most of the product platform research is directed towards developing approaches that maximize the usage of common "
1194,"A new product configuration design method based on extensible product family is presented in this paper. The extensible product family is a multi-layered model with extensible function, extensible principle, and extensible structure. Treating extensible element as a basic unit, the model can be used to associate extensible parts with reusable factors in the range from 0 to 1. The principle of configuration method has been implemented in software. Complicated rule editing and modification are handled by Ch, an embeddable C/C++ interpreter. Designers can establish and edit the configuration rules including formulas dynamically. According to the client requirements and nearest-neighbor matching, the results of the designed configuration can be obtained automatically. Furthermore, the multi-dimensional information about parameters and reusable factors can be displayed and analyzed graphically. If the client requirements or configuration rules are changed, the system can be easily re-configured to obtain designed results based on the new configuration quickly. The system has been successfully deployed and used to design complicated products with a large number of configurations and different specifications such as elevators, machine tools and smut-collectors."
1195,"In nowadays’ changing manufacturing environment, designing product families based on product platforms has been well accepted as an effective means to fulfill product customization. The current production practice and academic research of platform based product development mostly focus on the design domain, whereas limited attention is paid to how production can take advantage of product families for realizing economy of scale through enormous repetitions. This paper puts forward a concept of process platforms, based on which an efficient and cost saving production configuration for new members of a product family can be achieved. A process platform implies three aspects, including generic representation, generic structures and generic planning. The issues and rationale of production configuration based on a process platform are presented. A multilevel system of nested colored object-oriented Petri Nets with changeable structures is proposed to model the configuration of production processes. To construct a process platform from existing process data, a data mining approach based on text mining and tree matching is introduced to identify the generic process structure of a process family. An industrial example of high variety production of vibration motors for hand phones is also reported."
1196,"One important source of variance in the performance and success of products designed for use by people is the people themselves. In many cases, the acceptability of the design is affected more by the variance in the human users than by the variance attributable to the hardware from which the product is constructed. Consequently, optimization of products used by people may benefit from consideration of human variance through robust design methodologies. We propose that design under uncertainty methodologies can be utilized to generate designs that are robust to variance among users, including differences in age, physical size, strength, and cognitive capability. Including human variance as an inherent part of the product optimization process will improve the overall performance of the product (be it comfort, maintainability, cognitive performance, or other metrics of interest) and could lead to products that are more accessible to broader populations, less expensive, and safer. A case study involving the layout of the interior of a heavy truck cab is presented, focusing on simultaneous placement of the seat and steering wheel adjustment ranges. Tradeoffs between adjustability/cost, driver accommodation, and safety are explored under this paradigm."
1197,"Optimal design problems with probabilistic constraints, often referred to as Reliability-Based Design Optimization (RBDO) problems, have been the subject of extensive recent studies. Solution methods to date have focused more on improving efficiency rather than accuracy and the global convergence behavior of the solution. A new strategy utilizing an adaptive sequential linear programming (SLP) algorithm is proposed as a promising approach to balance accuracy, efficiency, and convergence. The strategy transforms the nonlinear probabilistic constraints into equivalent deterministic ones using both first order and second order approximations, and applies a filter-based SLP algorithm to reach the optimum. Simple numerical examples show promise for increased accuracy without sacrificing efficiency."
1198,"This work proposes a novel concept of failure surface frontier (FSF), which is a hyper-surface consisting of the set of the non-dominated failure points on the limit states of a given failure region. FSF better represents the limit state functions for reliability assessment than conventional linear or quadratic approximations on the most probable point (MPP). Assumptions, definitions, and benefits of FSF are discussed first in detail. Then, a discriminative sampling based algorithm was proposed to identify FSF, from which reliability is assessed. Test results on well known problems show that reliability can be accurately estimated with high efficiency. The algorithm is also effective for problems of multiple failure regions, multiple most probable points (MPP), or failure regions of extremely small probability."
1199,"Engineering design problems frequently involve a mix of both continuous and discrete uncertainties. However, most methods in the literature deal with either continuous or discrete uncertainties, but not both. In particular, no method has yet addressed uncertainty for categorically discrete variables or parameters. This article develops an efficient optimization method for problems involving mixed continuous-discrete uncertainties. The method reduces the number of function evaluations performed by systematically filtering the discrete factorials used for estimating reliability based on their importance. This importance is assessed using the spatial distance from the feasible boundary and the probability of the discrete components. The method is demonstrated in examples and is shown to be very efficient with only small errors."
1200,"Uncertainty analysis, which assesses the impact of the uncertainty of input variables on responses, is an indispensable component in engineering design under uncertainty such as reliability based design and robust design. However, uncertainty analysis is not an affordable computational burden in many engineering problems. In this paper, a new uncertainty analysis method is proposed with the purpose of accurately and efficiently estimating the cumulative distribution function (CDF), probability density function (PDF) and statistical moments of a response given the distributions of input variables. The bivariate dimension-reduction method and numerical integration are used to calculate the moments of the response; then Saddlepoint Approximations are employed to estimate the CDF and PDF of the response. The proposed method requires neither the derivatives of the response nor the search of the Most Probable Point (MPP), which is needed in the commonly used First - or Second - Order Reliability Methods (FORM or SORM). The efficiency and accuracy of the proposed method is illustrated with three example problems. The method is more accurate and efficient for estimating the full range of the distribution of a response than FORM and SORM. This method provides results as accurate as Monte Carlo simulation, with a significantly reduced computational effort."
1201,"Early in the engineering design cycle, it is difficult to quantify product reliability or compliance to performance targets due to insufficient data or information to model uncertainties. Probability theory can not be therefore, used. Design decisions are usually, based on fuzzy information that is vague, imprecise qualitative, linguistic or incomplete. Recently, evidence theory has been proposed to handle uncertainty with limited information as an alternative to probability theory. In this paper, a computationally efficient design optimization method is proposed based on evidence theory, which can handle a mixture of epistemic and random uncertainties. It quickly identifies the vicinity of the optimal point and the active constraints by moving a hyper-ellipse in the original design space, using a reliability-based design optimization (RBDO) algorithm. Subsequently, a derivative-free optimizer calculates the evidence-based optimum, starting from the close-by RBDO optimum, considering only the identified active constraints. The computational cost is kept low by first moving to the vicinity of the optimum quickly and subsequently using local surrogate models of the active constraints only. Two examples demonstrate the proposed evidence-based design optimization method."
1202,"The Sequential Optimization and Reliability Assessment (SORA) method is a single loop method containing a sequence of cycles of decoupled deterministic optimization and reliability assessment for improving the efficiency of probabilistic optimization. However, the original SORA method as well as some other existing single loop methods is not efficient for solving problems with changing variance. In this paper, to enhance the SORA method, three formulations are proposed by taking the effect of changing variance into account. These formulations are distinguished by the different strategies of Inverse Most Probable Point (IMPP) approximation. Mathematical examples and a pressure vessel design problem are used to test and compare the effectiveness of the proposed formulations. The “Direct Linear Estimation Formulation” is shown to be the most effective and efficient approach for dealing with problems with changing variance. The gained insight can be extended and utilized to other optimization strategies that require MPP or IMPP estimations."
1203,"Analytical target cascading (ATC) is a methodology for hierarchical multilevel system design optimization. In previous work, the deterministic ATC formulation was extended to account for uncertainties using a probabilistic approach. Random quantities were represented by their expected values, which were required to match among subproblems to ensure design consistency. In this work, the probabilistic formulation is augmented to allow introduction and matching of additional probabilistic characteristics. Applying robust design principles, a particular probabilistic analytic target cascading (PATC) formulation is proposed by matching the first two moments of random quantities. Several implementation issues are addressed, including representation of probabilistic design targets, matching interrelated responses and linking variables under uncertainty, and coordination strategies for multilevel optimization. Analytical and simulation-based optimal design examples are used to illustrate the new PATC formulation. Design consistency is achieved by matching the first two moments of interrelated responses and linking variables. The effectiveness of the approach is demonstrated by comparing PATC results to those obtained using a probabilistic all-in-one (PAIO) formulation."
1204,"Current design decisions must be made while considering uncertainty in both models and inputs to the design. In most cases this uncertainty is ignored in the hope that it is not important to the decision making process. This paper presents a methodology for managing uncertainty during system-level conceptual design of complex multidisciplinary systems. The methodology is based upon quantifying the information available in computationally expensive subsystem models with more computationally efficient kriging models. By using kriging models, the computational expense of a Monte Carlo simulation to assess the impact of the sources of uncertainty on system-level performance parameters becomes tractable. The use of a kriging model as an approximation to an original computer model introduces model uncertainty, which is included as part of the methodology. The methodology is demonstrated as a decision making tool for the design of a satellite system."
1205,"Mathematical optimization plays an important role in engineering design, leading to greatly improved performance. Deterministic optimization however, may result in undesired choices because it neglects uncertainty. Reliability-based design optimization (RBDO) and robust design can improve optimization by considering uncertainty. This paper proposes an efficient design optimization method under uncertainty, which simultaneously considers reliability and robustness. A mean performance is traded-off against robustness for a given reliability level of all performance targets. This results in a probabilistic multi-objective optimization problem. Variation is expressed in terms of a percentile difference, which is efficiently computed using the Advanced Mean Value (AMV) method. A preference aggregation method converts the multi-objective problem to a single-objective problem, which is then solved using an RBDO approach. Indifference points are used to select the best solution without calculating the entire Pareto frontier. Examples illustrate the concepts and demonstrate their applicability."
1206,"This paper presents an efficient genetic algorithm based methodology for robust design that produces compressor fan blades tolerant against erosion. A novel geometry modeling method is employed to create eroded compressor fan blade sections. A multigrid Reynolds-Averaged Navier Stokes (RANS) solver HYDRA with Spalart Allmaras turbulence model is used for CFD simulations to calculate the pressure losses. This is used in conjunction with Design of Experiment techniques to create Gaussian stochastic process surrogate models to predict the mean and variance of the performance. The Non-dominated Sorting Genetic Algorithm (NSGA-II) is employed for the multiobjective optimization to find the global Pareto-optimal front. This enables the designer to trade off between mean and variance of performance to propose robust designs."
1207,"NASA’s space exploration vehicles, like any other complex engineering system, are susceptible to failure and ultimately loss of mission. Researchers, therefore, have devised a variety of quantitative and qualitative techniques to mitigate risk and uncertainty associated with such low-volume high-cost missions. These techniques are often adopted and implemented by various NASA centers in the form of risk management tools, procedures, or guidelines. Most of these techniques, however, aim at the later stages of the design process or during the operational phase of the mission and therefore, are not applicable to the early stages of design. In particular, since the early conceptual design is often conducted by concurrent engineering teams (and sometimes in distributed environments), most risk management methods cannot effectively capture different types of failure in both subsystem and system levels. The current risk management practice in such environments is mostly ad-hoc and based on asking “what can go wrong?” from the team members. As such, this paper presents a new approach to risk management during the initial phases of concurrent and distributed engineering design. The proposed approach, hereafter referred to as Risk and Uncertainty Based Integrated Concurrent Design (or RUBIC-Design), provides a solid rigor for using functional failure data to guide the design process throughout the design cycle. The new approach is based on the functional model of space exploration systems (or any other mission-critical engineering system for that matter) and has the capability of adjusting in real-time as the overall system evolves throughout the design process. The application of the proposed approach to both single-subsystem and multi-subsystem designs is demonstrated using a satellite reaction wheel example."
1208,"The effect of uncertainty reduction measures on the weight of laminates for cryogenic temperatures is investigated. The uncertainties in the problem are classified as error and variability. Probabilistic design is carried out to analyze the effect of reducing the uncertainty on the weight. For demonstration, variability reduction takes the form of quality control, while error is reduced by including the effect of chemical shrinkage in the analysis. It is found that the use of only error control leads to 12% weight reduction, the use of only quality control leads to 20% weight savings and the use of error and variability control measures together reduces the weight by 37%. In addition, the paper also investigates how to improve the accuracy and efficiency of probability of failure calculations (performed using Monte Carlo simulation technique). Approximating the cumulative distribution functions for strains is shown to lead to more accurate probability of failure estimations than the use of response surface approximations for strains."
1209,"We present a deterministic, non-gradient based approach that uses robustness measures for robust optimization in multi-objective problems where uncontrollable parameters variations cause variation in the objective and constraint values. The approach is applicable for cases with discontinuous objective and constraint functions, and can be used for objective or feasibility robust optimization, or both together. In our approach, the parameter tolerance region maps into sensitivity regions in the objective and constraint spaces. The robustness measures are indices calculated, using an optimizer, from the sizes of the acceptable objective and constraint variation regions and from worst-case estimates of the sensitivity regions’ sizes, resulting in an outer-inner structure. Two examples provide comparisons of the new approach with a similar published approach that is applicable only with continuous functions. Both approaches work well with continuous functions. For discontinuous functions the new approach gives solutions near the nominal Pareto front; the earlier approach does not."
1210,"Since decision-making at the conceptual design stage critically affects final design solutions at the detailed design stage, conceptual design support techniques are practically mandatory if the most efficient realization of optimal designs is desired. Topology optimization methods using discrete elements such as frame elements enable a useful understanding of the underlying mechanics principles of products, however the possibility of changing prior assumptions concerning utilization environments exists since the detailed design process starts after the completion of conceptual design decision-making. In order to avoid product performance reductions due to such later-stage environmental changes, this paper discusses a reliability-based topology optimization method that can secure specified design goals even in the face of environmental factor uncertainty. This method can optimize mechanical structures with respect to two principal characteristics, namely structural stiffness and eigen-frequency. Several examples are provided to illustrate the utility of the method presented here for mechanical design engineers."
1211,"In last two decades, significant attentions have been paid to develop design optimization methodologies under various uncertainties: reliability-based design optimization (RBDO), possibility-based design optimization (PBDO), etc. As a result, a variety of methods of uncertainty-based design optimization have been proposed and are mainly catagorized as: parallel-loop method, serial-loop method, and single-loop method. It has been reported that each method has its own strong and weak points. Thus, this paper attempts to understand various methods better, and proposes to develop an integrated framework for uncertainty-based design optimization. In short, the integrated design framework timely involves three phases (deterministic design optimization, parallel-loop method, single-loop method) to maximize numerical efficiency without losing computational stability and accuracy in the process of uncertainty-based design optimization. While the parallel-loop method maintains numerical stability well with a minimal computation, deterministic design optimization and single-loop method will improve numerical efficiency at the beginning and end of uncertainty-based design optimization. Thus, the proposed method is called adaptive-loop method. It will be shown that integrated framework using the proposed method is applicable for various design optimization methodologies under aleatory or epistemic uncertainties, such as RBDO, PBDO, etc. Examples are used to demonstrate the effectiveness of integrated framework using the adaptive-loop method in terms of numerical efficiency."
1212,"This paper demonstrates the effect of various safety measures used to design aircraft structures for damage tolerance. In addition, it sheds light on the effectiveness of measures like certification tests in improving structural safety. Typically, aircraft are designed with a safety factor of 2 on the service life in addition to other safety measures, such as conservative material properties. This paper demonstrates that small variation in material properties, loading and errors in modeling damage growth can produce large scatter in fatigue life, which means that quality control measures like certification tests are not very effective in reducing failure probability. However, it is shown that the use of machined cracks in certification can substantially increase the effectiveness of certification testing."
1213,"Several methods have been proposed for estimating transmitted variance to enable robust parameter design using computer models. This paper presents an alternative technique based on Gaussian quadrature which requires only 2"
1214,"The objective of the work presented in this paper is to enable production of large, complex components on rapid prototyping (RP) machines whose build volume is less than the size of the desired component. Such oversized parts can be produced as fabrications if a suitable subdivision can be generated. The methodology presented here creates a decomposition designed for both Rapid Prototyping (DFRP) and assembly (DFA). Any component can be subdivided by an array of orthogonal planes but the shapes resulting from such a brute force approach could have geometries that are difficult to produce accurately on many rapid prototyping systems. Typically, complications will arise when features have insufficient strength to withstand finishing processes or have a cross-section prone to distortion (e.g. warping) during the build process (e.g. thin sections and cusps). Consequently, the method proposed for partitioning considers potential manufacturing problems and modifies the boundaries of individual components where necessary. As part of the decomposition process, the system also generates complimentary male/female (i.e. matching protrusion/depression) assembly features at the interface between the component parts in order to improve the integrity of the final component."
1215,"Interactive design gives engineers the ability to modify the shape of a part and immediately see the changes in the part’s stress state. Virtual reality techniques are utilized to make the process more intuitive and collaborative. The results of a meshless stress analysis are superimposed on the original design. As the engineer modifies the design using subdivision volume free-form deformation, the stress state for the modified design is computed using a Taylor series approximation. When the designer requests a more accurate analysis, a stress re-analysis technique based on the pre-conditioned conjugate gradient method is used with parallel processing to quickly compute an accurate approximation of the stresses for the new design."
1216,"Prototyping plays an important role in industrial product designs. In this paper, for achieving a more intuitive and interactive prototyping, a selective clay milling center is introduced based on a synthesis of clay modeling, 3-Dimensional (3-D) scanning, robot machining and advanced geometric tools. In the system, the product shape design may start either from a physical hand-made clay model or a virtual Computer-Aided Design (CAD) model. Via 3-D scanning techniques, manual modifications of the clay model can be captured in the CAD form and in the meantime, geometric modifications of the CAD model can be fed back to the physical model by an efficient robot machining method, named selective clay milling. This design cycle is repeated until a satisfied prototype iterates. For a better control of the interactions between the manual modeling and robot milling, a 3-D scanning based calibration system has been developed in order to arbitrarily position the workpiece in the design process. With several experiments, the effectiveness of the proposed system is shown and the possible applications of the proposed system in industrial product design are described as well."
1217,"Even though building functional metal parts directly from CAD files has been the focus of many researches for many years, parts made by Layered Manufacturing (LM) have limited surface accuracy and long build time due to the sacrificial support structures. The Multi-Axis Laser Aided Manufacturing Process (LAMP) system improves build time by adding two more rotation axes to the system in order to reduce the support structures. The strategy to decompose the part model to sub-volumes or cells and the algorithm to arrange the deposition of those cells are discussed. The problems and questions of how much material should be deposited along the determined directions are also addressed in this paper."
1218,"This paper deals with the problem of automatic fairing of two-parameter B-Spline spherical and spatial motions. The concept of two-parameter freeform motions brings together the notion of the analytically determined two-parameter motions in Theoretical Kinematics and the concept of freeform surfaces in the field of Computer Aided Geometric Design (CAGD). A dual quaternion representation of spatial displacements is used and the problem of fairing two-parameter motions is studied as a surface fairing problem in the space of dual quaternions. By combining the latest results in surface fairing from the field of CAGD and computer aided synthesis of freeform rational motions, smoother (C3  continuous) two-parameter rational B-Spline motions are generated. The results presented in this paper are extensions of previous results on fine-tuning of one-parameter B-spline motions. The problem of motion smoothing has important applications in the Cartesian motion planning, camera motion synthesis, spatial navigation in visualization, and virtual reality systems. Several examples are presented to illustrate the effectiveness of the proposed method."
1219,"This paper addresses the problem of structural shape and topology optimization. A level set method is adopted as an alternative approach to the popular homogenization based methods. The paper focuses on four areas of discussion: (1) The level-set model of the structure’s shape is characterized as a region and global representation; the shape boundary is embedded in a higher-dimensional scalar function as its “iso-surface.” Changes of the shape and topology are governed by a partial differential equation (PDE). (2) The velocity vector of the Hamilton-Jacobi PDE is shown to be naturally related to the shape derivative from the classical shape variational analysis. Thus, the level set method provides a natural setting to combine the rigorous shape variations into the optimization process. Finally, the benefit and the advantages of the developed method are illustrated with several 2D examples that have been extensively used in the recent literature of topology optimization, especially in the homogenization based methods."
1220,"We present a new robust optimization method that ensures feasibility of an optimized design when there are uncontrollable variations in design parameters. This method is developed based on the notion of a sensitivity region, which is a measure of how far a feasible design is from the boundary of a feasible domain in the parameter variation space. As the design moves further inside the feasible domain, and thus becoming more feasibly robust, the sensitivity region becomes larger. Our method is not sampling-based so it does not require a presumed probability distribution as input and is efficient in terms of function evaluations. In addition, our method does not use gradient approximation and thus is applicable to problems having non-differentiable constraint functions and large parameter variations. As a demonstration, we applied our method to an engineering example, the design of a control valve actuator linkage. In this example, we show that the method is efficient and the optimum design obtained is robust."
1221,"Laser aided deposition quality largely depends on the powder stream structure below the nozzle. Modeling of the powder concentration distribution rarely relies on the numerical approach partially due to the complex phenomenon involved in the two-phase turbulence flow. In this paper, a numerical model is introduced to predict the particle-gas flow precisely and economically in order to meet the practical requirement for coaxial nozzle design optimizations. This model is able to quantitatively predict the powder stream concentration mode under different outer shielding gas directions and inner/outer gas velocity ratio. The numerical simulation results are compared with the experimental study using prototyped coaxial nozzles. The results are found to match. This study shows that the particle concentration mode is influenced significantly by the outer gas direction and gas flow settings."
1222,"The current practice in problem decomposition assumes that (1) design problems can be "
1223,"Feature retrieval is of great importance in shape modelling, in terms of supporting design reuse by obtaining reusable geometric entities. However, conventional techniques for feature retrieval are generally limited to the extraction of feature lines, curve segments, or surfaces, and the feature distortion imposed by feature interaction remains unconsidered. This paper investigates approaches for freeform feature retrieval by means of signal processing techniques. By treating features or regions of interest as surface signals, we employ digital filters to separate the feature signal from that of the domain surface, retrieving the “pure” feature from an existing shape model. Strategies for different model types are elaborated, for instance, the exact feature retrieval method designed for shape models with explicit data structure, such as B-Rep, or other accessible representations; and the signal filtering method for models with structured or unstructured data sets, such as that in mesh or point cloud models. Specifically, in the signal filtering method feature retrieval is implemented by the convolving operator in the frequency domain. By transforming the problem of shape decomposition from geometric extraction in the spatial domain to computation in the frequency domain, the proposed methods not only brings in significant computational efficiency, but also reduces the complexity of problem solving for feature retrieval. Provided examples show that the proposed approaches can achieve satisfactory results for simple geometries, whereas for sophisticated shapes guidelines for the design of dedicated filters are elaborated."
1224,"In assemblies involving compliant parts, springback and residual stress are caused when geometric variation in the assembly requires the parts be deformed to allow for assembly. A method is needed to predict the deformation due to the geometric variations. Researchers in the areas of Statistical Tolerance Analysis and Stochastic Finite Element Analysis have developed methods to account for statistical variation of parameters in a finite element model. A new statistical method uses an orthogonal polynomial-based covariance model to account for surface variation of the compliant parts over a wide range of wavelengths. The method has been demonstrated to match the accuracy of Monte Carlo simulation with a single calculation, and accurately model covariance data taken from real parts."
1225,"A knowledge-based requirements management tool is being developed for the automotive industry to aid in the specification of modular systems whose development is contracted out to the supply chain. The aim of this tool is to allow the original equipment manufacturer (OEM) and their system suppliers to have a shared conceptualization of the product requirements. This paper highlights some of the issues with the current paper-based specifications and the use of natural language to represent the requirements. The framework of the knowledge-based tool in order to address these issues through the application of ontology is described. A ‘product’ ontology for the specification of the seating assembly for a car is presented. The ontology encapsulates the required functionalities, design parameters, performance criteria, structure and geometry. This is the result of the consensual convergence in the vocabulary, definitions and attributes that describe the product requirements."
1226,"The paper presents a new model for collaborative design. The model is analogous to electrical circuits with current (rate of design artifact synthesis and analysis), voltage (knowledge that drives the design process), and resistance (barriers to the exchange of design information). The resistances are identified from a collaborative design taxonomy. This model is illustrated through a simple example. Extensions and an assessment of the model are provided."
1228,"In manufacturing industries, ultrasonic welding has established itself as one of the most effective techniques for fusing plastic assemblies due to its rapid performance and the absence of filler material. In this report, a thermoplastic polyurethane prototype of an orthopedics device that requires a hermetic seal is joined using ultrasonic welding. A robust design approach is used to study the manufacturing control factors that influence the process. The welding process factors and their interactions are used to characterize the resulting seal. Burst testing is used to assess weld strength. Optical microscopy in addition to SEM images, are used for a qualitative evaluation of the welded joint. Optimum process parameter settings from the robust design study deliver a strong and leak-proof weld."
1229,"This paper presents a new method for robot spray painting simulation. First, the authors introduce a method for rigid body motion interpolation proposed by Zefran for generating smooth trajectories of a spray gun. Second, the authors propose a method to solve inverse kinematics problems using Lie algebra for computing the motion of the robot manipulator from the spray gun’s motion. Third, the authors propose a method for calculation of color depth of the workpiece painted with the spray gun. Finally, in order to evaluate the usefulness of the three proposed methods, the authors develop a simulation system for robot spray painting."
1230,"As has been demonstrated the “microfactory”, which is a miniature manufacturing system proposed by the author and his research group, small machine tools that are comparable in size to their target products lead to large reductions in energy consumption and occupied space. They also increase the flexibility of system reconfiguration because of their low weight and small size. Although it had been thought that micro machine tools might not have sufficient metal cutting capability, experiments showed that were capable of micro mechanical fabrication. However, the design of miniature machine tools has not been fully optimized. For example, the design target of the first prototype, a performable miniature machine (“Micro lathe”), was to make the overall size as small as possible. The author proposed a design evaluation method to roughly estimate machine tool performances during its early design stage. In this paper, the above-mentioned design tool is applied to find suitable miniaturizing strategies. By applying the design tool to the miniaturization of machine tools, it is possible to determine which of the design candidates have the best theoretical performance and which of the local error factors would significantly affect machine performance. From the results of calculation, the tool can clarify the difference of effect of error sources on performances between normal machine tools and miniature machine tools. This leads to some suggestions regarding structures, sizes and suitable machine components. Design guidelines for miniature machine tools can be obtained from the information."
1231,"It is well known that one can exploit symmetry to speed-up engineering analysis and improve accuracy, at the same time. Not surprisingly, most CAE systems have standard ‘provisions’ for exploiting symmetry. However, these provisions are inadequate in that they needlessly burden the design engineer with time consuming and error-prone tasks of symmetry detection, symmetry cell construction and reformulation. In this paper, we propose and discuss an automated methodology for symmetry exploitation. First, we briefly review the theory of point symmetry groups that symmetry exploitation rests on. We then address symmetry detection and ‘symmetry cell’ construction. We then address an important concept of boundary mapping of symmetry cells, and relate it to the irreducible representations of point symmetry groups. By formalizing these concepts, we show how automated symmetry exploitation can be achieved, and discuss an implementation of the proposed work within the FEMLAB CAE environment."
1232,"Multistable equilibrium (MSE) systems are a type of adaptable system that can have multiple mechanical configurations requiring no power to maintain the stable configurations. Thus, power is only needed to move among the stable states, and each stable configuration represents a level of adaptability. Since stable equilibrium configurations can be defined by potential energy minima, we base the design of MSE systems on shaping the potential energy curve at desired equilibrium configurations. This view allows one to construct a performance space defined by how well candidate systems meet a desired potential energy curve. By using a Monte Carlo mapping to link the performance space to the design space in tandem with stochastic optimization methods, the designer determines whether or not a certain system topology can be designed as a MSE system. Qualitative and quantitative mapping procedures enable the designer to decide whether or not the desired design lies near the center or periphery of a performance space. This dictates how the optimization is to be executed which in turn informs the designer as to whether or not a feasible limit in the system performance has indeed been reached."
1233,"We propose an optimization method for a semi-active shock absorber for use in aircraft landing gear using Carroll’s FORTRAN Genetic Algorithm (GA) Driver. This method is compared with Powell’s conjugate direction method, a nonlinear programming (NP) approach, which uses not gradients, but only function values. In these optimizations, we handle variations in the maximum vertical acceleration of an aircraft during landing caused by the variation of the aircraft mass due to variations in the number of passengers and the amounts of cargo and fuel. The maximum vertical acceleration of an aircraft is set as an objective function to be minimized. Design variables searched in the first step of this optimization are discrete orifice areas formed by the outer surface of a hollow metering pin and a hole in the semi-active shock absorber. The design variable searched in the second step is an orifice area which is controlled based on the mass variation. For the GA runs, the ratio of the total number of optimum and near-optimum solutions to the total number of runs was greater than that for the NP runs. In addition, for the total GA runs, the total number of function evaluations per total number of optimum and near-optimum solutions was greater than that for the total NP runs. The optimum semi-active shock absorber is compared to the optimum passive shock absorber with respect to the variation of the acceleration of the aircraft mass. The ratio of maximum acceleration in the semi-active shock absorber to that in the passive shock absorber is 0.79 when the mass ratio is 0.65 maximum mass and is 0.58 when the mass ratio is 0.31 maximum mass."
1234,"A Concurrent Parameter Design (CPD) based on constraint network method is proposed. Since concurrent design emphasizes solving downstream problems early in the design period, the constraint network, which is used to collect the constraints from the multidisciplinary teams, uses the consistency algorithm to verify the design process early in the process and to assist the designers in determining design variables to reduce the multidisciplinary iterations in concurrent design. A consistency algorithm which is designed using interval arithmetic to refine the intervals is used to obtain a feasible solution space for the designers. Then, the designers choose the parameters in the solution space using their expertise. The constraint network can reduce the modification iterations among the multidisciplinary teams during the concurrent design. The quantitative effect of the downstream constraints can be analyzed before determining the design parameters and potential conflicts can be predicted. A parameter design example about Bogie of Railway Rolling Stock is given to show the validity of the method."
1236,"A quick look to the literature related to “Design” points out the fact that traditional fields are covered by a huge quantity of concepts and sub-topics dealing with various research fields. Some of these fields and related concepts are emerging and others are disappearing. The last two decades provided a large variations in the covered fields—probably because of the computers capacity and the knowledge sharing using the internet- We are convinced that is time now for a global understanding of the evolution of the related preoccupations in order to start a new phase that deals with the next guidelines issues in design. Many researchers and gurus have proposed evolution schemes from an epistemological point of view. However, we believe that we still need a pragmatic analysis of what have been done and what is going on now in the various publications that deals with design, conferences and journals. To do so, we present in this paper an approach that lies on the systematic analysis of the International Design Engineering Technical Conference (IDETC-ASME), the Design Conference, the Integrated Design and Manufacturing in Mechanical Engineering (IDMME) and the International Conference in Engineering Design (ICED). We also consider the major reviews and journals related to design such as Computer-Aided Design, Journal of mechanical design and many others. Our proposition, presents a global overview that shows the time-evolution of the topics, the density production, the emerging areas and concepts. We show then that some topics remain with a great interest and other are in a decreasing period. A specific study of what is going on within the Design Automation Conference is provided and an open discussion is started."
1237,"Superconducting niobium cavities are important components of linear accelerators. Buffered chemical polishing (bcp) on the inner surface of the cavity is a standard procedure to improve its performance. The quality of bcp, however, has not been optimized well in terms of the uniformity of surface smoothness. A finite element computational fluid dynamics (cfd) model was developed to simulate the chemical etching process inside the cavity. The analysis confirmed the observation of other researchers that the sections closer to the axis of the cavity received more etching than other regions. A baffle was used by lanl personnel to direct the flow of the etching fluid toward the walls of the cavity. A new baffle design was tined using optimization techniques. The redesigned baffle significantly improves the performance of the etching process. To verify these results an experimental setup for flow visualization was created. The setup consists of a high speed, high resolution ccd camera. The camera is positioned by a computer-controlled traversing mechanism. A dye injecting arrangement is used for tracking the fluid path. Experimental results are in general agreement with computational findings."
1238,"Weighting coefficients are used in Analytical Target Cascading (ATC) at each element of the hierarchy to express the relative importance of matching targets passed from the parent element and maintaining consistency of linking variables and consistency with designs achieved by subsystem child elements. Proper selection of weight values is crucial when the top level targets are unattainable, for example when “stretch” targets are used. In this case, strict design consistency cannot be achieved with finite weights; however, it is possible to achieve arbitrarily small inconsistencies. This article presents an iterative method for finding weighting coefficients that achieve solutions within user-specified inconsistency tolerances and demonstrates its effectiveness with several examples. The method also led to reduced computational time in the demonstration examples."
1239,"Today’s highly competitive and global marketplace is redefining the way companies do business: many companies are being faced with the challenge of providing as much variety as possible for the market with as little variety as possible between products. In order to achieve this, product families have been developed, allowing the realization of a sufficient variety of products to meet the customers’ demands while keeping costs relatively low. The challenge when designing a family of products is in resolving the tradeoff between product commonality and distinctiveness: if commonality is too high, products lack distinctiveness, and their individual performance is not optimized; on the other hand, if commonality is too low, manufacturing costs will increase dramatically. Toward this end, several commonality indices have been proposed to assess the amount of commonality within a product family. In this paper, we compare and contrast six of the commonality indices from the literature based on their ease of data collection, repeatability and consistency. Eight families of products are dissected and analyzed, and the commonality of each product family is computed using each commonality index. The results are then analyzed and compared, and recommendations are given on their usefulness for product family design. This study lays a foundation for understanding the relationship between different platform leveraging strategies and the resulting degree of commonality within a product family."
1240,"The importance of design robustness and reliability is a well-established notion and practice in today’s industry. Manufacturing companies strive to achieve Six-Sigma quality measures. While Virtual Prototyping is a key-factor in accelerating the product development process while reducing development costs, it has not contributed in the quest for improved product reliability and robustness performance since it is based on deterministic approaches. This paper provides a systematic approach to design for six-sigma simultaneously addressing variability and uncertainty present in real life on most design parameters. An example from the automotive industry illustrates the methodologies."
1241,"Three families of methods coexist for managing uncertainty (i.e. representing and propagating) of the product data during the preliminary design stages, namely: fuzzy methods, probabilistic methods and Constraint Programming (CP) methods. CP methods over reals are, up to now, the least frequently used approaches but they are worth being studied further for a use in design engineering thanks to a number of satisfactory properties and to recent significant advances. They may be roughly considered as a collection of methods that are sophisticated evolutions of interval analysis. The objective of this paper is to assess four of these major methods (namely the {"
1242,"This paper describes a procedure to optimize the front structure of a vehicle for improved performance in the leg impact portion of pedestrian safety regulations proposed by the European Enhanced Vehicle-Safety Committee (EEVC). The first step in this procedure was to perform a simulation of the EEVC leg impact test with detailed finite element models of the EEVC leg impactor and the baseline design of a vehicle front structure. Next, a simplified, parametric finite element model of the vehicle front structure was used with the leg impactor model to simulate the leg impact test, and the results were correlated to the detailed finite element model and the test results. The leg impact simulation with the parametric vehicle model was then incorporated into an optimization procedure developed within the optimization code ISIGHT. In this procedure the parameters that controlled the vehicle geometry and structural stiffness in the simplified model were altered by ISIGHT to improve performance in the leg impact test."
1243,"In this paper, we describe a new approach for computing the area of a mesh projected onto a plane. This approach utilizes the graphics hardware’s line/object intersection capability and a recursive subdivision strategy to achieve performance and precision control. This approach starts from digitizing the projection plane into a grid of rectangular elements. For each element the graphics engine is utilized to check whether projection lines passing through the nodes of the element intersect the object in the model space. If all lines intersect the object, the element is considered “inside” and its area will be accounted towards the final projection area. If none of the lines has an intersection, the element is considered “outside” and discarded. For those elements that lay along the boundary of the projected area (which means some of their lines intersect the model while others don’t) we subdivide them until they are sufficiently small and the given area tolerance is met. Heuristics are derived for deciding the initial grid resolution and the level of subdivisions needed to meet/exceed a given area tolerance. Implementation results are demonstrated and compared with a classic polygon-clipping approach."
1244,"Resources for development projects are often scarce in the real world. Generally, many projects are to be completed that rely on a common pool of resources. Besides resource constraints, there exists data dependency among tasks within each project. A genetic algorithm approach with one-point uniform crossover and a refresh operator is proposed to minimize the overall duration or makespan of multiple projects in a resource constrained multi project scheduling problem (RCMPSP) without violating inter-project resource constraints or intra-project precedence constraints. The proposed GA incorporates stochastic feedback or rework of tasks. It has the capability of capturing the local optimum for each generation and therefore ensuring a global best solution. The proposed Genetic Algorithm, with several variants of GA parameters is tested on sample scheduling problems with and without stochastic feedback. This algorithm demonstrates to provide a quick convergence to a global optimal solution and detect the most likely makespan range for parallel projects of tasks with stochastic feedback."
1245,"This paper presents an effective method for inverse dynamic modeling of a five-axis milling machine with parallel kinematic chains (PKM). For solving the inverse dynamics, the methodology of using the principle of virtual work is introduced, which corrects a theoretic error in formulating the dynamic equations of motions sound in previous literatures. A corresponding computational algorithm for solving the inverse dynamics of the parallel kinematic machine is given and two cases of motion trajectories are calculated to check the proposed method. The corrected dynamic modeling is robust and features higher computational efficiency than other dynamic modeling methods such as recursive Newton-Euler method or Lagrangian formulations. Using this dynamic modeling and simulation method, we can anticipate the dynamic behavior of the five-axis machine and develop a suitable algorithm for motion control and dynamic optimization."
1246,"This paper describes a software tool called DRed (the D esign R ationale ed itor), that allows engineering designers to record their design rationale (DR) at the time of its generation and deliberation. DRed is one of many proposed derivatives of the venerable IBIS concept, but by contrast with other tools of this type, practicing designers appear surprisingly willing to use it. DRed allows the issues addressed, options considered, and associated arguments for and against, to be captured graphically. The software, despite still being essentially a research prototype, is already in use on high profile design projects in an international aerospace company, including the presentation of results of design work to external customers. The paper compares DRed with other IBIS-derived software tools, to explain how it addresses problems that seem to have made them unsuitable for routine use by designers. In addition to the capture and presentation of the DR itself, the set of linked DR graphs can be used to provide a map of the contents of an electronic Design Folder, containing all the documents created by an individual or team during a design project. The structure of the knowledge model instantiated in such a Design Folder is described. By reprising a design case study published at the DTM 2003 conference, concerning the design of a Mobile Arm Support (MAS), the DRed knowledge model is compared with the previously proposed Design Data Model (DDM), to show how it addresses the shortcomings identified in the DDM. Finally the methodology and results of the preliminary evaluation of the use of DRed by aerospace designers are presented."
1247,"Building a smooth and well structured surface to fit unstructured 3-D data is always an interesting topic in Computer-Aided Design (CAD). In this paper, a method of approximating complex freeform shapes with parameterized freeform feature templates is proposed. To achieve this, a portion of a digitized 3-Dimensional (3-D) shape should be matched, or fitted, to a deformable shape feature template, where the deformation is a function of intrinsic feature parameters. 3-D shape matching to, possibly sparse, inaccurate or otherwise degraded, freeform surface data is known to be hard. Using a variant of the directed Hausdorff distance measure of shapes, it is shown that convergence towards a shape match is feasible. Based on sensitivity analyses of the shape distance measures, it is determined that adjusting coefficients of the optimization function in different stages of optimizations can accelerate the optimization procedure. By the matching results, a standard deviation-like function is proposed to achieve automatic feature recognition. With the proposed extendable concept, complex freeform shapes are tracked and fitted automatically. Based on a defined interference ratio, interfered feature can also be identified. Numerical experiments were conducted in order to verify the proposed method and to find the maximal degree of feature interference for which matching is successful. It is also described how the presented technique can be applied in shape modeling applications."
1248,"The main task of a product family designer is to decide the right components/design variables to share among products to maintain economies of scale with minimum sacrifice in the performance of each product in the family. The decisions are usually based on several criteria, but production cost is of primary concern. Estimating the production cost of a family of products involves estimating the production cost of each product in the family including the cost effects of common and variant components/design variables in the family. In this paper, we introduce a production cost estimation framework for product family design based on Activity-Based Costing (ABC), which is composed of three stages: (1) allocation, (2) estimation, and (3) analysis. In the allocation stage, the production activities that are necessary to produce all of the products in the family are identified and modeled with an activity table, a resource table, and an activity flow. To allocate the activities to products, a product family structure is represented by a hierarchical classification of the items that form the product family. In the estimation stage, production costs are estimated by converting the production activities to costs using key cost drivers that consume main resources. In the analysis stage, components/design variables for product family design are investigated with resource sharing methods through activity analysis. As an example, the proposed framework is applied to estimate the production cost of a family of cordless power screwdrivers."
1249,"Formulations for the optimal design of plane grids with maximum band gaps are presented. Periodic band-gap structures prevent waves in certain frequency ranges from propagating. Materials or structures with band gaps have many applications, including frequency filters, vibration protection devices and wave guides. Here, a simple model of a periodic plane grid structure is presented and then an optimization problem is formulated where the structure’s band gap above a particular frequency is maximized by the selective addition of non-structural masses. Numerical implementation issues are discussed and examples are presented."
1250,"Both multiple objectives and computation-intensive black-box functions often exist simultaneously in engineering design problems. Few of existing multi-objective optimization approaches addresses problems with expensive black-box functions. In this paper, a new method called the Pareto set pursing (PSP) method is developed. By developing sampling guidance functions, this approach progressively provides a designer with a rich and evenly distributed Pareto optimal points. This work describes PSP in detail with analysis of its properties. From testing and design application, PSP demonstrates considerable efficiency, accuracy, and robustness. Theoretical proof of convergence of PSP is also given. It is believed that PSP has a great potential to be a practical tool for multi-objective optimization problems."
1251,"Product platform concepts are often deployed to achieve product variety and hence effective product customization. One of the popular methods to achieve product variety is to "
1252,"A new mathematical model for representing geometric tolerances is applied to a part with an angled face and is extended to show its sensitivity to different specifications for dimensioning and tolerancing the part. The model is compatible with the ASME/ISO Standards for geometric tolerances. Central to the new model is a Tolerance-Map® , a hypothetical volume of points that corresponds to all possible locations and variations of a segment of a plane which can arise from tolerances on size, position, form, and orientation. Every Tolerance-Map is a convex set. This model is one part of a bi-level model that we are developing for geometric tolerances. The new model makes stackup relations apparent in an assembly, and these can be used to allocate size and orientational tolerances; the same relations also can be used to identify sensitivities for these tolerances. All stackup relations can be met for 100% interchangeability or for a specified probability. This paper develops several Tolerance-Maps for a part with an angled end face for different tolerance specifications. These specifications are linear size, angularity, angular size, “linear size & angularity” and “linear & angular size” tolerance. Comparison of Tolerance-Maps for their content for these specifications led to the following conclusions: a) only angular size tolerance is not sufficient for tolerancing an angled face; b) if the value of tolerance remains the same, the allowable variation is more in a part having only an angularity tolerance than in one having only a size tolerance."
1253,"Forging sequence design is mainly carried out using empirical rules for the design of the intermediate die shapes, in addition to many trail-and-error runs resulting in prolonged development times and higher costs. An integrated optimal design of preform shapes and process conditions approach to minimize the energy required is essential. The research presented in this article aims at developing an optimization algorithm to determine the optimum intermediate die shape-designs that minimize the total energy required during the forging process sequence. It is based on the results obtained in the previous research with focus on knowledge base and database representation to design precision forging solid gears and provide detailed process specification. A three-step algorithm, which addresses gear construction design, manufacturability analysis of gear construction and die-design optimization, is used to generate the parametric gear model and automatically extract design information for manufacturing process planning based on the feature-based parametric design system. Utilization of the shape optimization method for preform stages avoids costly production problems. The optimized approach provides accurate description of all stages involved in the forging process. Forging load and energy required, along with metal flow and detailed geometry specification of die forms for every forging stage are obtained. The forging energy requirements based on this approach are as much as 25% lower than those arrived from die designs based on actual tooth profile geometry."
1254,"In this paper, an alternate passive leg structure is proposed for Tricept machine tool to form a modified Tricept machine tool. The global stiffness of the modified Tricept is derived and compared with that of the Tricept machine tool. First, the configurations of the Tricept and modified Tricept are introduced respectively. Then, the global velocity equations are derived and the stiffness models of the two configurations are presented and analyzed. Finally, the advantages and disadvantages of the two types of passive leg structure are analyzed and concluded and stiffness simulations are conducted."
1255,"Vehicle package development is an important part of the entire vehicle design. It consists of determining the occupant’s spatial environment, the vehicle’s mechanical spatial configuration and the overall exterior/interior dimensions while meeting the engineering requirements, including packaging, structure, manufacturing, etc. Developing and verifying the occupant compartment configuration is usually conducted by using a seating buck. To build a seating buck, vehicle interior surfaces are generated in CAD using vehicle exterior surfaces, package layouts and master sections. During early program stages, this information is scattered, incomplete and constantly changing, which makes the seating buck creation challenging and the package design decision-making more difficult. A new method has been developed to quickly generate the seating buck surfaces from scattered information. It has shown to significantly reduce the time conventionally required for the seating buck surface modeling. This paper documents the method and process and summarizes the potential of the method and its impact on vehicle package design."
1256,"Geometric parameter and tolerance design are two important phase in product design, and will affect the assembly functionality of the product. The aim of the article is to present a new method of parameter and tolerance constraints modeling for assembly. In the nominal domain, a graphical representation of the linkage of geometric constraints is termed the nominal loop circuit (NLC), which helps to formulate the stack path of parameters into parameter equations. In the variational domain, assembly stackup functions are established. In the tolerance domain, tolerance inequations are formulated. The parameter equations, tolerance inequations and assembly stackup functions constitute parameter and tolerance constraints model, which can be used to optimize of parameter and tolerance. The method is demonstrated by a gear pump design problem."
1257,
1258,"The ability to optimize shape and microstructure creates tremendous potential for designing lightweight high-performance structural components, but this potential can only be realized if design procedures are effectively linked to fabrication techniques. In this paper we describe such links. The key to this work is to base the topology optimization on a unit cell that matches the physical unit cell used in the fabricated object. Distortion and rotation of the unit cell can control effective material properties at all locations within the component by affecting changes in density, degree of anisotropy, and orientation of principle material axes. Advances in solid freeform fabrication (SFF) and related hybrid fabrication techniques make production of components with complex microstructures feasible. This paper describes the complete design process, from specializations of topology optimization to interpretation of optimization results and communication of design information to SFF machines. The process includes a new procedure to establish the orientation of an orthotropic material for multiple loading cases."
1259,"Portable cooling systems play an important role in assisting human operations in unfriendly environments, such as soldiers continuously working in a desert area for long hours. Typical cooling system designs utilizing a vapor compression cycle driven by electrical power usually have high weights due to batteries and as a result, compromise the effectiveness of the portable cooling system. A self-contained absorption cycle cooling system design based on micro-scale thermal technology has demonstrated unique advantages in minimizing system weight while providing reasonable thermal efficiency. This system adopts a heat actuated absorption/desorption thermal cycle to raise the pressure of the refrigerant vapor without a heavy battery load. Design challenges exist: 1) multi-physics considerations when integrating the thermodynamic and transport models for the heat pump and peripheral component devices; 2) trade-off among multi-functional design requirements of system weight and thermal efficiency, using the inputs of cooling load, heat rejection temperature, and heat transfer characteristics based on the micro-channel geometry. No existing design automation tools are available on the market to directly support these design tasks. In this work, physics-based system-level models are developed and validated against state-of-the-art prototypes. The use of these models is demonstrated through the design of a 150-watt portable cooling system, typically used by the military in desert training. The system modeling methodology is implemented in Java as a part of an Integrated Design Support Environment, and has been used to generate trade-off study results. These results show that the current implementation is effective, and is a significant step toward a complete integrated design support environment to analyze and synthesize high-quality micro-scale portable cooling systems."
1260,"It is well recognized that conceptual design is the most critical stage of product development process. Yet, existing MEMS (Micro-Electro-Mechanical Systems) design synthesis models or methods are very restrictive in supporting MEMS conceptual design, in that they are only applicable to specific or specific types of designs, where building blocks for design synthesis have to be pre-specified by the designers. To address this problem, this paper proposes a MEMS conceptual design synthesis framework, which consists of a behavior representation that caters for the multidisciplinary MEMS design characteristics and a design synthesis strategy that is able to explore multidisciplinary phenomena for the development of MEMS initial design concepts. The behavior representation incorporates information of both physical interactions and chemical/biological/other reactions that take place during a MEMS device’s behavioral process. The design synthesis is accomplished by both forward and backward synthetic search strategies in identifying the relevant phenomena for the development of the desired behavioral processes. The framework can be used to develop both the physical structure of a MEMS device and the substances that are necessary for the chemical/biological/other reactions. A software prototype implementing the proposed framework is also presented, followed by a MEMS design case study."
1261,"2003 was a year that a number of accidents broke out in Japan. The sudden increate in accidents and troubles had the people wonder if there is something fundamentally wrong with the way they are running the business. These series of accidents and failure brought much attention to the Study of Failure. The Study of Failure was first published in 1996 and gained national attention in 2000, however, its roots are found in the late 70s. This paper is intended to provide background information about the Study of Failure, where it came from what efforts are underway in Japan. Instead of making precise records of what happened, the Study of Failure concentrates on finding the root cause, which often times is organizational rather than individual, provides ways for effectively recording them and analyzing them so other people can receive the maximum benefit from learning about the events. It has no intention of accusing persons who may have caused the events. There are now publicly available databases and privately developed software based on the studies. The government is putting efforts into educating the people about these subjects."
1262,"Due to the increasing complexity of the modern industrial context in an evolutionary environment, several changes (e.g. new technology, new system, human errors, etc.) may affect road safety. Analyzing the change impact on design requirements is a complex task especially when it deals with complex systems such as Vehicle Safety Systems (VSS). To handle a change impact analysis in road safety field, VSS designers require a specific knowledge stemmed from accidentology. In this paper, we develop a multi-view model of the road accident, which is crucial to extract the required knowledge. Indeed, this multi-view model allows the analysis of the impact of a given change on the Driver-Vehicle-Environment system from different viewpoints and on different grain of size. This allows an efficient approach to detect exhaustively the perturbations due to the change and thereby to anticipate and handle their effects. We use a Knowledge Engineering approach to implement the multi-view model in a Knowledge-Based System providing accidentologists and VSS designers with an efficient tool to carry out an analysis of change impact on analysis design requirements."
1263,"The popularity of sheet metal in modern engineering artifacts is due to the fact that it is both inexpensive as a raw material and inexpensive to form into components. In comparison to forging or machining components, sheet metal can produce lightweight and inexpensive design solutions. The main shortcomings of sheet metal are that resulting components have a limited rigidity and the feasibility of the parts is constrained by the inherent two-dimensionality of the initial sheet. Furthermore, design and manufacturing engineers are challenged by finding a shape that satisfies all spatial constraints and by deciding the optimal sequence of operations for making this product that minimizes both time and associated manufacturing costs. In the past two years, we have been working towards an automated tool that creates candidate sheet metal topologies and optimizes them for spatial constraints as well as time and cost objectives. While we have yet to complete this goal, we have to date developed a representation capable of creating a wide variety of sheet metal topologies (see DETC2002/DAC-34087) and have recently created a thorough evaluation method which is presented in this paper along with some preliminary results."
1264,"Reliability-Based Design Optimization (RBDO) can provide optimum designs in the presence of uncertainty. It can therefore, be a powerful tool for design under uncertainty. The traditional, double-loop RBDO algorithm requires nested optimization loops, where the design optimization (outer) loop, repeatedly calls a series of reliability (inner) loops. Due to the nested optimization loops, the computational effort can be prohibitive for practical problems. A single-loop RBDO algorithm is proposed in this paper for both normal and non-normal random variables. Its accuracy is the same with the double-loop approach and its efficiency is almost equivalent to deterministic optimization. It collapses the nested optimization loops into an equivalent single-loop optimization process by imposing the Karush-Kuhn-Tucker optimality conditions of the reliability loops as equivalent deterministic equality constraints of the design optimization loop. It therefore, converts the probabilistic optimization problem into an equivalent deterministic optimization problem, eliminating the need for calculating the Most Probable Point (MPP) in repeated reliability assessments. Several numerical applications including an automotive vehicle side impact example, demonstrate the accuracy and superior efficiency of the proposed single-loop RBDO algorithm."
1265,"This paper studies geometric design of uniform developable B-spline surfaces from two boundary curves. The developability constraints are geometrically derived from the de Boor algorithm and expressed as a set of equations that must be fulfilled by the B-spline control points. These equations help characterize the number of degrees of freedom (DOF’s) for the surface design. For a cubic B-spline surface with a first boundary curve freely chosen, five more DOF’s are available for a second boundary curve when both curves contain four control points. There remain (7-2m) DOF’s for a cubic surface consisting of m consecutive patches with C2  continuity. The results are in accordance with previous findings for equivalent composite Bézier surfaces. Test examples are illustrated to demonstrate design methods that fully utilize the DOF’s without leading to over-constrained systems in the solution process. Providing a foundation for systematic implementation of a CAGD system for developable B-spline surfaces, this work has substantial improvements over past studies."
1266,"In this paper, three points are emphasized to cope with computational creative design, decomposition, mapping and reconstitution (D-M-R for short), which is proposed to cope with divergent exploration, automatic transformation and convergent exploitation. It could be concluded that, management of creative process is the key issue to develop creative computational design tools; and the modeling of design tools could facilitate the creative thought processes. Although creative work need the tension between building and breaking out of intellectual traditions, the creative activity in CAD systems need large knowledge base to support design activities and is capable of learning. The decomposition, mapping and reconstitution model could be a common exploration, transformation and exploitation procedure for management of computational design tools, therefore helpful for creative work; specifically packages of design tools have been developed to testify creativity in design practices, and the relationship between creativity and automation."
1267,"The availability of computationally efficient and accurate methods for probabilistic computation is crucial to the success of applications of probabilistic design using complex engineering simulation models. To address this need, a Saddlepoint Approximation method for probabilistic engineering analysis is introduced. A general performance function is approximated at the Most Likelihood Point with either linear or quadratic forms and the Saddlepoint Approximation is then applied to evaluate the probability associated with the performance. The proposed approach provides highly accurate probabilistic results while maintaining minimum computational requirement. Two examples are presented to demonstrate the effectiveness of the proposed method."
1268,"This paper describes a system and underlying algorithms to perform geometric containment analysis to determine if a newly designed rotational part can be manufactured from a part in an existing database of rotational parts. Only material removal of the database part is considered in order to obtain the newly designed part from the database part. The system uses a three-step algorithm to test for containment. The first step analyzes feasibility of containment using bounding cylinders. If the bounding cylinder of the query part is bigger than the part in the database, then the database part cannot contain the query part and it is eliminated from consideration. The second step analyzes feasibility of containment by ignoring off-axis features. Any part that fails to satisfy containment at this stage is eliminated from consideration. The third step analyzes the remaining parts from the database for feasibility of containment by including the off-axis features. Finally, the system rank-orders all the database parts that can contain the query part based on their volume differences with the query part. The system described in this paper can be used to find an existing part from which to manufacture a newly designed part. This capability is expected to significantly reduce proliferation of parts, to improve manufacturing responsiveness, and to reduce the cost of new products."
1269,"Reverse engineering (RE) is the process of defining and instantiating a model based on the measurements taken from an exemplar object. Traditional RE is costly, requiring extensive time from a domain expert using calipers and/or coordinate measurement machines to create new design drawings/CAD models. Increasingly RE is becoming more automated via the use of mechanized sensing devices and general purpose surface fitting software. This work demonstrates the ability to reverse-engineer parts by combining feature-based techniques with freeform surface fitting to produce more accurate and appropriate CAD models than previously possible."
1270,"This paper discusses a systems framework for platform architecture analysis. The framework considers architectural analysis at three levels; the individual product offerings within a product family, the platform(s) being leveraged across the family, and the evolution potential for the platform/product family. The framework is decomposed into elements that consider a systems perspective: function, form, concepts, interfaces, needs/goals, upstream and downstream influences, and timing/operation. An application of the framework to a transport refrigeration product family is presented as a case study. The results of this case study indicate that the framework is promising, and it continues to be developed and applied within UTC."
1271,"The use of kriging models for approximation and metamodel-based design and optimization has been steadily on the rise in the past decade. The widespread usage of kriging models appears to be hampered by (1) the lack of guidance in selecting the appropriate form of the kriging model, (2) computationally efficient algorithms for estimating the model’s parameters, and (3) an effective method to assess the resulting model’s quality. In this paper, we compare (1) Maximum Likelihood Estimation (MLE) and Cross-Validation (CV) parameter estimation methods for selecting a kriging model’s parameters given its form and (2) and an R2  of prediction and the corrected Akaike Information Criterion for assessing the quality of the created kriging model, permitting the comparison of different forms of a kriging model. These methods are demonstrated with six test problems. Finally, different forms of kriging models are examined to determine if more complex forms are more accurate and easier to fit than simple forms of kriging models for approximating computer models."
1272,"This paper presents a method for identifying the optimal designs of components and joints in the space frame body structures of passenger vehicles considering structural characteristics, manufacturability and assembleability. Dissimilar to our previous work based on graph decomposition, the problem is posed as a simultaneous determination of the locations and types of joints in a structure and the cross sections of the joined structural frames, selected from a predefined joint library. The joint library is a set of joint designs containing the geometry of the feasible joints at each potential joint location and the cross sections of the joined frames, associated with their structural characteristics as equivalent torsional springs obtained from the finite element analyses of the detailed joint geometry. Structural characteristics of the entire structure are evaluated by finite element analyses of a beam-spring model constructed from the selected joints and joined frames. Manufacturability and assembleability are evaluated as the manufacturing and assembly costs estimated from the geometry of the components and joints, respectively. The optimization problem is solved by a multi-objective genetic algorithm using a direct crossover. A case study on an aluminum space frame (ASF) of a middle size passenger vehicle is discussed."
1273,"In the present paper, a new approach for structural topology optimization based on implicit topology description function (TDF) is proposed. TDF is used to describe the shape/topology of a structure, which is approximated in terms of the nodal values. Then a relationship is established between the element stiffness and the values of the topology description function on its four nodes. In this way and with some non-local treatments of the design sensitivities, not only the shape derivative but also the topological derivative of the optimal design can be incorporated in the numerical algorithm in a unified way. Numerical experiments demonstrate that by employing this approach, the computational efforts associated with TDF (and level set) based algorithms can be saved. Clear optimal topologies and smooth structural boundaries free from any sign of numerical instability can be obtained simultaneously and efficiently."
1274,"This paper describes the initial results from new research in the area of electrochemical deposition applied to rapid layered manufacturing with heterogeneous materials. Our interest in electrochemical layered manufacturing stems from earlier collaborative projects on heterogeneous modeling and fabrication [1–4]. The current work focuses on employing electrochemical deposition of binder into layers sequentially added to a powder bed as a method for additive fabrication of working parts. We show that electrodeposition provides a feasible approach to (1) eliminating the need for a fugitive binder system, (2) reducing thermal postprocessing operations for three-dimensional printing of working parts, and (3) achieving a room temperature fabrication process minimizing the need for thermal postprocessing thereby eliminating residual thermal stresses and dimensional inaccuracies from thermal expansion/contraction associated with processing at elevated temperatures. We also demonstrate the feasibility of direct, single-step fabrication of fully dense parts."
1275,"Designing an internal combustion engine involves compromising among multiple performance metrics and targets with multiple control and noise factors. The main challenges are in determining the critical performance metrics, finding the optimal compromise between these metrics, and correctly represent the most important control and noise factors through CAE modeling and optimization. This paper presents a methodology for practical application of robustness and performance optimization using a CAE model. The key element of the methodology is a concept of surrogate noise. With this concept, the multiple noise factors affecting the system performance are represented through a limited number of noise factors for CAE modeling. The other part of the methodology is to substitute complicated and computationally time intensive CAE modeling with a cheap-to-compute Gaussian Kriging model through Optimal Sampling and Design of Experiment. The final part of the methodology is performing multi-criteria robustness and performance optimization as well as performance and robustness confirmation of the optimal design point. The proposed methodology has been applied to a practical problem of designing the IC engine main bearing system. The results of the analysis have provided practical recommendations and directions to drive the main bearing system design. In this paper, the methodology is demonstrated through the presentation of a simplified form of this investigation."
1276,"A multidisciplinary optimization method is presented to support the design process of partially-filled liquid containers subject to the disciplines of sloshing and impact analysis. This paper represents a part of a study on Multidisciplinary Design and Optimization of liquid containers, and shows experimental techniques used to try to better understand sloshing as a phenomenon and to evaluate the capabilities of the commercial Computational Fluid Dynamics (CFD) code in question. Experimental validation includes qualitative comparison of visual free-surface behavior and quantitative comparisons of pressure measurements in the time and frequency domain. The liquid motion exhibits good comparisons in time with some deviations in wave amplitude due to a modification of the low frequency content of the input signal to the CFD simulation. This modification was caused by both the experimental signal filtration process and deficiencies in the low-frequency measurement capability of the accelerometer. In the frequency domain the first two odd oscillatory modes are accurately captured. A candidate objective function for the quantitative evaluation of the sloshing phenomenon is proposed. Using the response surface method in LS-OPT, various single (sloshing or impact only) and multidisciplinary optimization formulations are presented and results are examined. As expected, the multidisciplinary optimum proved to be a compromise between the optima obtained when considering the two single disciplines independently."
1277,"A new systematic approach to identify the optimal design configuration and attributes to minimize potential construction project changes is introduced in this research. The first part of this paper focuses on the configuration design aspect. In this research, the relations between design configurations and construction tasks are modeled by axiomatic design matrices. The design configuration that is most independent to the construction tasks is identified based upon the axiomatic design approach. In addition, estimation of potential project change cost due to the potential design configuration changes is also discussed. Case studies in pipeline engineering design and construction have been conducted to show the effectiveness of the introduced approach."
1278,"This research introduces a new systematic approach to identify the optimal design configuration and attributes to minimize the potential construction project changes. The second part of this paper focuses on the attribute design aspect. In this research, the potential changes of design attribute values are modeled by probability distribution functions. Attribute values of the design whose construction tasks are least sensitive to the changes of these attribute values are identified based upon Taguchi Method. In addition, estimation of the potential project change cost due to the potential design attribute value changes is also discussed. Case studies in pipeline engineering design and construction have been conducted to show the effectiveness of the introduced approach."
1279,"With the advent of highly complex engineering simulation models that describe the relationship between input variables and output response, the need for an efficient and effective sensitivity analysis is more demanding. In this article, a generalized approach that can provide efficient as well as accurate global sensitivity indices is developed. The approach consists of two steps: running an orthogonal array based experiment using moment-matched levels of the input variables and followed by a variance contribution analysis. The benefits of the approach are demonstrated through three different examples."
1280,"Many existing selection methods require that the Decision Maker (DM) state his/her preferences precisely. However, the DM may not have enough information about the needs of end users thus causing variability in the preferences. To address this problem, we present a method for selection that accounts for variability in the DM’s preferences. Our method is interactive and iterative and assumes only that the preferences of the DM reflect an implicit value function that is quasi-concave and non-decreasing with respect to attributes. Due to the variability, the DM states his/her preferences with a range for Marginal Rate of Substitution (MRS) between attributes at a series of trial designs. The method uses the range of MRS preferences to eliminate “dominated designs” and find a set of “non-eliminated designs”. We present a heuristic to reduce the set of non-eliminated designs and obtain a set of “potentially optimal designs”. The significance of potentially optimal designs is that only one of these designs will be the most preferred for any subset of the range of MRS preferences. We present a payload design selection example to demonstrate and verify that our method indeed finds the set of potentially optimal designs."
1281,"The Hypothetical Equivalents and Inequivalents Method (HEIM) has been developed to support decision making in multiattribute problems where one decision maker is making the decision. In this paper HEIM is modified to support group decision making in multiattribute problems, resulting in the Group Hypothetical Equivalents and Inequivalents Method (G-HEIM). Instead of aggregating attribute weights or overall alternative values from each individual as is common in other group decision methods, G-HEIM operates by aggregating individual preferences. It is recognized that in group decision making, common preferences among group members can rarely be guaranteed, unless individual freedom is greatly limited. G-HEIM instead allows individuals to freely express preferences over a number of hypothetical alternatives and then explores the level of conflict or differences from the aggregated group preferences. The relationship between the level of conflicting preferences and the usability of the resulting decision is also directly studied using the G-HEIM. An automotive selection example is used to illustrate the approach."
1282,"Decentralized systems constitute a special class of design under distributed environments. They are characterized as large and complex systems divided into several smaller entities that have autonomy in local optimization and decision-making. The mechanisms behind this network of decentralized design decisions create difficult management and coordination issues. Standard techniques to modeling and solving decentralized design problems typically fail to understand the underlying dynamics of the decentralized processes and therefore result in suboptimal solutions. This paper aims to model and understand the mechanisms and dynamics behind a decentralized set of decisions within a complex design process. This paper builds on already existing results of convergence in decentralized design for simple problems to extend them to any kind of quadratic decentralized system. This involves two major steps: developing the convergence conditions for the distributed optimization problem, and finding the equilibrium points of the design space. Illustrations of the results are given in the form of hypothetical decentralized examples."
1283,"This paper presents a model-based systems engineering methodology that can be applied to perform a root cause analysis on "
1284,"This paper presents a methodology for design optimization of decomposed systems in the presence of uncertainties. We extend the analytical target cascading (ATC) formulation to probabilistic design by treating stochastic quantities as random variables and parameters and posing reliability-based design constraints. We model the propagation of uncertainty throughout the multilevel hierarchy of elements that comprise the decomposed system by using the advanced mean value (AMV) method to generate the required probability distributions of nonlinear responses. We utilize appropriate metamodeling techniques for simulation-based design problems. A simple yet illustrative hierarchical bi-level engine design problem is used to demonstrate the proposed methodology."
1285,"Commercial feature-based design systems are based on describing the design model in some form of sequential representation of primitive shapes and operations called features. In these systems, the overall design process, the behavior of building blocks and the characteristics of the final model, are governed by the construction sequence. These systems do not check for the conformity of the final shape with the actual design intent of features, and allow their design and engineering intent to be altered during the design process. The research work presented here describes a new design methodology and feature representation for facilitating a design environment that is independent of any construction order or constraint-based dependencies and provides a mechanism for maintaining design and engineering intent of the design features. The methodology works by dynamically evaluating the features using a planning algorithm such that the validity of each feature is maintained. These are intended to serve as a generic template that can be used to design and develop specific design features and CAD software systems."
1286,"In manufacturing processes, it is widely accepted that uncertainty plays an important role and should be taken into account during analysis and design processes. However, uncertainty quantification of its effects on an end-product is a very challenging task, especially when an expensive computational effort is already needed in deterministic models such as sheet metal forming simulations. In this paper, we focus our work on the variance estimation of the system response. A weighted three-point-based strategy is proposed to efficiently and effectively estimate the variance of the system response. Three first-order derivatives for each variable are used to estimate the nonlinear behavior and variance of the system. The details of the derivation of the approach are presented in the paper. The optimal locations of the three points along each axis in the standard normal space and weights for input variables following normal distributions are proposed as (−1.8257,0.0,+1.8257) and (0.075,0.850,0.075), respectively. For input variables following uniform distributions "
1287,"This paper proposes a design optimization method for machine products that is based on the decomposition of performance characteristics, or alternatively, extraction of simpler characteristics, to accommodate the specific features or difficulties of a particular design problem. The optimization problem is expressed using hierarchical constructions of the decomposed and extracted characteristics and the optimizations are sequentially repeated, starting with groups of characteristics having conflicting characteristics at the lowest hierarchical level and proceeding to higher levels. The proposed method not only effectively enables achieving optimum design solutions, but also facilitates deeper insight into the design optimization results, and aids obtaining ideas for breakthroughs in the optimum solutions. An applied example is given to demonstrate the effectiveness of the proposed method."
1288,"This paper discusses a method to determine the optimal direction of the principal moment of inertia in frames element cross-sections for the design of mechanical structures at the conceptual design phase. The direction in each frame element is determined by maximizing the structural stiffness. Construction of the optimization procedure is based on the KKT-conditions and the balance of bending moments applied to each frame element. This method is implemented as an application in a structural topology optimization procedure that uses frame elements. Finally, several examples are presented to confirm that the proposed method is useful for the topology optimization method discussed here."
1289,"Many highly accurate computer simulation tools have been developed for assembly line design, such as for simulation of assembly processes, but these tools require much input information and are generally utilized only in detailed design stages. This paper proposes a rapid analysis method for manual assembly line design, which can be utilized in the conceptual design stage. This method is based on a layout tool where design engineers can construct assembly line models using 2- and 3-D views. This method provides design evaluation techniques for multiple important criteria such as volume flexibility, visibility, and so on, using the layout data. Spatial evaluation and quantitative efficiency analyses can be simultaneously performed, which enhance collaborative decision-making in the conceptual design stage."
1290,"Our “Study of Failure” has shown the effects of failure case illustration and text based diagonal scenario expression to successfully convey the essence of failure cases to the reader. A well drawn failure case illustration generates a good image of the failure event in the readers mind, thus succeeds in passing the failure knowledge to the reader. A carefully produced diagonal scenario expression has the same effect. We demonstrated the power of these two fundamentally different representations through an experiment: A failure case illustration alone was shown to a group of people who were asked to define a diagonal scenario expression for the case. The reverse test started from a diagonal scenario expression to reach an illustration that the group had no prior knowledge about. Our tests showed that people can produce a fairly good representation in the other form starting from either an illustration or a diagonal scenario alone."
1291,"When creating a design automation system for a mature product there already exists a complete and functional product design and the task is to retrace the initial design process to find the input parameters, algorithms, rules, relations and solution strategies (design process information) that govern this initial design. This paper presents strategies and procedures for retracing, naming, classifying and storing the design process information governing the design variables of a mature product design, seen from a CAD representation perspective. Emphasis is on strategy for storing the design process information for use with the CAD representation as well as system transparency and efficient reuse of the documented and stored information."
1292,"Efficient product configuration systems have been widely recognized by industrial companies as an important tool for meeting increasing customer requirements for specifically adapted products according to their needs/requirements. The main obstacle in this area has been the long-term maintenance of the product configuration models with the new product definition information generated throughout the product lifecycle. Much of the work in this area has focused on product architecture issues, data modelling techniques and software tools. However, in order to be useful, the models and software applications must be put into the right business process context that is supported by the proper organizational framework. Thus, the paper presents a generic process-oriented approach for change management of product configuration-related information in industrial companies. The process contains steps for identification of new and/or changed product configuration knowledge, request of a change in the product configuration model, evaluation of the request, and finally an update of the product configuration model in the system. In addition, there is a description of employees’ competence profile descriptions and the organizational roles needed to support an effective product configuration management process in an industrial company."
1293,"A novel meander-line structure implemented with bimorph piezoelectric actuators driven by alternating current power is developed in this article. Via the generated traveling wave, this mechanism is able to transport parts. Dynamic model of the structure as well as motion trajectory and optimal transport feed rate is studied, and also verified by the practical experiment."
1294,"Recent developments in computer capabilities and software enabled the application of deterministic optimization and Robust Design methods in real world aero engine development programs. This paper describes the methods used and shows several applications of this technology. The first example is the application of a Monte-Carlo simulation to support design decisions in the HP turbine casing air system. Here the main goal was to achieve a robust design addressing the variation of build tolerances on flow areas. The variation of parameters as mass flows, pressures and temperatures based on 5000 permutations of the base model give a high confidence level for achieving reliable system behavior for a large population of engines. In addition, dependencies of result parameters on input variations indicate the main levers for system improvement. A second example is the optimization of compressor discs. Here the main emphasis was on the influence of manufacturing tolerances and on the best method to evaluate these tolerances for longer running analysis tasks. Therefore, results of a full Monte-Carlo simulation are compared with results based on two surrogate models, a response surface and a Taylor series expansion. As a final example the optimization of a HP turbine disc for which a Design of Experiment has been performed to generate a response surface model is discussed. Using the response surface data the life variability due to assumptions in the thermal modeling have been quantified and used to adjust the constraints for the subsequent deterministic optimization for weight of the HP turbine. Using deterministic optimization and especially Robust Design methods a considerable decrease in development time and cost as well as an increased product quality and reliability have been achieved. However, deterministic optimization methods alone normally drive designs on to the constraint boundaries, leading to “cliff-edge” designs. Therefore, the application of Robust Design methods is required to increase the product reliability. These methods still require a considerable computing effort, so the widespread application is just starting."
1295,"Several procedures have been developed in the literature for reliability-based design optimization (RBDO), including the Reliability Index Approach (RIA), the Performance Measure Approach (PMA), and more recent techniques wherein the reliability and optimization calculations are decoupled. This paper extends the decoupled approach to include standard deviations as design parameters and wherein simulation or other methods can replace the traditional first order analytical method for reliability assessment. The methods are extended to robust design and their applicability is investigated. The paper also investigates a single loop method and extends it for the robust design problem. The accuracy and computational efficiency of the various RBDO methods are compared."
1296,"We present an integrated engineering design and marketing approach to facilitate the selection of a robust set of product designs to carry forward to the prototype stage. Our approach considers variability (i.e., noise or uncertainty) in both (i) engineering design domain, and (ii) customer preferences in marketing domain, to prune a set of design alternatives to a manageable size. In the design domain, our approach evaluates performance and feasibility robustness of a design alternative when there are variations in uncontrollable parameters. The goal of our approach in the design domain is to obtain a set of design alternatives that shows the best possible performance while maintaining feasibility even if the alternatives are subject to applications and environments that are different from their standard laboratory conditions (i.e., nominal parameter values). In the marketing domain, our approach considers the impact of performance variations in different usage situations and conditions on customer preferences and the uncertainties and sampling errors in estimating customer preferences. In addition, competitive products and their positions are considered in pruning the set of design alternatives. We illustrate our approach in the context of the design of a cordless power tool example, which highlights the advantages of using our approach."
1297,"High-speed automotive valve train design requires realistic models of the valve train. However, this frequently results in highly nonlinear systems with discontinuities and constraints. Optimality criteria and trade-offs for the designs are frequently performed through a process of simulation and iterative refinement. This paper presents CamOE, a cam design optimization package based on direct multiple shooting optimal control theory, incorporating structured sequential quadratic programming. The code allows the designer to incorporate the constraints of importance and to consider and synthesize appropriate optimality criteria. This allows him or her to synthesize the cam profile at the design stage without resorting to a tedious trial-and-error design process. This paper presents CamOE as a software environment that permits rapid feedback to the designer through the process of numerical experiments in specifying criteria and constraints on the automotive valve train."
1298,"Many engineering optimization problems can be considered as multistage decision-making problems. If the system involves uncertainty in the form of linguistic parameters and vague data, a fuzzy approach is to be used for its description. The solution of such problems can be accomplished through fuzzy dynamic programming. However, most of the existing fuzzy dynamic programming algorithms can not deal with mixed-discrete design variables in the optimization of mechanical systems containing fuzzy information. They often assumed that a fuzzy goal is imposed only on the final state for simplicity, the values of fuzzy goal and other parameters need to be predefined, and an optimal solution is obtained in the continuous design space only. To better reflect the nature of uncertainties present in real-life optimization problems, a mixed-discrete fuzzy dynamic programming (MDFDP) approach is proposed in this work for solving multistage decision-making problems in mixed-discrete design space with a fuzzy goal and a fuzzy state imposed on each stage. The feasibility and versatility of the proposed method are illustrated by considering the design of a four-bar truss. To the authors’ knowledge, this work represents the first fuzzy dynamic programming method reported in the literature for dealing with mixed-discrete optimization problems."
1299,"This work presents a methodology to find the optimal topology of a three-dimensional structure subject to impact loads, using the approach of ground structure. The method uses of the concept of topology optimization as a material allocation problem, which has been successfully used in the past to design structures modeled with shell and solid finite elements in the automotive industry. A simple example is shown to demonstrate the method."
1300,"A new class of pattern search algorithms called Objective Function-based Pattern Search is proposed for 3D Layout. These algorithms are driven by decreasing expected change in objective function rather than by decreasing step size. Current pattern search algorithms rely on decreasing step size of all patterns at the same rate. Also, all translation and rotation patterns are active at all step sizes of patterns. The new class of algorithms decreases the step sizes of the patterns based on the expected change in objective function value due to that pattern. Also, whether a pattern is active or inactive at a particular step size is decided by the expected change in objective function due to that pattern at that step size. The new algorithm is found to be up to 50% faster in runtime compared to previous pattern search based algorithms."
1301,"Complex new product development requires numerous decisions by many individuals and groups, which are often geographically and temporally distributed. There is a need to share and coordinate distributed resources and synchronize decisions, and recent advances in information technology (IT) pose an untapped potential for assisting in the capture, storage, retrieval and facilitated use of product development information. We exploit IT to address this problem through the proposed approach to Product Family Planning. By sharing assets such as components, processes and knowledge across a family of products, companies can efficiently develop differentiated products and increase the flexibility and responsiveness of their product realization process. In this paper we describe our recent efforts in realizing an information management infrastructure for product family planning and platform customization. In particular, we focus on three current research thrusts to identify product platform leveraging strategies to support future product family planning: (1) an evolutionary approach to product platforming, (2) a bottom-up approach to product platforming, and (3) industry-based platform case studies. Future research directions are also outlined."
1302,"A suite of math-based marketing and financial tools has been deployed and exercised within an automated, multidisciplinary parametric design framework. This suite of tools includes a market share estimator based on Cook’s S-Model, a Technical Cost Model for estimating the variable and fixed costs of the vehicle’s body system, a database of cost estimates for other vehicle systems, and a profit estimator developed from a standard accounting template. Development of the S-Model market share estimator included completion of a Demand-Price analysis for the midsize sedan segment and collection of publicly available value curves predominantly covering the powertrain performance and interior roominess disciplines. A flexible input-output interface was developed for the Technical Cost Model to provide a means of propagating changes in body design parameters throughout the framework. A series of exercises including analysis of a baseline vehicle, optimization of a hypothetical vehicle concept for net income, and a hypothetical architectural parameter study were conducted to demonstrate the capabilities of a multidisciplinary parametric design framework enabled with marketing and financial tools. These exercises demonstrate that existing engineering and business discipline tools can effectively interoperate to design for profitability in a multidisciplinary parametric design environment. They also illustrate several key challenges in automated design for profitability, such as those encountered in defining the role of price as a design variable in a tightly coupled design-for-profit system and in generating cost estimates using a continuously variable design representation."
1303,"Design optimization is becoming and increasingly important tool for design. In order to have an impact on the product development process it must permeate all levels of the design in such a way that a holistic view is maintained through all stages of the design. The design process can be viewed as a process of increasing the information of the finished product. Design optimization is on one hand a part of the design process, but can also be used as a metaphor for the whole design process. In this paper an information theoretical approach is taken to establish a performance criterion for an optimization method. Furthermore, this approach is extended do describe the design process."
1304,"This paper presents a fixturing method for sacrificial fixturing machining using CNC equipment. The focus of the paper is not on the method itself, but on the economics of sacrificial fixturing CNC machining, which defines the domain of use for the results described in the paper. The paper presents an economic model of machining, and then analyzes the use of the method as a function of: the number of parts to be produced, the ratio of material removed to final part volume, the number of features on the part, and the basic part geometry. We conclude that sacrificial fixturing is a very practical method that should be seriously considered when machining small batches of parts, rapid prototyping with CNC machining and parts with some particular geometric characteristics."
1305,"In this paper, parts from Ford GT were machined based on tool path created using Curvature Matched Machining and three other popular CAD systems. Five-axis flat end mill methods are used as the baseline of comparison. The performance of these CAD packages was compared using the benchmark of tool path density, surface finish, and post-machining finishing time. Results show that CM2 has advantage over today’s leading CAM capability in terms of both machining efficiency and tool path computation time."
1306,"As a new generation of printing technology, thermal inkjet (TIJ) has been widely adopted to meet the increasing demand for high printing quality and efficiency at an affordable price. High air barrier tubes play an important role in the reliable operation of the printhead in a commercial thermal inkjet printer. Desired tube qualities include low stiffness and low pressure drop, along with others. Tube stiffness and pressure drop can be lowered through the selection of proper tube layer configuration, geometry and material properties. However, the existing tube design practice is highly heuristic and design results are not optimal. Furthermore, there is no robust design consideration in the current design, and it is hard to trace and compare the tube quality from different groups of designers. In this work, a comparative study using industrial examples is conducted to search for a reusable robust design methodology for TIJ tube design. Two cases using different optimization strategies are investigated. In case A, a performance based optimization strategy is used, and the design objectives are the target performances without variation consideration. In Case B, a robust design based optimization strategy is used, and the variations of the target performances are incorporated in the design objectives. A comparison of their results with the current practice shows that the optimization strategies can greatly improve the efficiency of the current tube design process. More important, the optimization strategy with variation consideration yields robust results and provides much richer design knowledge to support designers with various experiences to make better decisions."
1307,"Reliability-based design optimization (RBDO) methods are optimization algorithms that utilize reliability methods to evaluate probabilistic constraints and/or objective functions used to prescribe reliability. For practical applications, it is important that RBDO methods are efficient, i.e, they only require a manageable number of numerical evaluations of underlying functions since each one can be computationally expensive. The type of reliability methods and the manner in which they are used in conjunction with optimization algorithms strongly affect computational efficiency. The first order reliability method (FORM) and its inverse are proved to be efficient and widely accepted for reliability analysis. RBDO methods have therefore employed FORM or inverse FORM to numerically evaluate probabilistic constraints and objective functions. During the last decade, the efficiency of RBDO methods has been further improved through problem reformulation. Our goal is to present RBDO methods from a mathematical optimization perspective by formalizing FORM, inverse FORM, and associated RBDO formulations. This new perspective helps not only to clearly reveal their close relationships but also provides a common ground for understanding different types of RBDO methods. Using numerical studies reported in the literature, we indicate the numerical efficiency, convergence, and accuracy of existing RBDO methods."
1308,"A novel reliability-based design optimization (RBDO) method using simulation-based techniques for reliability assessments and efficient optimization approach is presented in this paper. In RBDO, model-based reliability analysis needs to be performed to calculate the probability of not satisfying a reliability constraint and the gradient of this probability with respect to each design variable. Among model-based methods, the most widely used in RBDO is the first-order reliability method (FORM). However, FORM could be inaccurate for nonlinear problems and is not applicable for system reliability problems. This paper develops an efficient optimization methodology to perform RBDO using simulation-based techniques. By combining analytical and simulation-based reliability methods, accurate probability of failure and sensitivity information is obtained. The use of simulation also enables both component and system-level reliabilities to be included in RBDO formulation. Instead of using a traditional RBDO formulation in which optimization and reliability computations are nested, a sequential approach is developed to greatly reduce the computational cost. The efficiency of the proposed RBDO approach is enhanced by using a multi-modal adaptive importance sampling technique for simulation-based reliability assessment; and by treating the inactive reliability constraints properly in optimization. A vehicle side impact problem is used to demonstrate the capabilities of the proposed method."
1309,"Product families and product platforms have been suggested as design strategies to serve heterogeneous markets via mass customization. Numerous, individual cost advantages of these strategies have been identified for various life cycle processes such as product design, manufacturing, or inventory. However, these advantages do not always occur simultaneously, and sometimes even counteract each other. To develop a better understanding of these phenomena, this paper investigates the cost implications of the underlying design decision: the product architecture choice. The investigation includes factors such as product life cycle phases, allocation rules, and cost models, all of which impact the cost analysis results. Based on this investigation, directions for future research on product architecture costing are provided."
1310,"Computing the minimum distance between two models in a virtual scene is a fundamental operation useful in simulation, path planning, haptics, and modeling. In an environment with heterogeneous model representations, distance functions can be difficult to formulate and may require multiple specialized methods. In this paper, we demonstrate a generalized method for finding the distance between models with different representations and demonstrate it on a variety of models."
1311,"In this paper we review the current state of automated MEMS synthesis with a focus on generative methods. We use the design of a MEMS resonator as a case study and explore the role that geometric constraints and human interaction play in a computer-aided MEMS design system based on genetic algorithms."
1312,"In order to model uncertainties and achieve the required reliability, Reliability Based Design Optimization (RBDO) has evolved as a dominant design tool. Many methods have been introduced in solving the RBDO problem. However, the computational expense associated with the probabilistic constraint evaluation still limits the applicability of the RBDO to practical engineering problems. In this paper, a Two-Level Approximation method (TLA) is proposed. At the first level, a reduced second order approximation is used for better optimization solution; at the second level a linear approximation is used for faster reliability assessment. The optimal solution is obtained interatively. The proposed method is tested on certain numerical examples, and results obtained are compared to evaluate the cost-effectiveness."
1313,"Product platform design plays a vital role in determining two important aspects of a products family: efficiency (cost savings due to commonality) and effectiveness (capability to satisfy performance requirements). In this work, sensitivity analysis and cluster analysis are used to improve both efficiency and effectiveness of a product family design. A strategy of commonization is employed to form a platform. An illustrative example is used to demonstrate the merits of the proposed method, and the results are compared with existing results from the literature."
1315,"This paper presents a full parametric model of a turbomachinery blade. The model forms the geometric backbone of a new aerodynamic design suite, which aims at speeding up the coupled 2D/3D aerodynamic design process. The approach employed here follows the basic design concepts of turbomachinery blades, which are typically defined by a series of cross-sectional aerofoils stacked at their radial location to a three-dimensional blade. Unlike the geometry management in current design systems, the paradigm of a CAD based Master Model has been incorporated in the geometry definition and the corresponding software architecture. Therefore all blade features have been modelled as computational components in the flexible object-oriented software environment. The blade parameterisation enables the 2D aerofoil construction either from the common superposition of the camber line and thickness distribution or the direct control of the suction and pressure side. The sensitivity of the aerofoil aerodynamic performance with respect to a design parameter can be quickly assessed with a fast 2D flow solver. The parameters of the radial stacking line define the axial and tangential shift of each section. The parametric concept facilitates the inclusion of specific shape control techniques such as curvature manipulation and surface smoothing. Furthermore the use of optimisation methods is greatly simplified by the modular program structure, which allows to access single modules of the blade design tool in a batch job. Since the blade design process involves different coordinate systems, unique mapping functions are essential for a consistent update of the blade geometry during a design cycle. The interface to the CAD system is based on the standard data exchange format STEP. The CFD interface makes use of the NetCDF data format for automatic grid generation."
1316,"Reliability-based robust design optimization deals with two objectives of structural design methodologies subject to various uncertainties: reliability-based design and robust design. A reliability-based design optimization deals with the probability of failure, while a robust design optimization minimizes the product quality loss. In general, the product quality loss is described by using the first two statistical moments: mean and standard deviation. In this paper, a performance moment integration (PMI) method is proposed by using numerical integration scheme for output response to estimate the product quality loss. For the reliability part of the reliability-based robust design optimization, the performance measure approach (PMA) and its numerical method, hybrid-mean value (HMV) method, are used. New formulations of reliability-based robust design optimization are presented for three different types of robust objectives, such as smaller-the-better, larger-the-better, and nominal-the-better types. Examples are used to demonstrate the effectiveness of reliability-based robust design optimization using the proposed PMI method for different types of robust objective."
1317,"Laser scanners offer a fast and simple way of collecting large amounts of geometric data from real-world objects. Although this aspect makes them attractive for design and reverse engineering, the laser-scanner data is often noisy and not partitioned into meaningful surfaces. A good partitioning, or segmentation, of the scanner data has uses including feature detection, surface boundary generation, surface fitting, and surface reconstruction. This paper presents a method for segmenting noisy three-dimensional surface meshes created from laser-scanned data into distinct regions closely approximated by explicit surfaces. The algorithm first estimates mesh curvatures and noise levels and then uses the curvature data to construct seed regions around each vertex. If a seed region meets certain criteria, it is assigned a region number and is grown into a set of connected vertices approximated by a bicubic polynomial surface. All the vertices in a region are within known distance and surface normal tolerances from their underlying surface approximations. The algorithm works on noisy or smooth data and requires little or no user interaction. We demonstrate the effectiveness of the segmentation on real-world examples."
1318,"This study proposes a methodology that would enable a design educator or a design practitioner to optimally select a solid modeling software (solid modeler) for varying objectives. Tasks accomplished to propose the methodology include: (1) reviewing past literature to compile the criteria used for selecting solid modelers, (2) preliminary comparison of a number of solid modelers on established criteria, (3) running designed experiments for comparing the user performance on predetermined solid modeling functions, and (4) compiling the experience gained as a generic methodology. The application was completed over a two-year period while a systematic selection process was undertaken at The Pennsylvania State University (Penn State). This paper documents the entire selection process including the design performance data collected. In addition the performance data, the results of the preliminary preference data are also reported. The set of outcomes of the study is expected to aid companies and design educators in making solid modeler selection decisions."
1319,"In vehicle safety engineering, it is important to determine the severity of occupant injury during a crash. Computer simulations are widely used to study how occupants move in a crash, what they collide during the crash and thus how they are injured. The vehicle motion is typically defined for the occupant simulation by specifying a crash pulse. Many computer models used to analyze occupant kinematics do not calculate both vehicle motion and occupant motion at the same time. This paper presents a framework of response surface methodology for the crash pulse prediction and vehicle structure design optimization. The process is composed of running simulation at DOE sampling data points, generating surrogate models (response surface models), performing sensitivity analysis and structure design optimization for time history data (e.g., crash pulse). Within this framework, the engineer can perform DOE sampling, surrogate modeling, main effect plot within any time interval, and design optimization. Some recent applications are presented to demonstrate how these approaches are employed for a vehicle structure design."
1320,"The importance of sensitivity analysis in engineering design cannot be over-emphasized. In design under uncertainty, sensitivity analysis is performed with respect to the probabilistic characteristics. Global sensitivity analysis (GSA), in particular, is used to study the impact of variations in input variables on the variation of a model output. One of the most challenging issues for GSA is the intensive computational demand for assessing the impact of probabilistic variations. Existing variance-based GSA methods are developed for general functional relationships but require a large number of samples. In this work, we develop an efficient and accurate approach to GSA that employs analytic formulations derived from metamodels of engineering simulation models. We examine the types of GSA needed for design under uncertainty and derive generalized analytical formulations of GSA based on a variety of metamodels commonly used in engineering applications. The benefits of our proposed techniques are demonstrated and verified through both illustrative mathematical examples and the robust design for improving vehicle handling performance."
1321,"Preliminary design of a complex system often involves exploring a large design space. This may require repeated use of computationally expensive simulations. To ease the computational burden, surrogate models are built to provide rapid approximations of more expensive models. However, the surrogate models themselves are often expensive to build because they are based on repeated experiments with computationally expensive simulations. An alternative approach is to replace the detailed simulations with simplified approximate simulations, thereby sacrificing accuracy for reduced computational time. Naturally, surrogate models built from these approximate simulations will also be imprecise. A strategy is needed for improving the precision of surrogate models based on approximate simulations without significantly increasing computational time. In this paper, a new approach is taken to integrate data from approximate and detailed simulations to build a surrogate model to describe the relationship between output and input parameters. Experimental results from approximate simulations form the bulk of the data, and they are used to build a model based on a Gaussian process. The fitted model is then ‘adjusted’ by incorporating small amounts of data from detailed simulations to obtain a more accurate prediction model. The effectiveness of this approach is demonstrated with a design application for a cellular material that is used to cool a microprocessor. The emphasis is on the method and not on the results "
1322,"As engineering products become more complicated, collaboration among multi-disciplinary design teams that are separated by location, time and across organizations is becoming an increasingly difficult task. To be effective, collaboration requires exchanging, interpreting and integrating knowledge in various locations. According to a recent study, the cost of this breakdown in knowledge in the automotive industry alone is at least $1 billion per year. There has been a significant amount of research in recent years to improve the accessibility of knowledge during design. Very little has, however, been invested in format, flow and relationships of knowledge to support the "
1323,"To overcome the limitations of existing variance-based methods for Probabilistic Sensitivity Analysis (PSA) in design under uncertainty, a new PSA approach based on the concept of relative entropy is proposed. The relative entropy based method evaluates the impact of a random variable by measuring the divergence between two probability density functions of a response. The method can be applied both globally over the whole distribution of a performance response (called global response probabilistic sensitivity analysis–GRPSA) and in any regional range of a response distribution (called regional response probabilistic sensitivity analysis–RRPSA). The former is the most useful for studying variable impact on robust design objective, while the latter provides insight into reliability constraints. The proposed method is applicable to both the prior-design and post-design stages, for variable screening and uncertainty reduction, respectively. The proposed method is verified by numerical examples and industrial design cases."
1324,"Virtual prototyping attempts to replace physical models with virtual models for the purpose of design evaluation. One task a virtual prototyping environment can address is the accessibility of the components of a mechanical system. In this paper, we demonstrate a haptics-based virtual prototyping system for finding collision-free paths for moving models in complex polygonal environments. The system can handle models and environments with hundreds of thousands of triangles, and augments innate human talents at searching for collision-free paths."
1325,"Research into group decision-making suggests that, dependent on the information distributed prior to a group discussion, the decision and discussion content can be predicted. While the impact to group decision-making has been studied, its impact on collaborative activities such as design review has not been well investigated. A full factorial design of experiments (3×3, DOE) is conducted to investigate the influence of group cohesion and the awareness of the presence of unshared information among group members on design review effectiveness. The results suggest that awareness may have an effect on locating design issues by representation, functional group domain, and the total amount of design issues located."
1326,"Product design needs great team efforts from multi-disciplinary participants, even external partners, for collaborative problem solving. Design conflicts within and between functional teams do occur in such a collaborative design process. Detection and resolution of design conflicts through design conformance checking therefore becomes a critical activity in the joint design problem solving. This paper presents the development of a J2EE application prototype to support the STEP-based design conformance checking. A STEP-compliant information model has been specified to represent 3D CAD objects and other design information, while a knowledge representation model been proposed to describe design rules and constraints. The STEP objects and rule objects are managed and processed by the enterprise Java beans of a J2EE application server, which continuously applies the rule objects to the STEP objects and finally draws a conclusion for the design conformance checking. Application scenarios are discussed in the paper to illustrate the effectiveness of both the STEP/rule objects modeling approaches and the prototype system for support of the design compliance checking in distributed environment."
1327,"Much of today’s engineering analysis work consists of running complex computer codes (simulation programs), in which a vector of responses are obtained when values of design variables are supplied. To save time and effort in simulation, sampling (design of experiments) techniques are applied to help develop metamodels (empirical models or surrogate models) that can be used to replace the expensive simulations in future design stages. The usage of metamodels also helps designers to integrate inter-disciplinary codes and grasp the relationship between inputs and outputs. In this paper, we focus on a very important topic in studies of sampling and metamodeling techniques, i.e., the sequential design of experiments and metamodeling; the research question is: How to design sequential computer experiments to get accurate metamodels? After discussion of design and metamodeling strategies, a Sequential Exploratory Experimental Design (SEED) method is developed to help identify data points at different stages in metamodeling. Given limited resources, it is expected that more accurate metamodels can be developed with SEED. A single-variable example is used to help illustrate the SEED method."
1328,"Utilizing available assembly resources can greatly reduce the development time and cost for platforms and new product family members. This paper presents a method to utilize existing assembly plant resources, during the development of new product family members, by comparing existing assembly plant with feasible assembly processes. An assembly sequence design space that can be used to perform new family member process and existing plant utilization trade-off study is presented in the paper. The assembly sequence design space is combinatorial in nature. This research seeks to provide a systematic method to represent and reason with combinatorial design spaces in which these assembly process combinations lie. Models that capture effects of constraints on these spaces, explicitly represent feasible regions, and efficiently enumerate designs within this space are investigated. Application of the assembly sequence space to perform assembly plant utilization trade-off study is demonstrated using a coffeemaker family."
1329,"We describe a sketch-based interface designed to provide engineers with a computer environment similar to pen and paper. With our interface, users can construct functional engineering models simply by drawing sketches on a computer screen. Unlike paper sketches, however, our interface allows users to interact with their sketches in real time to modify existing objects and add new ones. To demonstrate the utility of our system, we have developed a sketch-based interface for designing and analyzing simple vibratory mechanical systems. The technical contributions of our work include: (1) a sketch parsing method for automatically locating the distinct graphical symbols in a sketch, (2) a general-purpose, trainable symbol recognizer, and (3) special purpose prerecognizers that consider shape information and make use of drawing conventions."
1330,"The transfer of design data among different CAD systems or subsequent downstream analysis applications is critically important to the acceleration of the product development cycle. Since each vendor has its own proprietary native file format, this transfer of data among differing systems is difficult at best. International standards such as IGES and STEP have evolved to address this challenge, but they are generally not sufficiently explicit. Each vendor writes its own “flavor” of the standard that other applications may not understand. This paper bridges a gap between disparate systems by developing a strategy to assess the completeness and robustness of models represented in IGES or STEP format, and a technique to either repair the representation or add missing information so that a downstream application can properly interpret it. The method ensures that the receiving system gets a full and accurate NURBS-based representation: the original surface, the corresponding full complement of model space trim curves, and the corresponding full complement of parameter space trim curves. With all the information present, the downstream system is more likely to receive the information it requires to interpret the model."
1331,"Reverse engineering of mechanical systems often begins with large datasets produced from laser scanning of physical artifacts. Commonly it is necessary to remove noise and filter them; however, selecting noisy regions and preserving sharp edges on desired features is difficult using standard GUI interfaces. We demonstrate a haptic interface for marking and preserving features in noisy data and for performing local smoothing operations. The force-feedback provides a natural interface for these operations."
1332,"Configuration design and packaging optimization correspond to finding the optimal placement of a series of objects in a system while satisfying functional requirements and minimizing criteria. The research presented in this paper is applied to the configuration design of vehicles and more specifically the US Army FMTV (Family of Medium Tactical Vehicles). This paper presents the latest developments of the design methodology based on a multiple objective genetic algorithm. A swap operator specifically constructed for packaging problems is presented. For this multi-criteria problem, the goal of the methodology is to explore the objective space in order to find multiple Pareto designs. This research is motivated by the need to add non-conventional components, namely, a fuel cells auxiliary power unit and its associated modules, on the FMTV. Three objectives are considered: vehicle dynamic behavior, maintainability, and survivability. The methodology uses several in-house and commercial analysis packages to evaluate the fitness of the evolving designs. The new approach is systematically evaluated comparing the results to those of the original method with respect to its ability in generating the Pareto front of the multi-criteria design problem."
1333,"We describe a new general algorithm for the automated design, analysis and repair of nonlinear physical systems. The process iterates a two-phase exploration-estimation cycle. The exploratory phase seeks a new improvement or test to perform to the system based on some initial internal model. The estimation phase performs the suggested operation and observes the outcomes; it then improves the internal model so as to explain all observations so far. This process relies on very few, targeted, and carefully planned interactions with the physical systems. We describe an implementation of this method using two evolutionary algorithms, where the exploratory phase uses a simulator to evolve improvements or tests, and the estimation phase uses observations to evolve the simulator itself. We demonstrate this algorithm for analysis, design and repair of electromechanical systems."
1334,"Failures have impact on the society as well as on the entity that caused the failure. The size of impact varies with each case. Although large scale accidents will cause great impact on the society and the originator, it is not only the size of failure that determines the size of impact on the originator. When an unethical corporate misconduct is revealed, the company at times will disappear by loss of business or administrative disposition. To measure the impact of failures on the originator and to help make decisions of whether to disclose or cover the event, we defined two quantities associated with failure; “Profit of Failure” and “Loss of Failure”. The former measures monetary gain for covering up a failure, and the later the loss in case the failure is disclosed. We applied our method of calculation to 18 cases of past failures and identified different groups. Some cases, the loss exceeds the profit and business owners are encouraged to publicly disclose the event as soon as it internally becomes known to keep the damage smaller. In other cases, the loss is smaller than the profit and in which case, business owners may decide to cover up the event. Even in the later case, business owners may want to disclose the event anyway because recently changed regulations protect the whistle blowers better and for ethical reasons."
1335,"The concurrent consideration of design and manufacturing requirements at the early stages of design is one of the cited challenges in microsystem design. In this paper, we take the first steps, through an example, towards addressing these issues through the use of the compromise Decision Support Problem (cDSP). The cDSP is a domain-independent hybrid multiobjective decision support formulation utilized in engineering design. The design of a parylene microchannel for a microscale gas chromatography system is refined using the cDSP. The objective is to adjust the geometry of the microchannel to create a satisficing design for one fabrication goal and two performance goals. The cDSP is utilized for five scenarios, one in which all three goals are given equal priority, one for each of three goals when they are given first priority, and one in which the performance goals are given equal priority. We are more interested in demonstrating the method than the results per se. Our goal is to show how microsystem designers can use the cDSP to gain some insight into how these goals interact and how design decisions can be made with this insight."
1336,"The combined interference matrix and the combined contact matrix of product components with connection matrices, special connection relations, the disassembly sequence matrix, instability of sub-assemblies, changes of sub-assembly disassembly direction or tools, platform and the effect of gravity are investigated to obtain the optimum disassembly process. This methodology improves Huang and Huang’s method. Software is generated and two products were used as examples. Results of optimum disassembly processes were same as those obtained from other researchers. Therefore, the methodology is feasible for study of disassembly processes of products, and software can reduce the computer memory and time."
1337,"Monte Carlo simulation is commonly employed to evaluate system probability of failure for problems with multiple failure modes in design under uncertainty. The probability calculated from Monte Carlo simulation has random errors due to limited sample size, which create numerical noise in the dependence of the probability on design variables. This in turn may lead the design to spurious optimum. A probabilistic sufficiency factor (PSF) approach is proposed that combines safety factor and probability of failure. The PSF represents a factor of safety relative to a target probability of failure, and it can be calculated from the results of Monte Carlo simulation (MCS) with little extra computation. The paper presents the use of PSF with a design response surface (DRS), which fits it as function of design variables, filtering out the noise in the results of MCS. It is shown that the DRS for the PSF is more accurate than DRS for probability of failure or for safety index. The PSF also provides more information than probability of failure or safety index for the optimization procedure in regions of low probability of failure. Therefore, the convergence of reliability-based optimization is accelerated. The PSF gives a measure of safety that can be used more readily than probability of failure or safety index by designers to estimate the required weight increase to reach a target safety level. To reduce the computational cost of reliability-based design optimization, a variable-fidelity technique and deterministic optimization were combined with probabilistic sufficiency factor approach. Example problems were studied here to demonstrate the methodology."
1338,"This paper introduces a novel cross-evaluation matrix (CEM) rating based approach for the development of Pareto efficient frontiers and the finding of discrete sets of globally non-inferior design sets. This work, based on concepts from data envelopment analysis (DEA), can facilitate the enumeration of design candidates in a multi-criteria formulation. In addition, it is expected that the resulting design sets will provide the basis for the establishment of a value system and subsequent preference based rank ordering of expected outcomes in the single-criterion formulation. A unique feature of this cross-evaluation matrix approach is its ability to handle problems without requiring a priori tradeoff formulation or multiattribute model development. As such, its application does not require assignment of a set of a priori weight constants as in many well-established Pareto-optimal generating methods, nor does it need any a priori information of the global minimum or maximum of the attribute functions. Recognizing that the enumeration of multiple discrete solution alternatives can be best achieved in a parallel computation environment, the implementation in this work is executed with the aid of a genetic algorithm strategy. The effectiveness of the integrated approach in yielding Pareto-optimal candidate design sets under different scenarios are studied in the context of illustrative examples, including two engineering case studies, and the results are discussed."
1339,"In this work, we propose an integrated framework for probabilistic optimization that can bring both the design objective robustness and the probabilistic constraints into account. The fundamental development of this work is the employment of an inverse reliability strategy that uses percentile performance for assessing both the objective robustness and probabilistic constraints. The percentile formulation for objective robustness provides an accurate probabilistic measure for robustness and more reasonable compound noise combinations. For the probabilistic constraints, compared to a traditional probabilistic model, the proposed formulation is more efficient since it only evaluates the constraint functions at the required reliability levels. The other major development of this work is a new search algorithm for the Most Probable Point of Inverse Reliability (MPPIR) that can be used to efficiently evaluate the performance robustness and percentile performance in the proposed formulation. Multiple techniques are employed in the MPPIR search, including the steepest decent direction and an arc search. The algorithm is applicable to general non-concave and non-convex functions of system performance with random variables following any continuous distributions. The effectiveness of the MPPIR search algorithm is verified using example problems. Overall, an engineering example on integrated robust and reliability design of a vehicle combustion engine piston is used to illustrate the benefits of the proposed method."
1340,"In this paper, we investigate and extend a method of selecting among a set of concepts or alternatives using multiple, potentially conflicting criteria. This method, called the Hypothetical Equivalents and Inequivalents Method (HEIM), has been shown to avoid the many pitfalls of already existing methods for such problems, such as pair-wise comparison, ranking methods, rating methods, and weighted sum approaches. The existence of multiple optimal sets of attribute weights based on a set of stated preferences is investigated. Using simple visualization techniques, we show that there is a range of weights that satisfy the constraints of HEIM. Depending on the attribute weights used, multiple possible alternative winners could exist. The visualization techniques, coupled with an indifference point analysis, are then used to understand the robustness of the solution obtained and determine the appropriate additional constraints necessary to identify a single robust optimal alternative."
1341,"Reliability analysis methods are commonly used in engineering design, in order to meet reliability and quality measures. An accurate and efficient computational method is presented for reliability analysis of engineering systems at both the component and system levels. The method can easily handle implicit, highly nonlinear limit-state functions, with correlated or non-correlated random variables, which are described by any probabilistic distribution. It is based on a constructed response surface of an indicator function, which determines the “failure” and “safe” regions, according to the performance function. A Monte Carlo simulation (MCS) calculates the probability of failure based on a response surface of the indicator function, instead of the computationally expensive limit-state function. The Cross-Validated Moving Least Squares (CVMLS) method is used to construct the response surface of the indicator function, based on an Optimum Symmetric Latin Hypercube (OSLH) sampling technique. A number of numerical examples highlight the superior accuracy and efficiency of the proposed method over commonly used reliability methods."
1342,"In Reliability-Based Design (RBD), uncertainties usually imply for randomness. Nondeterministic variables are assumed to follow certain probability distributions. However, in real engineering applications, some of distributions may not be precisely known or uncertainties associated with some uncertain variables are not from randomness. These nondeterministic variables are only known within intervals. In this paper, a method of RBD with the mixture of random variables with distributions and uncertain variables with intervals is proposed. The reliability is considered under the condition of the worst combination of interval variables. In comparison with traditional RBD, the computational demand of RBD with the mixture of random and interval variables increases dramatically. To alleviate the computational burden, a sequential single-loop procedure is developed to replace the computationally expensive double-loop procedure when the worst case scenario is applied directly. With the proposed method, the RBD is conducted within a series of cycles of deterministic optimization and reliability analysis. The optimization model in each cycle is built based on the Most Probable Point (MPP) and the worst case combination obtained in the reliability analysis in previous cycle. Since the optimization is decoupled from the reliability analysis, the computational amount for MPP search is decreased to the minimum extent. The proposed method is demonstrated with a structural design example."
1343,"The use of probabilistic optimization in structural design applications is hindered by the huge computational cost associated with evaluating probabilistic characteristics, where the computationally expensive finite element method (FEM) is often used for simulating design performance. In this paper, a Sequential Optimization and Reliability Assessment (SORA) method with analytical derivatives is applied to improve the efficiency of probabilistic structural optimization. With the SORA method, a single loop strategy that decouples the optimization and the reliability assessment is used to significantly reduce the computational demand of probabilistic optimization. Analytical sensitivities of displacement and stress functionals derived from finite element formulations are incorporated into the probability analysis without recurring excessive cost. The benefits of our proposed methods are demonstrated through two truss design problems by comparing the results with using conventional approaches. Results show that the SORA method with analytical derivatives is the most efficient with satisfactory accuracy."
1344,"Mechanical fatigue subject to external and inertia transient loads in the service life of mechanical systems often leads a structural failure due to accumulated damage. Structural durability analysis that predicts the fatigue life of mechanical components subject to dynamic stresses and strains is a compute intensive multidisciplinary simulation process, since it requires an integration of several computer-aided engineering tools and large amount of data communication and computation. Uncertainties in geometric dimensions due to manufacturing tolerances cause the indeterministic nature of fatigue life of the mechanical component. Due to the fact that uncertainty propagation to structural fatigue under transient dynamic loading is not only numerically complicate but also extremely expensive, it is a challenging task to develop structural durability-based design optimization process and reliability analysis to ascertain whether the optimal design is reliable. The objective of this paper is development of an integrated CAD-based computer-aided engineering process to effectively carry out the design optimization for a structural durability, yielding a durable and cost-effectively manufacturable product. In addition, a reliability analysis is executed to assess the reliability for the deterministic optimal design."
1345,"There are two sorts of uncertainty inherent in engineering design, "
1346,"A robust optimization of an automobile valvetrain is presented where the variation of engine performances due to the component dimensional variations is minimized subject to the constraints on mean engine performances. The dimensional variations of valvetrain components are statistically characterized based on the measurements of the actual components. Monte Carlo simulation is used on a neural network model built from an integrated high fidelity valvetrain-engine model, to obtain the mean and standard deviation of horsepower, torque and fuel consumption. Assuming the component production cost is inversely proportional to the coefficient of variation of its dimensions, a multi-objective optimization problem minimizing the variation in engine performances and the total production cost of components is solved by a multi-objective genetic algorithm (MOGA). The comparisons using the newly developed Pareto front quality index (PFQI) indicate that MOGA generates the Pareto fronts of substantially higher quality, than SQP with varying weights on the objectives. The current design of the valvetrain is compared with two alternative designs on the obtained Pareto front, which suggested potential improvements."
1347,"Robust design is a methodology for improving the quality of a product or process by minimizing the effect of variations in the inputs without eliminating the causes of those variations. In robust design, the best design is obtained by solving a multicriteria optimization problem, trading off the nominal performance against the minimization of the variation of the performance measure. Because these methods often combine the two criteria with a weighted sum or another fixed aggregation strategy, which are known to miss Pareto points, they may fail to obtain a desired design. To overcome this inadequacy, a more comprehensive preference aggregation method is combined into robust design. Two examples are presented to illustrate the effectiveness of the proposed method."
1348,"We present a method for estimating the parameter sensitivity of a design alternative for use in robust design optimization. The method is non-gradient based: it is applicable even when the objective function of an optimization problem is non-differentiable and/or discontinuous with respect to the parameters. Also, the method does not require a presumed probability distribution for parameters, and is still valid when parameter variations are large. The sensitivity estimate is developed based on the concept that associated with each design alternative there is a region in the parameter variation space whose properties can be used to predict that design’s sensitivity. Our method estimates such a region using a worst-case scenario analysis and uses that estimate in a bi-level robust optimization approach. We present a numerical and an engineering example to demonstrate the applications of our method."
1349,"In an effort to improve customization for today’s highly competitive global marketplace, many companies are utilizing product families to increase variety, shorten lead-times, and reduce costs. The key to a successful product family is the product platform from which it is derived either by adding, removing, or substituting one or more modules to the platform or by scaling the platform in one or more dimensions to target specific market niches. This nascent field of engineering design research has matured rapidly in the past decade, and this paper provides an extensive review of the research activity that has occurred during that time to facilitate product platform design and optimization. Techniques for identifying platform leveraging strategies within a product family are reviewed along with optimization-based approaches to help automate the design of a product platform and its corresponding family of products. Examples from both industry and academia are presented throughout the paper to highlight the benefits of platform-based product development, and the paper concludes with a discussion of promising research directions to help bridge the gap between planning and managing families of products and designing and manufacturing them."
1350,"This paper discusses the optimal design of common components used for a class of products. While simultaneously designing multiple products has become an important concept in manufacturing in these days, alliances involved in such activities are extended from the traditional form. This means the existence of a chance that an integrator designs a set of components apart from particular products or a supplier commonalizes components independently from integrators. That is, any methodology for simultaneously designing a set of components becomes necessary behind ones for simultaneously designing a set of products. This paper formulates the design problem of common components as an optimization problem, investigates the condition of optimal design through the tradeoff among the level of system-level performance, the number of different components, etc. Then a computational procedure is configured for optimizing the commonalization of components apart from designing a particular set of products by using multivariate analysis, an optimization code based on mini-max operation and a genetic algorithm for constrained nonlinear mathematical programming. Finally the proposed optimization procedure is preliminarily applied to a design problem of liftgate dumpers for passenger cars for demonstrating the meaning of the levels of optimal design and tradeoff structure."
1351,"Designing family of products require analysis and evaluation of performance for the entire product family. In the past, products were mainly mass-produced hence the use of CAD/CAE was restricted to developing and analyzing individual products. Since the products offered using a platform approach include a variety of products built upon a common platform, CAD/CAE tools need to be explored further to assist in customization of products according to the customer needs. In this paper we investigate the development of a Product Family FEA (PFFEA) module that can support FEA analysis of user customized product families members. Customer specifications for family members are gathered using the internet, users are allowed to scale and change configurations of products. These specifications are then used to automatically generate 3D solid models of the product and then perform FEA to determine feasibility of the customer specified product. In this paper, development of the PFFEA module is illustrated using a family of lawn trimmer and edger. The PFFEA module uses Pro/E to generate the solid model and ANSYS as the base FEA software."
1352,"Product family design involves carefully balancing the commonality of the product platform with the distinctiveness of the individual products in the family. While a variety of optimization methods have been developed to help designers determine the best design variable settings for the product platform and individual products within the family, production costs are thought to be an important criterion to choose the best platform among candidate platform designs. Thus, it is prerequisite to have an appropriate production cost model to be able to estimate the production costs incurred by having common and variant components within a product family. In this paper, we propose a production cost model based on a production cost framework associated with the manufacturing activities. The production cost model can be easily integrated within optimization frameworks to support a Decision-Based Design approach for product family design. As an example, the production cost model is utilized to estimate the production costs of a family of cordless power screwdrivers."
1353,"In this paper, a methodology is presented to determine the optimum number of product platforms to maximize overall product family profit with simplifying assumptions. This methodology is attempting to aid various manufacturing industries who are seeking ways to reduce product family manufacturing costs and development times through implementation of platform strategies. The methodology is based on a target market segment analysis, market leader’s performance vs. price position, and a two-level optimization approach for platform and variant designs. The proposed methodology is demonstrated for a hypothetical automotive vehicle family that attempts to serve seven different vehicle market segments. It is found that the use of three distinct platforms maximizes overall profit by pursuing primarily a horizontal leveraging strategy."
1354,"Recent advances in rapid prototyping technology make it a useful tool in assessing the early designs of not only individual parts but also assemblies. These "
1355,"Recent developments in Computer Aided Design (CAD) have drastically reduced overall design cycle time and cost. In this paper, wirePATH, a new method for rapid direct tooling, is presented. By using specialized interactive segmentation computer software and wire electrical discharge machining (wire EDM), wirePATH can reduce manufacturing time and cost for injection molds, casting patterns, and dies. Compared to other conventional-mold making methods, wirePATH can reduce fabrication time by as much as 40 to 70%. Wirepath can use a combination of wire EDM and other processes. Our method provides a new means to produce a greater variety in products by changing only portions of the tooling. Segments allow a part of a mold to be replaced to accommodate design changes and repair. WirePATH enables new applications of wire EDM to more complex shapes by bridging the gaps between CAD, our method, wire EDM and conventional manufacturing processes."
1356,"A layered manufacturing technique that uses electrophotography is described where powder is picked up and deposited using a charged photoconducting surface and deposited layer by layer on a build platform. A test bed was designed and constructed to study the application of electrophotography to layered manufacturing. The test bed can precisely deposit powder in the desired shape on each layer. The feasibility of printing powder layer by layer was demonstrated. The electric field required to transfer the powder on to the platform (or onto previously printed layers) was studied. It was found that corona charging the top layer of the part is necessary to continue printing powder as the part height increases."
1357,"Multi-Piece molds, which consist of more than two mold pieces, are capable of producing very complex parts—parts that cannot be produced by the traditional molds. The tooling cost is also low for multi-piece molds, which makes it an ideal candidate for pre-production prototyping and bridge tooling. However, designing multi-piece molds is a time-consuming task. This paper describes geometric algorithms for automated design of multi-piece molds. A Multi-Piece Mold Design Algorithm (MPMDA) has been developed to automate several important mold-design steps: finding parting directions, locating parting lines, creating parting surfaces, and constructing mold pieces. MPMDA constructs mold pieces based on global accessibility analysis results of the part and therefore guarantees the disassembly of the mold pieces. A software system has also been developed and successfully tested on several complex industrial parts."
1358,"Even though the machining process has been integrated to the Multi-Axis Laser Aided Manufacturing Process (LAMP) System in order to get good surface finish functional parts [1], the quality of parts produced by the LAMP system is still very much dependent upon the choice of deposition paths. [2] Raster motion paths are replaced by offset spiral-like paths, which are discussed in this paper. Most commercial CAD/CAM packages are feature-based, and their use requires the effort and expertise of the user. The shape has to be decomposed into manufacturing features before the software packages can generate the paths. [3] Path planning has long been studied as discussed in this paper. There are still some problems associated with the previous algorithms and also assumptions are usually made. [6, 7, 27] An algorithm for directly generating offset edges, which can be developed to be the deposition paths, is presented in this paper. The skeleton of a layer or a slice of a 3-D CAD drawing is first generated. Based on that skeleton, the offset edges are incrementally constructed. This paper focuses on the characteristics of skeleton and offset edges as well as the construction algorithm for those edges. Simulations are used to verify this method."
1359,"Finding effective and interactive tools for extracting freeform shape information continues to be a challenging problem in reverse engineering. Given a freeform shape, it may be constructed by adding one shape, named a pattern, to another. In this paper, an approach of extracting the pattern by template fitting is proposed. By similarity analysis, a user-defined region of interest in the shape can be matched, or fitted, to a shape template. According to the different methods in constructing the shape, several different kinds of R 3  to R 3  functions are defined. With those functions, the original shape is mapped to the fitted shape template, thus the template can be used as a “ruler” to measure the region of interest in the shape. The measuring results, e.g., the extracted pattern, can be generated through an inverse mapping, thus it can be used in the future design. Several implementations were conducted based on ACIS®  and OpenGL®  in order to verify the proposed method. It is also described how the proposed technique can be applied in practical shape modeling applications."
1360,"This paper presents a new decomposition method for partitioning complex design problems based on an extended Hierarchical Cluster Analysis (HCA). After a complex design problem is represented using a function-parameter incidence matrix, this new decomposition method allows transforming the originally unorganized matrix into a block-angular form matrix. By means of the resulting matrix, a coordination part and design blocks can be further identified and obtained. In particular, the extended HCA plays an important role in this method, contributive to aligning all non-zero elements, also known as 1s elements, of the matrix along its main diagonal as compactly as possible. As such, a post process, called Partition Point Analysis (PPA), can be further applied to the matrix to finally form the coordination part and the related design blocks, subject to such decomposition criteria as block size and coordination size limits. A powertrain design example is employed for illustration of the decomposition method newly developed."
1361,"Achieving the dimensional integrity for a complex structural assembly is a demanding task due to the manufacturing variations of parts and the tolerance relationship between them. While assigning tight tolerances to all parts would solve the problem, an economical solution is taking advantage of small motions that joints allow, such that critical dimensions are adjusted "
1362,"A method is presented for synthesizing multi-component structural assemblies with maximum structural performance and manufacturability. The problem is posed as a relaxation of decomposition-based assembly synthesis [1,2,3], where both topology and decomposition of a structure are regarded as variables over a ground structure with non-overlapping beams. A multi-objective genetic algorithm [4,5] with graph-based crossover [6,7,8], coupled with FEM analyses, is used to obtain Pareto optimal solutions to this problem, exhibiting trade-offs among structural stiffness, total weight, component manufacturability (size and simplicity), and the number of joints. Case studies with a cantilever and a simplified automotive floor frame are presented, and representative designs in the Pareto front are examined for the trade-offs among the multiple criteria."
1363,"The objective of this paper is to develop techniques to sub-group multiple responses of a system. The proposal is intended for explorative preprocessing steps prior to the execution of a more formal multi-objective optimization. The sub-grouping techniques are developed based on orthonormal expansions. Factor Analysis (FA) and Total Sensitivity Analysis (TSA) are suggested when the relationships among responses are linear and non-linear, respectively. Automotive road Noise Vibration and Harshness (NVH) data is used to illustrate the application of the proposed methodologies."
1364,"In the context of concurrent engineering, this paper presents a quite innovative approach to the collaborative optimisation process, which couples a multi-objective genetic algorithm with an asynchronous communication tool. To illustrate this methodology, three European companies’ collaboration on the optimisation of a ship hull is described. Our study demonstrates that when multi-objective optimisation is carried out in a distributed manner it can provide a powerful tool for concurrent product design."
1365,"This paper presents a methodology to perform structural topology design optimization for crashworthiness considering a prescribed and safe structural behavior through the dynamic equilibrium equation. This implementation, called here "
1366,"Engineering design decisions have more value and lasting impact if they are made in the context of the enterprise that produces the designed product. Setting targets that the designer must meet is often done at a high level within the enterprise, with inadequate consideration of the engineering design embodiment and associated cost. For complex artifacts produced by compartmentalized hierarchical enterprises, the challenge of linking the target setting rationale with the product instantiation is particularly demanding. The previously developed analytical target cascading process addresses the problem of translating supersystem design targets into design targets for all systems in a multilevel hierarchically structured product, so that local targets are consistent with each other and allow top targets to be met as closely as possible. In this article the process of rigorously setting the supersystem targets in an enterprise context is explored as a model-based approach termed “analytical target setting.” The effectiveness of linking analytical target setting and cascading is demonstrated in an automotive truck vehicle example."
1367,"Recent improvements in vehicle propulsion systems, such as hybrid electric and fuel cells, demand new configuration solutions that may be totally different from conventional designs. Packaging of vehicle components is still a new area of research. This paper describes a configuration optimization method based on a multiple objective genetic algorithm. The method is applied to configuration optimization of a mid-size truck, in which two objectives are considered: ground clearance and dynamic behavior. A vehicle packaging model was developed using the commercial CAD software, ACIS, to analyze interference among vehicle components. An eight-degree of freedom model was used to analyze the dynamic behavior of a given configuration for the J-turn maneuver. Parallel computation technology was also incorporated to accelerate the optimization process. The applicability of this method is discussed and exemplified with the design of a mid-size truck for two propulsion systems: conventional diesel and hybrid diesel electric. A set of Pareto solutions is generated in which tradeoff decisions can be made to select a final design."
1368,"A new mathematical model for representing the geometric variations of lines is extended to include form and accumulation (stackup) of tolerances in an assembly. The model is compatible with the ASME/ANSI/ISO Standards for geometric tolerances. Central to the new model is a Tolerance-Map© , a hypothetical volume of points which corresponds to all possible locations and variations of a segment of a line (the axis) which can arise from tolerances on size, position, orientation, and form. Every Tolerance-Map is a convex set in a metric space. The new model makes stackup relations apparent in an assembly, and these can be used to allocate size and orientational tolerances; the same relations also can be used to identify sensitivities for these tolerances. All stackup relations can be met for 100% interchangeability or for a specified probability. Much of the detail in this paper would probably reside internally to software for designers, yet would not be included in the interface; its workings should be invisible to the user."
1370,"The paper aims at dimensioning a mechanism in order to make it robust, and synthesizing its dimensional tolerances. The design of a mechanism is supposed to be robust when its performance is as little as sensitive as possible to variations. First, a distinction is made between three sets to formulate a robust design problem; (i) the set of Design Variables ("
1371,"Knowledge attrition in the cutting tool industry can be mitigated and also the efficiency of the design process can be improved if past design solutions, which are the embodiment of rich expert knowledge, are made easily accessible to the designer for reuse. An effort is underway to realize this desirable situation at Widia Valenite, a company concerned with the design and manufacture of cutting tools. An ontology of cutting tool design exists, however this ontology is too detailed for designers who want quick access to past designs to satisfy new design problems. The potential of this ontology is only realized if a reuse view of the ontology is taken to retrieve design from a database of designs. This work presents the development and validation of the information requirements, in the form of descriptor terms, selected from the ontology for which designers make data entry that allows relevant past designs to be recalled. After these descriptor terms are identified, their success in recalling relevant past designs is demonstrated within a CBR application."
1372,"This paper presents a useful measuring method for rapidly verifying geometric errors using a double-readheads planar encoder system (DRPES). With the calculated model and the various specified test paths, the proposed measuring method provide the rapid performance, simplicity of setup, low cost and pre-process verification of CNC machine tools. Complete setups and procedures for geometric error verification are clearly presented. Experimental results showed that more information on analyzing the types of errors can be obtained and the performance of verification can be further improved. The proposed system completes the geometric verification of a CNC machine tool within one hour."
1373,"A design methodology is presented which decreases cycle time and opportunities for error through automated execution of a consistent design procedure. The Product Design Generator (PDG) methodology is useful for existing devices with a well-established design process. Two such examples are given, the Thermomechanical In-plane Microactuator (TIM) and the micro force gauge. In both PDGs, the designer inputs a finite set of requirements which automatically updates parametric design models. The necessary analyses are then executed, and product artifacts such as a CAD file, technical document, and test procedures are generated. The application of this method reduces the opportunities for error by ten times for the TIM PDG and five times for the micro force gauge PDG. The design cycle time is reduced from hours to minutes for both devices."
1374,"Knowledge-based design is a concept for the computer-aided provision and application of different representations of knowledge along the product development process. In this paper, a knowledge taxonomy is proposed, possible applications of knowledge-based design and the resulting benefits are discussed as well as open questions and research needs."
1375,"It is necessary for product teams with diverse expertise to communicate during the product development process, notably during design reviews. As this expertise may be distributed across different geographic locations of an organization, design review teams are facing new challenges in effective communication. This paper presents the results of a controlled user study devised to examine the effectiveness of various communication methods for design reviews. Speech only, text only, and free communication methods were chosen to simulate current technologies commonly used in situations of geographic distribution. Primary results from the study include: group design reviews were approximately twice as effective as individual design reviews; free communication produced greater perceived effectiveness than speech only communication, speech only communication produced greater perceived effectiveness than text only communication; and certain personality factors, such as extroversion and intuition, may have contributed to higher productivity in design review teams."
1376,"The development of on-board car safety systems requires an accidentology knowledge base for the development of new functionalities as well as their improvement and evaluation. The Knowledge Discovery in accident Database (KDD) is one of the approaches allowing the construction of this knowledge base. However, considering the complexity of the accident data and the variety of their sources (biomechanics, psychology, mechanics, ergonomics, etc.), the analytical methods of the KDD (clustering, classification, association rules etc.) should be combined with expert approaches. Indeed, there is background knowledge in accidentology which exists in the minds of accidentologist experts and which is not formalized in the accident database. The aim of this paper is to develop a Knowledge Representation Model (KRM) intended to incorporate this knowledge in the KDD process. The KRM is implemented in a knowledge-based system, which provides an expert classification of the attributes characterizing an accident. This expert classification provides an efficient tool for data preparation in a KDD process. Our method consists of combining the modeling systemic approach of complex systems and the modeling cognitive approach KOD (Knowledge Oriented Design) in knowledge engineering."
1377,"Shape knowledge indexing is crucial in both design reuse and knowledge engineering, in which the pivot issue is to establish the unique representation of the invariant shape properties. Treating the shape of the region of interest as a surface signal, in this paper, a local shape-indexing scheme is developed by applying the affine invariant nature of the Fourier spectrum of the spatial shape distribution. The shape-coding scheme is theoretically proven being strictly invariant under affine transformations. A framework applying the invariant shape code in shape knowledge indexing is presented. Associated examples and the quantity analysis results are provided to justify the robustness, simplicity, and adaptability of the proposed shape knowledge-indexing scheme. Further, the proposed approach could be regarded as an alternative choice to represent local shape knowledge, especially for that of freeform features."
1378,"The reality of the Knowledge Management (KM) joins in a multiplicity of ends and situations. In the scientific literature, KM seems to appear as a sort of more or less unified and more or less generative “field of research” of specialists’ community. Nevertheless, a detailed analysis of the scientific production relative to the Knowledge Management shows essentially that the management of knowledge and competence became a preoccupation in a big part of sciences and techniques. This is translated by a big number of actors (university, consultant, industrial, etc.) constituting a community of preoccupations. It deals also with a profusion of publications, various networks and a rising offer of specialized trainings. However, the big variety of points of view and interpretations which join to the knowledge and competence management calls up to a lot of caution as for any other scientific discipline, and invites to understand the senses which are given to them. Indeed, no fundamental scientific result appeared really: literature supplies only approaches which hold more feeling than it is important, or very pragmatic applications, sending back mostly to particular cases of companies."
1380,"One of the objectives of concurrent engineering has been to integrate more and more knowledge as soon as possible during the product development process. In such a method and owing to designer creativity, new design solutions are carried out. Design alternatives then appear; the differences can be relative to the functions, the technology, the materials or the manufacturing process as well. This paper presents some first specifications in modelling those design alternatives. Much more design solutions can then be kept in designers’ mind instead of focusing on the "
1381,"Engineering design is essentially a collaborative decision-making process that requires rigorous evaluation, comparison and selection of design alternatives and optimization from a global perspective on the basis of different classes of design criteria. Increasing design knowledge and supporting designers to make right and intelligent decisions can achieve the improvement of the design and design efficiency. This paper develops a knowledge-based decision support model and framework that can be extensively applied for an engineering system, which allows for the seamless / smooth integration of collaborative product development with optimal product performance. The developed hybrid robust design decision support model quantitatively incorporates qualitative design knowledge and preferences of multiple, conflicting attributes stored in a knowledge repository so that a better understanding of the consequences of design decisions can be achieved from an overall perspective. Two new concepts and mechanisms, transforming bridge and regulatory switch, are introduced in integration of decision support models. The results of this work provide a framework for an efficient decision support environment involving distributed resources to shorten the realization of products with optimal life-cycle performance and competitiveness. The developed methodology and framework are generic and flexible enough to be used in a variety of decision problems. Case application and studies for concept evaluation and selection in design for mass customization are provided."
1382,"Engineering changes are inevitable in a product development life cycle. The requests for engineering changes can be due to new customer requirements, emergence of new technology, market feedback, or variations of components and raw materials. Each change generates a level of impact on costs, time to market, tasks and schedules of related processes, and product components. Change management tools available today focus on the management of document and process changes. Assessments of change impact are typically based on the “rule of thumb”. Our research has developed a methodology and related techniques to quantify and analyze the impact of engineering changes to enable faster and more accurate decision-making in engineering change management. Reported in this paper are investigations of industrial requirements and fundamental issues of change impact analysis as well as related research and techniques. A framework for a knowledge-supported change impact analysis system is proposed. Three critical issues of system implementation, namely integrated design information model, change plan generator and impact estimation algorithms, are addressed. Finally the benefits and future work are discussed."
1383,"The primary obstacle in automated design for crashworthiness is the heavy computational resources required during the optimization processes. Hence it is desirable to develop efficient optimization algorithms capable of finding good solutions without requiring too many model simulations. This paper presents an efficient mixed discrete and continuous optimization algorithm, Mixed Reactive Taboo Search (MRTS), and its application to the design of a vehicle B-Pillar subjected to roof crush conditions. The problem is sophisticated enough to explore the MRTS’ capability of identifying multiple local optima with a single optimization run, yet the associated finite element model (FEM) is not too large to make the computational resources required for global optimization prohibitive. The optimization results demonstrated that a single run of MRTS identified a set of better designs with smaller number of simulation runs, than multiple runs of Sequential Quadratic Programming (SQP) with several starting points."
1384,"Passenger vehicle crashworthiness is one of the essential vehicle attributes. According to National Highway Traffic Safety Administration (NHTSA), there were over six million vehicle crashes in the United States in the year 2000, which claimed the lives of more than forty thousand persons. Vehicle crashworthiness is difficult to satisfy in a manner appeasing to other design decisions about the vehicle. This paper aims at developing a novel methodology for crashworthiness optimization of vehicle structures. Based on observations of the manner of structural deformation, the authors propose the abstraction of the actual vehicle structure, which is to be represented as a linkage mechanism having special nonlinear springs at the joints. The special springs are chosen to allow the motion of the mechanism to capture the overall motion of the actual vehicle structure. It thus becomes possible to optimize the mechanism, which is an easier task than directly optimizing the vehicle structure. A realization of the optimized mechanism is then performed to obtain an equivalent structure, and then direct optimization of the realized structure is performed for further tuning. The study presented shows the success of the proposed approach in finding better designs than direct optimization while using comparatively less computational resources."
1385,"This paper presents a method where a multi objective optimization technique is used together with response surface methods in order to support crashworthiness design. As in most engineering design problems there are several conflicting objectives that have to be considered when formulating a design problem as an optimization problem. Here this is exemplified by the desire to minimize the intrusion into the passenger compartment area and simultaneously obtain low maximum acceleration during vehicle impact. These two objectives are naturally conflicting, since low maximum acceleration implies large intrusion. The contribution of this paper is to show a successful application of a set of existing methods to solve a real world engineering problem. The paper also presents methods of illustrating the results obtained from the multi-objective optimization."
1386,"A design sensitivity analysis of high frequency structural-acoustic problem is formulated and presented. The Energy Finite Element Method (EFEM) is used to predict the structural-acoustic responses in high frequency range, where the coupling between the structural and acoustic domain are modeled by using radiation efficiency. The continuum design sensitivity formulation is derived from the governing equation of EFEM and the discrete method is applied in the variation of the structural-acoustic coupling matrix. The direct differentiation and adjoint variable method are both developed for the sensitivity analysis, where the difficulty of the adjoint variable method is overcome by solving a transposed system equation. Parametric design variables such as panel thickness and material damping are considered for sensitivity analysis, and the numerical sensitivity results show excellent agreement comparing with the finite difference results."
1387,"The effect of tempering (artificial aging) on the axial crush strength of four cell extruded rectangular aluminum alloy (AA) 6061 and 6063 tubes is developed in this report. Increasing the aging time from the press quenched condition increases the flow strength of the material and also increases the axial energy absorbed up to a point when the material showed fracture. Good predictions were made between experimental, theoretical, and numerical mean crush loads for various tempers of AA6061 and AA6063, except in the cases where a large amount of fracture was present. A recommended temper time of three hours was obtained for AA6063, with an increase in mean crush load of 60%. A recommended temper time of six hours was obtained for AA6061 with an increase in mean crush load of 40%. These results will be useful in future aluminum automotive body projects, both for their predictive capability, and for the temper recommendations."
1388,"Direct application of most optimization techniques, especially Multi-Objective Evolutionary Algorithms (MOEAs) that require many response evaluations, is computationally prohibitive for most real-world engineering simulations. In this paper, an approximation-assisted approach to multi-objective optimization of computationally expensive response functions is presented. We employ a Bayesian approach, referred to as "
1389,"In machine tool development, control software engineering has reached a cost proportion of over fifty percent of the total development costs. Highly customized user requirements and the compulsion to shorten development cycles accompany the need to master risen quality requirements of the mechatronic product machine tool. This enforces a strategy change from prevailing sequential engineering to concurrent engineering. The paper proposes a Hardware-in-the-Loop simulation environment as an interdisciplinary discussion platform to virtually implement, evaluate and optimize a machine tool throughout all stages of development."
1390,"This paper describes the framework of nonlinear finite element model validation for vehicle crash simulation. Several methods were developed to quantify transient time-domain data (functional data). The concept of correlation index was proposed to determine the degree to which a model is an accurate representation of the real world from the perspective of the intended uses of the model. The methodologies developed in this paper can also be used for CAE model updating, parameter tuning, and model calibration."
1391,"Kriging is a popular metamodeling technique for analysis of computer experiment. However, the likelihood function near the optimum is flat in some situations, and this may lead to very large random variation in the maximum likelihood estimate. To overcome this difficulty, a penalized likelihood approach is proposed for the kriging model. The proposed method is particularly important in the context of a computationally intensive simulation model where the number of simulation runs must be kept small. We demonstrate the proposed approach for the reduction of piston slap, an unwanted engine noise due to piston secondary motion. Issues related to practical implementation of the proposed approach are discussed."
1392,"A variety of metamodeling techniques have been developed in the past decade to reduce the computational expense of computer-based analysis and simulation codes. Metamodeling is the process of building a “model of a model” that provides a fast surrogate for a computationally expensive computer code. Common metamodeling techniques include response surface methodology, kriging, radial basis functions, and multivariate adaptive regression splines. In this paper, we present Support Vector Regression (SVR) as an alternative technique for approximating complex engineering analyses. The computationally efficient theory behind SVR is presented, and SVR approximations are compared against the aforementioned four metamodeling techniques using a testbed of 22 engineering analysis functions. SVR achieves more accurate and more robust function approximations than these four metamodeling techniques and shows great promise for future metamodeling applications."
1393,"Metamodeling approach has been widely used due to the high computational cost of using high-fidelity simulations in engineering design. The accuracy of metamodels is directly related to the experimental designs used. Optimal experimental designs have been shown to have good “space filling” and projective properties. However, the high cost in constructing them limits their use. In this paper, a new algorithm for constructing optimal experimental designs is developed. There are two major developments involved in this work. One is on developing an efficient global optimal search algorithm, named as enhanced stochastic evolutionary (ESE) algorithm. The other is on developing efficient algorithms for evaluating optimality criteria. The proposed algorithm is compared to two existing algorithms and is found to be much more efficient in terms of the computation time, the number of exchanges needed for generating new designs, and the achieved optimality criteria. The algorithm is also very flexible to construct various classes of optimal designs to retain certain structural properties."
1394,"Modern engineering design problems often involve computation-intensive analysis and simulation processes. Design optimization based on such processes is desired to be efficient, informative, and transparent. This work proposes a rough set based approach that can identify multiple subregions in a design space, within which all of the design points are expected to have a performance value equal to or less than a given level. The rough set method is applied iteratively on a growing sample set. A novel termination criterion is also developed to ensure a modest number of total expensive function evaluations to identify these sub-regions and search for the global optimum. The significances of the proposed method are two folds. First, it provides an intuitive method to establish the mapping from the performance space to the design space; given a performance level, its corresponding design region(s) can be identified. Such a mapping can be used to explore and visualize the entire design space. Second, it can be naturally extended to a global optimization method. It also bears potentials for more abroad applications to problems such as robust design optimization. The proposed method was tested with a number of test problems."
1395,"The use of kriging models for approximation and global optimization has been steadily on the rise in the past decade. The standard approach used in the Design and Analysis of Computer Experiments (DACE) is to use an Ordinary kriging model to approximate a deterministic computer model. Universal and Detrended kriging are two alternative types of kriging models. In this paper, a description on the basics of kriging is given, highlighting the similarities and differences between these three different types of kriging models and the underlying assumptions behind each. A comparative study on the use of three different types of kriging models is then presented using six test problems. The methods of Maximum Likelihood Estimation (MLE) and Cross-Validation (CV) for model parameter estimation are compared for the three kriging model types. A one-dimension problem is first used to visualize the differences between the different models. In order to show applications in higher dimensions, four two-dimension and a 5-dimension problem are also given."
1396,"The presence of black-box functions in engineering design, which are usually computation-intensive, demands efficient global optimization methods. This work proposes a new global optimization method for black-box functions. The global optimization method is based on a novel mode-pursuing sampling (MPS) method which systematically generates more sample points in the neighborhood of the function mode while statistically covers the entire search space. Quadratic regression is performed to detect the region containing the global optimum. The sampling and detection process iterates until the global optimum is obtained. Through intensive testing, this method is found to be effective, efficient, robust, and applicable to both continuous and discontinuous functions. It supports simultaneous computation and applies to both unconstrained and constrained optimization problems. Because it does not call any existing global optimization tool, it can be used as a standalone global optimization method for inexpensive problems as well. Limitation of the method is also identified and discussed."
1397,"Data clustering methods can be a useful tool for engineering design that is based on numerical optimization. The clustering method is an effective way of producing representative designs, or clusters, from a large set of potential designs. These methods have recently been applied to the clustering of Pareto-optimal solutions from multi-objective optimization. The results presented here focus on the application of clustering to single objective optimization results. In the case of single objective optimization, the method is used to determine the clusters in a set of quasi-optimal feasible solutions generated by an optimizer. A data clustering procedure based on an evolutionary method is briefly described. The number of clusters is determined automatically and need not be known a priori. The method is demonstrated by application to the results of a turbine blade coolant passage shape optimization problem. The solutions are transformed to a lower-dimensional space for better understanding of their variance and character. Engineering information, such as the shapes and locations of the internal passages, is supported by the visualization of clustered solutions. The clustering, transformation, and visualization methods presented in this study might be applicable to the increasing interpretation demands of design optimization."
1398,"We propose an optimization method for a semi-active shock absorber for use in aircraft landing gear, in order to handle variations in the maximum vertical acceleration of an aircraft during landing caused by the variation of the aircraft mass due to the variations in the number of passengers, and the amounts of cargo and fuel. In this optimization, the maximum vertical acceleration of an aircraft is set as an objective function to be minimized. Design variables searched in the first step of this optimization are discrete orifice areas formed by the outer surface of a hollow metering pin and a hole in the semi-active shock absorber. The design variable searched in the second step is a compensating orifice area which is controlled based on the mass variation. Using the optimum target orifice area obtained in the second step, we optimally determine a practical orifice area that is controlled by a stepping motor. The optimizations for a passive shock absorber and for semi-active shock absorbers with target and practical orifice areas indicate that the semi-active shock absorbers can handle aircraft mass variation much better than the optimum passive shock absorber. Furthermore, the robustness of the optimum practical orifice area controlled by a stepping motor is shown via simulation."
1399,"Constraint Programming (CP) is a promising technique for managing uncertainty in conceptual design. It provides efficient algorithms for reducing, as quickly as possible, the domains of the design and performance variables while complying to the engineering and performance constraints linking them. In addition, CP techniques are suitable to graphically represent 3D projections of the complete design space. This is a useful capability for a better understanding of the product concept’s degrees of freedom and a valuable alternative to optimization based upon the construction of an arbitrary preference aggregation function. Unfortunately, one of the main impediments for using "
1400,"A methodology is presented for studying the effects of automobile emission policies on the design decisions of profit-seeking automobile producers in a free-entry oligopoly market. The study does not attempt to model short-term decisions of specific producers. Instead, mathematical models of engineering performance, consumer demand, cost, and competition are integrated to predict the effects of design decisions on manufacturing cost, demand, and producer profit. Game theory is then used to predict vehicle designs that producers would have economic incentive to produce at market equilibrium under several policy scenarios. The methodology is illustrated with three policy alternatives for the small car market: corporate average fuel economy (CAFE) regulations, carbon dioxide emissions taxes, and diesel fuel vehicle quotas. Interesting results are derived, for example, it is predicted that in some cases a stiffer regulatory penalty can result in lower producer costs because of competition. This mathematical formulation establishes a link between engineering design, business, and marketing through an integrated optimization model that is used to provide insight necessary to make informed environmental policy."
1401,"This paper presents a new model for structural topology optimization. We represent the structural boundary by a level set model that is embedded in a scalar function of a higher dimension. Such level set models are flexible in handling complex topological changes and are concise in describing the boundary shape of the structure. Furthermore, a gradient-based procedure leads to a numerical algorithm for the optimum solution satisfying specified constraints. The result is a 3D topology optimization technique with outstanding flexibility of handling topological changes, without resorting to homogenization-based relaxations that are widely used in the recent literature."
1402,"The standard problem of finding the optimal layout of structural material associated with maximum stiffness is expanded to include consideration of thermal criteria. The problem is posed as a three-phase layout problem where the phases include an insulating or fire retardant material and an unknown distribution of heat sources, in addition to the structural material. The model used is simple, yet results suggest that the introduction of measures to control the temperature in the structure when subjected to significant heat transfer rates can result in layouts that differ substantially from solutions where thermal issues are ignored."
1403,"The present work introduces a new methodology for solving the topology optimization problem of a compliant gripper. A hybrid optimization technique is developed using simulated annealing as a random search method, while the simplex method (Nelder-Mead) is used as a direct search method. A new modified technique of motion from one search point to another based on the discrete nature of adding and/or removing a structural member is proposed. The traditional continuous simulated annealing technique is used to find the members’ heights. A discrete uni-variant search method is adopted following the simulated annealing and before the simplex method. This corresponds to about 14% of the number used in the old method and in the previous work in the literature, and about 86% of the optimization time is saved. The optimum design of a compliant mechanism is conducted for maximum flexibility and stiffness using the developed hybrid optimization technique."
1404,"This study utilizes the topology optimization with the finite element method and the simulated annealing algorithms to optimize the structure and the dynamic performance of a bike frame with dampers. Design domains, loadings and boundary conditions of bike frames are defined. Joint locations of a damper with the front and the rear frames and the joint location of the front and the rear frames are considered as design variables. The transient response and the acceleration of a bike on a sinusoid curved road surface are analyzed. Effects of the joint locations and the stiffness of frames on the dynamic performance are studied. The structural topology of frames and joint locations of a bike are recommended to improve the dynamic performance."
1405,"A robust topology exploration method is under development in which robust design techniques are extended to the early stages of a design process when a product’s layout or topology is determined. The performance of many designs is strongly influenced by "
1406,"Computer Aided Engineering (CAE) has been successfully utilized in mechanical industries, but few mechanical design engineers use CAE tools that include structural optimization, since the development of such tools has been based on continuum mechanics that limit the provision of useful design suggestions at the initial design phase. In order to mitigate this problem, a new type of CAE based on classical structural mechanics, First Order Analysis (FOA), has been proposed. This paper presents the outcome of research concerning the development of a structural topology optimization methodology within FOA. This optimization method is constructed based on discrete and function-oriented elements such as beam and panel elements, and sequential convex programming. In addition, examples are provided to show the utility of the methodology presented here for mechanical design engineers."
1407,"To accommodate the dual objectives of many engineering applications, one to minimize the mean compliance for the stiffest structure under normal service condition and the other to maximize the strain energy for energy absorption during excessive loadings, topology optimization with a multi-material model is applied to the design of energy absorbing structure in this paper. The effective properties of the three-phase material are derived using a spherical micro-inclusion model. The dual objectives are combined in a ratio formation. Numerical examples from the proposed method are presented and discussed."
1408,"The distributed compliance and smooth deformation field of compliant mechanisms provide a viable means to achieve shape morphing in many systems, such as flexible antenna reflectors and morphing aircraft wings. We previously developed a systematic synthesis approach to design shape morphing compliant mechanisms using Genetic Algorithm (GA). However, the design variable definition, in fact, allows the generation of invalid designs (disconnected structures) within the GA. In this research, we developed a load path representation to include the structure connectivity information into the design variables, thus improving the GA efficiency. The number of design variables is also independent of the number of elements in the finite element model that is used to solve for the structural deformation. The shape morphing synthesis approach, incorporating this path representation, is demonstrated through two examples, followed by discussions on further refinements."
1409,"Finite element analysis has become a key technology for a design process of manufacturing industry. A hexahedral mesh is focused, because using a hexahedral mesh increases the quality of analysis. However it is very difficult problem to generate high quality hexahedral meshes, and there are many challenging research topics. Our goal is to develop a method to generate hexahedral meshes automatically to general volumes. Our method uses an intermediate model to recognize the input volume. The intermediate model is defined in the integer 3-dimensional space, and faces of the intermediate model are vertical to coordinate axes. Hexahedral mesh is generated by dividing the intermediate model into integer grids, and blocks of grids are projected into original volume. In this paper, we describe the method to generate a topology of the intermediate model. We use face clustering technique to generate the topology of the intermediate model. The faces of the input volume are clustered into 6 types; according to 3 coordinate axes and its direction, and clustered faces will be the faces of the intermediate model."
1410,"Even if researches on free-form surface deformation have produced a lot of various methods, very few of them are able to really control the shape in an adequately interactive way and most of them propose a unique solution to the underconstrained system of equations coming out of their deformation models. In our approach, where the deformation is performed through the static equilibrium modification of a bar network coupled to the surface control polyhedron, different minimizations have been proposed to overcome these limits and form a set of representative parameters that can be used to give access to the desired shape. In this paper, a reformulation of the optimization problem is presented thus enabling the generation of new shapes based on a common set of minimization criteria. Such a modification widens the variety of shapes still verifying the same set of constraints. When generalizing some of these minimizations the user has access to a continuous set of shapes while acting on a single parameter. Taking advantage of the reformulation, anisotropic surface behaviors are considered too and briefly illustrated. In addition, the possibility of defining several minimizations on different areas of a surface is sketched and aims at giving the user more freedom in local shape definition. The whole minimizations proposed are illustrated through examples resulting from our surface deformation software."
1411,"This paper presents a method for removing geometric noise from triangulated meshes while preserving edges and other intended geometric features. The method iteratively updates the position of each mesh vertex with a position lying on a locally fitted bivariate polynomial. The user selects the width of the vertex neighborhood, the order of the fitted polynomial, and a threshold angle to control the effects of the smoothing operation. To avoid smoothing over discontinuities, the neighborhood can be eroded by removing vertices with normals that deviate beyond a threshold from the estimated median normal of the neighborhood. The method is particularly suitable for use on laser scanner generated meshes of automobile outer body panels. Smoothing methods used on these meshes must allow "
1412,"Occlusion detection is a fundamental and important problem in optical sensor inspection planning. Many view-planning algorithms have been developed for optical inspection, however, few of them explicitly develop practical algorithms for occlusion detection. This paper presents a hierarchical space partition approach that divides both positional and surface normal space of an object for fast occlusion detection. A k-d tree is used to represent this partition. A novel concept of "
1413,"In a multi-axis hybrid manufacturing system, it is necessary to utilize a machining process to improve surface accuracy and guarantee overall geometry after the deposition process. Due to the complexity of the multi-axis system, it is necessary to find proper orientations of cutting tools for the CNC machine to finish surface machining. This paper presents an algorithm to find collision-free surface machining toolpath for a given workpiece. The concept of the 2-D visibility map and its properties are discussed. The algorithm to compute the 2-D visibility map is presented. With the help of the 2-D visibility map, an optimal a collision free tool approaching direction can be easily decided. Also the type of the surface machining toolpath for different types of surfaces is decided based on topological information and the machining toolpath (CL data for milling tool). The developed planning scheme has been tested via machine simulations and has shown that it can be effectively applied to cutter-path generation for multi-axis surface machining."
1414,"A taxonomy that classifies issues affecting the collaborative design process is proposed. These factors, which may inhibit or facilitate the progress or success of a design team, provide a description of collaborative design situations. The taxonomy includes top-level attributes of team composition, communication, distribution, design approach, information, and nature of the problem. An example collaborative design situation is used to illustrate the application of the taxonomy. This taxonomy is an initial step towards the creation of new collaborative support agent-based tools structured upon a fundamental understanding of the collaborative process with a theoretical foundation."
1415,"The decomposition and coordination of decisions in the design of complex engineering systems is a great challenge. Companies who design these systems routinely allocate design responsibility of the various subsystems and components to different people, teams or even suppliers. The mechanisms behind this network of decentralized design decisions create difficult management and coordination issues. However, developing efficient design processes is paramount, especially with market pressures and customer expectations. Standard techniques to modeling and solving decentralized design problems typically fail to understand the underlying dynamics of the decentralized processes and therefore result in suboptimal solutions. This paper aims to model and understand the mechanisms and dynamics behind a decentralized set of decisions within a complex design process. By using concepts from the fields of mathematics and economics, including Game Theory and the Cobweb Model, we model a simple decentralized design problem and provide efficient solutions. This new approach uses numerical series and linear algebra as tools to determine conditions for convergence of such decentralized design problems. The goal of this paper is to establish the first steps towards understanding the mechanisms of decentralized decision processes. This includes two major steps: studying the convergence characteristics, and finding the final equilibrium solution of a decentralized problem. Illustrations of the developments are provided in the form of two decentralized design problems with different underlying behavior."
1416,"In Computer-Aided Design, when creating the solid model of a part, the designer knows how the part will interface with other parts, however this information is not stored with the part model. For catalog parts, it would be useful to be able to embed this assembly information into the part model in order to automate the process of applying mating constraints, to reduce the assembly designer’s effort or to allow for automated exploration of alternative configurations. This research evaluates and compares different schemes for capturing the attributes of assembly interfaces and appending that information to solid models. The schemes studied involve (i) different combinations of ways to constitute ports and include labeling, (ii) different bases for determining port compatibility with respect to design intent, and (iii) different ways of evaluating connectability with respect to part geometry. The scheme that we conclude is best minimizes the number of ways the system will try to put parts together at the expense of effort from the solid model designer to provide more information."
1417,"As CAE (Computer Aided Engineering) applications become increasingly precise, the knowledge and technical skill required to operate such applications has become more highly specialized. However, such tools have not been utilized in the initial design process of mechanical products, where designers cannot construct detailed analytical models. This paper proposes a cross-sectional shape design optimization system that supports the initial design process for bar structures. The cross-sectional design problem is formulated as an eight-objective optimization problem that can be solved using genetic algorithms. A method for generating cross-sectional shapes satisfying designer-required characteristics is also proposed. These methods, which reduce the number of trial and error processes and product design failures, are expected to enable shortened product development lead-times."
1418,"We have developed a data visualization interface that facilitates a design by shopping paradigm, allowing a decision-maker to form a preference by viewing a rich set of good designs and use this preference to choose an optimal design. Design automation has allowed us to implement this paradigm, since a large number of designs can be synthesized in a short period of time. The interface allows users to visualize complex design spaces by using multi-dimensional visualization techniques that include customizable glyph plots, parallel coordinates, linked views, brushing, and histograms. As is common with data mining tools, the user can specify upper and lower bounds on the design space variables, assign variables to glyph axes and parallel coordinate plots, and dynamically brush variables. Additionally, preference shading for visualizing a user’s preference structure and algorithms for visualizing the Pareto frontier have been incorporated into the interface to help shape a decision-maker’s preference. Use of the interface is demonstrated using a satellite design example by highlighting different preference structures and resulting Pareto frontiers. The capabilities of the design by shopping interface were driven by real industrial customer needs, and the interface was demonstrated at a spacecraft design conducted by a team at Lockheed Martin, consisting of Mars spacecraft design experts."
1419,"This paper presents a framework for representing and deploying error-proofs ("
1420,"A microfactory is a system that can perform manufacturing processes within a very limited space such as a desktop. However, design optimization of miniature machine tools in microfactories have not been studied enough. Since the miniature machine tool designs are not supported by existing design experience as normal machine tools are, design guidelines for miniature machine tool are strongly demanded. And a design tool to analyze machine performance without prototyping will be also necessary because the miniature machines have wider design choices than normal machine tools have, based on its small size and less constraints. This paper focuses on a robust design tool combining form-shaping theory with the Taguchi method, to roughly estimate performance of miniature machine tools at its conceptual design stages. The effort not only identifies critical design parameters that have significant influence on the machining tolerance, but also determines which structure has the best theoretical performance. The paper tells that the proposing design evaluation method can help machine tool designers in determining the optimum structure of a miniature machine tool. The study also realizes two designs of miniature mills to measure positioning errors. The measurement ensures the design evaluation method can predict the machine performance well enough for usage in conceptual design stages. The paper concludes that the design evaluation method is applicable to a systematic miniaturization of a machine tool."
1421,"The embodiment design stage involves determination of geometric sizes, key parameter values, and matching of component variables to system requirements. This embodiment design stage can be parametrically represented as an iterative design-redesign problem. This paper presents a domain independent characterization of such problems; the characterization includes problem definition, design relations/procedures, and measures of goodness. The paper also discusses representation issues and solution techniques for design-redesign problems. Design tasks are differentiated as domain independent or problem specific and the scope of each design task with respect to the characterization is delineated. A Design Shell implemented on the basis of this characterization is described. This shell can be configured for evaluating designs in any domain. A case study illustrates the use of this Design Shell in characterizing a specific design problem and exploring its design space."
1422,"Mere collection of failure information and accident records do not effectively relay the knowledge associated with the case to the reader. We propose to collect data in a structured manner so the message is better transferred to the information receiver. We further developed a scheme that records the essence of each failure case in a sequence of predefined phrases displayed to the recorder in a hierarchy of phrases. We call the sequence the “scenario” of the event. Arranging the phrases in descending steps and supplementing it with an illustration and a key knowledge sentence composes the visual summary of the case. A glance at the visual summary and reading the scenario steps generate a good image of the case in the receiver’s mind. Among the predefined hierarchical phrases, we call those that express the cause of the event, failure cause phrases. Recording high level failure cause phrases from the hierarchy forces the event recorder to evaluate the root cause of the failure. To the top and second level (in the phrase hierarchy) failure cause phrases, we assigned 5-space vector components to characterize each phrase in terms of “knowledge”, “carefulness”, “judgment”, “organization”, and “nature”. This vector characterization of the failure cause phrases with the scenario allows us to further characterize each failure case as a linear combination of the predefined phrases. Once each failure case has its vector characterization, we can evaluate its similarity with other cases. Also, if we find the characterization of an individual, group, or organization in the same 5-space, we can warn about failure cases with similar characteristics that are likely to happen. The method is powerful in predicting failures so they can be avoided before happening."
1423,"This paper presents a design methodology for the thermal design and packaging of hybrid electronic-mechanical products. In this work, tight integration between ECAD and MCAD was achieved through the use of a web-based tool used in managing the concurrent designs, called the Domain Unified CAD Environment (DUCADE). This work also reduced the amount of time required for thermal simulation by using a web-based Design of Experiment Testbed (DOET) to systematically determine effects of varying system parameters before full-scale computational fluid dynamics (CFD) thermal modeling was performed. The design process began by proper selection of material, manufacturing process and cooling methods, based on electrical and integrated circuit design. DUCADE was then set up to monitor couplings between the various domains. This was followed by computer-aided-design and computer-aided-engineering of the mechanical package. In computer-aided-engineering, DOET was first used to determine variables that had significant effect on the thermal system response. Detailed CFD thermal simulations were then carried out in FLOTHERM only focusing on variables that the DOET determined to have strong effect. Rapid prototypes were fabricated to refine the design before final production. Each step of the cycle was tested and demonstrated through a case study on the design of the Berkeley Emulation Engine (BEE) which involved multi-disciplinary electrical, mechanical, and thermal design."
1424,"A practical modeling method for predicting vibration characteristics of turbine generator stator frames was developed. The structural parts that compose a stator frame were categorized into three groups: parts that affect ring-mode vibration as mass, parts that affect it as stiffness, and parts that affect it as both. A proper boundary condition, the value of a modal damping ratio, and an accurate representation of exciting forces were examined. The modeling method was then applied to another turbine generator. It was found that the predicted natural frequency agrees with measured one within only a 3% error. Based on the modeling method, a vibration analysis system for design was developed."
1425,"Poor interfacial properties between reinforcement fibers and a Polymethylmethacrylate (PMMA) matrix may result in debonding between them, which is a major failure mechanism for fiber reinforced bone cement. Optimization of the shape of the fibers can improve load transfer between the fibers and PMMA matrix, thereby providing maximum overall strength performance. This paper presents a procedure for structural shape optimization of short reinforcement fibers using finite element analyses. The composite is modeled by a representative element composed of a single short fiber embedded in PMMA matrix. In contrast to most previous work on this subject, contact elements are employed between the fiber and the matrix to model a low strength interface. Most models assume a perfect bond. Residual stress, due to matrix cure shrinkage and/or thermal stresses, is also included in the model. The design objective is to improve the stiffness of the composite. The results presented show that a threaded end, short fiber results in mechanical interlock between the fibers and the PMMA matrix, which helps to bridge matrix cracks effectively and improve the stiffness of the composite."
1426,"Reasoning about relationships among design constraints can facilitate objective and effective decision making at various stages of engineering design. Exploiting dominance among constraints is one particularly strong approach to simplifying design problems and to focusing designers’ attention on critical design issues. Three distinct approaches to constraint dominance identification have been reported in the literature. We lay down the basic principles of these approaches with simple examples and we apply these methods to a practical linear electric actuator design problem. The identification of dominance along with the use of Interval Propagation and Monotonicity Analysis leads to an optimal solution for a particular design configuration of the linear actuator. Identification of dominance also provides insight into the design of linear actuators, which may lead to effective decisions at the conceptual stage of the design."
1427,"A continuum-based shape and configuration design sensitivity analysis method for a finite deformation elastoplastic shell structure with frictionless contact has been developed. Shell elastoplasticity is treated based on the projection method that performs the return mapping on the subspace defined by the zero-normal stress condition. An incrementally objective integration scheme is used in the context of finite deformation shell analysis, wherein stress objectivity is preserved for finite rotation increments. The penalty regularization method is used to approximate the contact variational inequality. The material derivative concept is used to develop continuum based design sensitivity. The design sensitivity equation is solved without iteration at each converged load step. Numerical implementation of the proposed shape and configuration design sensitivity analysis is carried out using the meshfree method. The accuracy and efficiency of the proposed method is illustrated using numerical examples."
1428,"When a complex electromechanical system fails, the troubleshooting procedure adopted is often complex and tedious. No standard methods currently exist to optimize the sequence of steps in a troubleshooting process. The ad hoc methods generally followed are less than optimal methods and can result in high maintenance costs. This paper describes the use of behavioral models and multistage decision-making models in Bayesian networks for representing the troubleshooting process. It discusses advantages in using these methods and the difficulties in implementing them. An "
1429,"We present an optimization method of shock absorbers for analyzing the effect of mass variation of an impacting body on the response of a system including the shock absorber, such as landing gears of aircraft, elevators and coupling devices for railroad cars. The system including the optimum semi-active shock absorber is compared with those including two kinds of optimum passive shock absorbers regarding the variation of the acceleration of the impacting body. The maximum value and each of the maximum accelerations of the different masses of the impacting body are set as objective functions to be minimized, respectively. The design variables of these optimizations are reciprocals of resisting coefficients of the shock absorber. As a result of the optimizations, it is clarified that the optimum semi-active shock absorber can cope with the mass variation of the impacting body better than the optimum passive shock absorbers."
1430,"Global optimization of mechanical design problems using heuristic methods such as Simulated annealing (SA) and genetic algorithms (GAs) have been able to find global or near-global minima where prior methods have failed. The use of these nongradient based methods allow the broad efficient exploration of multimodal design spaces that could be continuous, discrete or mixed. From a survey of articles in the ASME Journal of Mechanical Design over the last 10 years, we have observed that researchers will typically run these algorithms in continuous mode for problems that contain continuous design variables. What we suggest in this paper is that computational efficiencies can be significantly increased by discretizing all continuous variables, perform a global optimization on the discretized design space, and then conduct a local search in the continuous space from the global minimum discrete state. The level of discretization will depend on the complexity of the problem, and becomes an additional parameter that needs to be tuned. The rational behind this assertion is presented, along with results from four test problems."
1431,"Optimal packaging problems occur in many industries and lend themselves to be analyzed by computer-based approaches due to their fast and effective solutions. Unlike other packaging in commercial industries, turret packaging has been done mainly by hand because positioning equipment within the turret with crew is hard to be automated. However, recent development of automated ammunition loading system makes crew unnecessary in turret and this makes it possible to automate the packaging process. This paper introduces a computational methodology that uses genetic algorithms as a search algorithm and solves tank turret packaging problems by integrating part placements with performance analysis of design requirements."
1432,"In some cases of developing a new product, response surface of an objective function is not always single peaked function, and it is often multi-peaked function. In that case, designers would like to have not oniy global optimum solution but also as many local optimum solutions and/or quasi-optimum solutions as possible, so that he or she can select one out of them considering the other conditions that are not taken into account priori to optimization. Although this information is quite useful, it is not that easy to obtain with a single trial of optimization. In this study, we will propose a screening of fitness function in genetic algorithms (GA). Which change fitness function during searching. Therefore, GA needs to have higher flexibility in searching. Genetic Range Genetic Algorithms include a number of searching range in a single generation. Just like there are a number of species in wild life. Therefore, it can arrange to have both global searching range and also local searching range with different fitness function. In this paper, we demonstrate the effectiveness of the proposed method through a simple benchmark test problems."
1433,"In this paper, a global optimization technique based on the Adaptive Response Surface Method (ARSM) is integrated with a Control Volume Finite Element Method (CVFEM) for thermofluid optimization. The objective of the optimization is to improve the thermal effectiveness of an aircraft de-icing strategy by re-designing the cooling bay surface shape. By optimizing objective function in terms of the de-icing strategy and shape of the intake scoop, the best performance of the helicopter engine is achieved. This design problem is implemented on two different physical models. One model involves a heat conduction finite element analysis (FEA) process and the other combines the heat conduction and potential fluid flow FEA processes. Based on the comparison between the ARSM predicted results and the plotted objective function, it is observed that the integrated technique provides an effective method for thermofluid optimization. It also shows that the ARSM has a good flexibility to work with the computationally intensive process, e.g. CVFEM, and, potentially, could be developed and applied to the multidisciplinary design optimization (MDO) due to its open structure."
1434,"In design problems, designers have to decide many properties of products to satisfy requirements from users or market. The designers also have to consider the environment under the use of the products and the environment is often unpredictable or difficult to be determined in detail. The optimization techniques are useful to support the designers to decide the properties of the product. However, it is required before the application of the optimization techniques to formulate mathematical models and it is difficult to formulate the all properties of products, for examples preferences of the customer. In this situation, it seems to be useful to derive several solutions that equip the variety or diversity about the value of design variables or objective functions. In this paper, the new method to derive several solutions using immune algorithms is described. The proposed method equips the interaction mechanism between the design parameter and the environment parameters. Through some numerical examples of the structural design problems and job-shop scheduling problems, the effectiveness is confirmed."
1435,"Design optimization is becoming and increasingly important tool for design. In order to have an impact on the product development process it must permeate all levels of the design in such a way that a holistic view is maintained through all stages of the design. One important area is in the case of optimization based on simulation, which generally requires non-gradient methods and as a consequence direct-search methods is a natural choice. The idea in this paper is to use the design optimization approach in the optimization algorithm itself in order to produce an efficient and robust optimization algorithm. The result is a single performance index to measure the effectiveness of an optimization algorithm, and the COMPLEX-RF optimization algorithm, with optimized parameters."
1436,"A global optimization method for continuous design variables called as Generalized Random Tunneling Algorithm is proposed. Proposed method is called as “Generalized” random tunneling algorithm because this method can treat the behavior constraints as well as the side constraints. This method consists of three phases, that is, the minimization phase, the tunneling phase, and the constraints phase. In the minimization phase, mathematical programming is used, and heuristic approach is introduced in the tunneling and constraint phases. By iterating these phases, global minimum is found. The characteristics of mathematical programming and heuristic approaches are included in the proposed method. Global minimum which may lie on the boundary of constraints is easily found by proposed method. Proposed method is applied to mathematical and structural optimization problems. Through numerical examples, the effectiveness and validity of proposed method have been confirmed."
1437,"Application of the compliant design methodology to manipulators has held the promise of delivering manipulators with many significant advantages, including low cost, small size, low backlash and friction, and high positioning accuracy. This approach has been demonstrated in part by Canfield et. al., [1] to a class of three-degree-of-freedom manipulators based on a specific parallel architecture topology. In [1], the authors’ intent was to develop two compliant manipulators that exhibit several of the features associated with compliant devices. However, upon review of the manipulators resulting from this work it is observed that many of the benefits that were expected were lost at some point in the design process, resulting in manipulators that were large, expensive and suffered significantly from required assembly and inaccuracies in manufacture. This paper will revisit the problem addressed in [1], using the modeling tools demonstrated in that paper but will present several improved development measures that will result in manipulators that exhibit multiple features promised by compliant devices. The resulting manipulators will then be compared against the manipulators from [1] with a summary of the performance and characteristics of each given and evaluated."
1438,"Discrete parameterization using full or partial ground structures of truss/frame elements is not appropriate for domain representation as they do not map all points in the continuum and can lead to dangling or overlapping elements in the optimal topology. Existing continuum parameterization using units cells with holes, ranked microstructures or penalized Young’s modulus (SIMP model) mainly have problems like the appearance of checkerboard patterns and stiffness singularity regions. This is probably due to point-contact between diagonally placed cells and such regions can be avoided by using higher order elements, perimeter constraints or filtering schemes which result in additional computational load on the optimization procedures. An edge connectivity throughout is ensured when using a "
1439,"In this paper, meso and micro scale electro-thermally compliant electro mechanical systems are synthesized for strength with polysilicon as the structural material. Local temperature and/or stress constraints are imposed in the topology optimization formulation. This is done to keep the topology thermally intact and also to keep local stresses below their allowable limit. Relaxation performed on both temperature and stress constraints allows them to be ignored when the material densities approach their non-existing states. Noting that both local constraints can be large in number with the number of cells, an active constraint strategy is employed. Honeycomb parameterization, which is a staggered arrangement of hexagonal cells, is used to represent the design region. This ensures at least a common edge between any two neighboring cells and thus avoids the appearance of both checkerboard and zero-stiffness singularities without any additional computational load."
1441,"A new type of compliant approximate dwell mechanism design is introduced. This new compliant mechanism includes an initially straight flexible beam and a flexible arc. Approximate dwell motion is obtained by incorporating the snap-through buckling motion of the flexible arc. Load-deflection curves of flexible mechanism components are modeled by fitting polynomials to the analytical nonlinear large deflection response. The kinematic synthesis of the compliant mechanism is done quasi-statically using loop closure theory and large deflection relations of flexible parts."
1442,"The use of Coulomb’s friction law with the principles of classical rigid body dynamics introduces mathematical inconsistencies. Specifically, the forward dynamics problem can have no solutions or multiple solutions. In these situations an explicit model of the contact compliance at the contact point can resolve these difficulties. In this paper, we introduce a distributed compliant model for dynamic simulation. In contrast to the rigid body model and the lumped model, our approach models each contact as a finite patch and uses half space approximation to derive solutions for the small deformations and force distributions in the contact patch. This approach leads to a linear complementarity problem formulation for the contact dynamics. The existence of an unique solution can be proved for both the lumped model in the point contact case and the more accurate, distributed model. Simulation algorithm that incorporates compliant contact models and linear complementarity theory are created and demonstrated through numerical examples."
1443,"A dimensional synthesis procedure to achieve prescribed roll center height variation of a vehicle’s sprung mass with respect to wheel jounce-rebound is presented. This may be used to size the relative lengths of the control arms of a short-long arm suspension mechanism in order to (i) fix the roll center with respect to ground, or (ii) fix the roll center relative to the sprung mass, or (iii) have the roll center move at a prescribed rate relative to the sprung mass, during wheel jounce-rebound. These design selections have a significant impact on the ride-handling characteristics of a vehicle. Numerical examples are provided to demonstrate the synthesis procedure."
1444,"In this paper we introduce a robust algorithm to solve the five-attitude spherical Burmester problem associated with exact linkage synthesis. The proposed algorithm solves for the unit vectors determining the four joint centers of the linkage while taking into account all redundant equations available, which enhances the robustness of the algorithm. In order to show the applicability of the proposed algorithm and to validate its robustness, two examples are included."
1445,"This paper presents a design for a reconfigurable packaging system that can handle cartons of different shape and sizes and is amenable to ever changing demands of packaging industries for perfumery and cosmetic products. The system takes structure of a multi-fingered robot hand, which can provide fine motions, and dexterous manipulation capability that may be required in a typical packaging-assembly line. The paper outlines advanced modeling and simulation undertaken to design the packaging system and discusses the experimental work carried out. The new packaging system is based on the principle of reconfigurability, that shows adaptability to simple as well as complex carton geometry. The rationale of developing such a system is presented with description of its human equivalent. The hardware and software implementations are also discussed together with directions for future research."
1446,"This paper examine the geometric design of the five degree-of-freedom RPS serial chain. This constrained robot can be designed to reach an arbitrary set of ten spatial positions. It is often convenient to consider tasks with fewer positions, and here we study the cases of seven through ten position synthesis. A generalized eigenvalue elimination technique yields analytical solutions for cases seven and eight. While cases nine and ten are solved numerically using homotopy continuation. An numerical example is provided for an eight position task."
1447,"In this paper we present a novel dyad dimensional synthesis technique for approximate motion synthesis. The methodology utilizes an analytic representation of the dyad’s constraint manifold that is parameterized by its dimensional synthesis variables. Nonlinear optimization techniques are then employed to minimize the distance from the dyad’s constraint manifold to a finite number of desired locations of the workpiece. The result is an approximate motion dimensional synthesis technique that is applicable to planar, spherical, and spatial dyads. Here, we specifically address the planar RR, spherical RR and spatial CC dyads since these are often found in the kinematic structure of robotic systems and mechanisms. These dyads may be combined serially to form a complex open chain (e.g. a robot) or when connected back to the fixed link they may be joined so as to form one or more closed chains (e.g. a linkage, a parallel mechanism, or a platform). Finally, we present some initial numerical design case studies that demonstrate the utility of the synthesis technique."
1448,"Synthesizing a motion generating 3-jointed planar chain, under no additional constraints, is trivial. Given a set of desired planar rigid body positions, one can select via straightforward geometric considerations the locations of the revolute (R) joints and prismatic (P) joints of a chain that will reach the positions. On the other hand, specifying constraints on joint limitations or physical parameters may result in no chains that reach the desired positions. In this paper, we study a rigid body in a set of positions in order to determine the point on the body that lies nearest a point, circle or line. Note that the point, circle or line is unknown and is determined as part of the process. The set of points formed by the rigid body point in all of its positions defines a workspace for the outermost moving pivot of the chain. By fitting a generic RPR, PRR or RRR chain’s workspace to these points, we can suggest nearly minimal joint constraints and physical parameters."
1449,"This paper considers the design of the cylindric PRS serial chain. This five degree-of-freedom robot can be designed to reach an arbitrary set of eight spatial positions. However, it is often convenient to choose some of the design parameters and specify a task with fewer positions. For this reason, we study the three through eight position synthesis problems and consider various choices of design parameters for each. A linear product decomposition is used to obtain bounds on the number of solutions to these design problems. For all cases of six or fewer positions, the bound is exact and we give a reduction of the problem to the solution of an eigenvalue problem. For seven and eight position tasks, the linear product decomposition is useful for generating a start system for solving the problems by continuation. The large number of solutions so obtained contraindicates an elimination approach for seven or eight position tasks, hence continuation is the preferred approach."
1450,"The effect of weights on curves and surface design is a well-researched topic in Computer Aided Geometric Design (CAGD). However, the influence of weights in the realm of rational motion approximation and interpolation has been largely unexplored. In this paper, we present a thorough mathematical exposition on the influence of weights on rational motion design. This leads naturally to providing the motion designer with guidelines on how to use weights for rational motion design. Several examples are presented towards the end."
1451,"This paper presents the kinematic synthesis of a CRR serial chain. This is a four-degree-of-freedom chain constructed from a cylindric joint and two revolute joints in series. The design equations for this chain are obtained from the dual quaternion kinematics equations evaluated at a specified set of task positions. In this case, we find that the chain is completely defined by seven task positions. Furthermore, our solution of these equations has yielded 52 candidate designs, so far; there may be more. This synthesis methodology shows promise for the design of constrained serial chains."
1452,"The identification of principal twists of the end-effector of a manipulator undergoing multi-degree-of-freedom motion is considered to be one of the central problems in kinematics. In this paper, we use dual velocity vectors to parameterize se"
1453,"This paper presents a simple but effective type synthesis method for spatial parallel mechanisms with three translational degrees of freedom based on the screw theory. Firstly all possible connecting-chain structures of three-DOF parallel mechanisms are enumerated. According to the reciprocal relationship between screw constraint forces and the motion screw, a novel synthesis method is presented. By using this method, type synthesis for three-DOF translational parallel mechanisms has been made in a systematic and detailed way. As a result, some novel parallel mechanisms generating spatial translation have been obtained. To verify the significance of type synthesis for this kind of mechanism, the paper also gives a concrete application instance, which is used for a micromanipulator for manipulating the bio-cells."
1454,"The instantaneous forward problem (IFP) singularities of a parallel manipulator (PM) must be determined during the manipulator design and avoided during the manipulator operation, because they are configurations where the end-effector pose (position and orientation) cannot be controlled by acting on the actuators any longer, and the internal loads of some links become infinite. When the actuators are locked, PMs become structures consisting of one rigid body (platform) connected to another rigid body (base) by means of a number of kinematic chains (limbs). The geometries (singular geometries) of these structures where the platform can perform infinitesimal motion correspond to the IFP singularities of the PMs the structures derive from. This paper studies the singular geometries both of the PS-2RS structure and of the 2PS-RS structure. In particular, the singularity conditions of the two structures will be determined. Moreover, the geometric interpretation of their singularity conditions will be provided. Finally, the use of the obtained results in the design of parallel manipulators which become either PS-2RS or 2PS-RS structures, when the actuators are locked, will be illustrated."
1455,"The closed-loop structure of a parallel robot results in complex kinematic singularities in the workspace. Singularity analysis become important in design, motion, planning, and control of parallel robot. The traditional method to determine a singular configurations is to find the determinant of the Jacobian matrix. However, the Jacobian matrix of a parallel manipulator is complex in general, and thus it is not easy to find the determinant of the Jacobian matrix. In this paper, we focus on the singularity analysis of a novel 4-DOFs parallel robot H4 based on screw theory. Two types singularities, i.e., the forward and inverse singularities, have been identified."
1456,"We present hardware results of a planar, translational cable-direct-driven robot (CDDR). The motivation behind this work was to present kinematics and statics modeling of the CDDR along with the method to maintain positive cable tension and implement them on CDDR hardware for experimental verification. Only translational CDDR is considered in this article; we attempt to keep zero orientation by control. We ignore gravity here because the end-effector is supported on a base plate with negligible friction. Results are presented and analyzed for two linear profiles and one circular profile."
1457,"In this paper, an NC interpolation algorithm for a tripod-based parallel kinematic machine is investigated. The algorithm can be implemented in two steps, the rough interpolation in the Cartesian space and the precise interpolation in the actuator space. The upper bound of the theoretical interpolation error due to the interpolation algorithm in the precise interpolation and nonlinear mapping is analyzed. The representation of the interpolation error distribution within the Cartesian space is depicted in terms of the variations of the interpolation period and the programming velocity. It was concluded that this error is sufficiently small and may be neglected."
1458,"Selecting a configuration for a machine tool that will best suit the needs of a forecast set of requirements can be a difficulty and costly exercise. This problem can now be addressed using an integrated virtual validation system. The system includes kinematic/dynamic analysis, kinetostatic model, CAD module, FEM module, CAM module, optimization module and visual environment for simulation and collision detection of the machining and deburring. It is an integration of the parallel kinematic machines (PKM) design, analysis, optimization and simulation. In this paper, the integrated virtual system is described in a detail, a prototype of a 3-dof PKM is modeled, analyzed, optimized and remote controlled with the proposed system. Some results and simulations are also given. Its effectiveness is shown with the results obtained by NRC-IMTI during the design of the 3-dof NRC PKM."
1459,"In this paper an optimization method based on the Mechanics of Parallel Robots and orientated on workspace is conducted in the construction of 6-HTRT parallel robot. By analyzing the characteristics of specific workspace and setting up objective functions, optimizations are implemented on the design of parallel robot. As a result of the optimization design, the parallel robot not only figures the minimum overall size of robot structural, but also has workspace unrestricted by the limit range of Hooke joint’s conical angles. The restriction factors on workspace of 6-HTRT parallel robot are reduced thus the algorithm for motion control of the robot is simplified and the performance of the parallel mechanism is improved."
1460,"A novel cable-based metrology system is presented wherein six cables are connected in parallel from ground-mounted string pots to the moving object of interest. Cartesian pose can be determined for feedback control and other purposes by reading the lengths of the six cables via the string pots and using closed-form forward pose kinematics. This paper focuses on a sculpting metrology tool, assisting a human artist in generating a piece from a computer model, but applications exist in manufacturing, rapid prototyping, robotics, and automated construction. The proposed real-time cable-based metrology system is less complex and more economical than existing commercial Cartesian metrology technologies."
1461,"A method to determine optimal placement of smart (active) materials based actuators in the structure of robot manipulators for the purpose of achieving higher operating speed and tracking precision is developed. The method is based on the evaluation of the transmissibility of the displacement from the integrated smart actuators to the robot manipulator joint and end-effector displacements. By studying the characteristics of the Jacobian of the mapping function between the two displacements for a given position of the robot manipulator, the optimal positioning of the smart actuators that provides maximum effectiveness in eliminating high harmonics of the joint motion or the end-effector motion is determined. In robots with serial and parallel kinematics chains containing non-prismatic joints, due to their associated kinematics nonlinearity, if the joint motions were synthesized with low harmonic trajectories, the end-effector trajectory would contain high harmonics of the joint motions. Alternatively, if the end-effector motion were synthesized with low harmonic motions, due to the inverse kinematics nonlinearity, the actuated joint trajectories would contain a significant high harmonic component. As the result, the operating speed and tracking precision are degraded. By integrating smart materials based actuators in the structure of robot manipulators to provide small amplitude and higher frequency motions, the high harmonic component of the actuated joint and/or the end-effector motions are eliminated. As the result, higher operating speed and tracking precision can be achieved."
1462,"This paper introduces the XZ Micropositioning Mechanism (XZMM) that is fabricated in the x-y plane and translates components in the x-z direction using one linear input. The positioning platform of the mechanism remains parallel to the substrate throughout its motion. The XZMM has been tested and actuated using thermal actuation and achieves an out-of-plane output displacement of 41 micrometers with a 27 micrometer x-direction input."
1463,"A purely analytical method has been developed for the kinetic (force or kinetostatic) analysis of frictionless planar mechanisms. It employs polar notation of vectors, the principle of conservation of energy and the force equilibrium of the links. Unlike many other methods which lead to a system of several simultaneous equations, it leads to only one algebraic or one vectorial equation at a time and, interestingly, it is less time consuming that the conventional graphical methods. The method is general, comprehensive and systematic such that it could also serve as a suitable teaching technique for manual approach to the problem. It easily lends itself to automation too."
1464,"The basic tool of path or motion generation synthesis for more than four prescribed positions is analytical calculation, but its process is quite complicated and far from straightforward. A novel computer simulation mechanism of six-bar linkage for path or motion generation synthesis is presented in this paper. In the case of five-precision points, using the geometric constraint and dimension-driving techniques, a primary simulation mechanism of four-bar linkage is created. Based on the different tasks of path and motion generation for kinematic dimensional synthesis, the simulation mechanisms of path and motion generation with Stephenson I, II and Watt six-bar linkages are developed from the primary simulation mechanism. The results of kinematic synthesis for five prescribed positions prove that the mechanism simulation approach is not only fairly quick and straightforward, but is also advantageous from the viewpoint of accuracy and repeatability."
1465,"A special class of planar and spatial linkage mechanisms is presented in which for a continuous full rotation or continuous rocking motion of the input link, the output link undergoes two continuous rocking motions. In a special case of such mechanisms, for periodic motions of the input link with a fundamental frequency ω, the output motion is periodic but with a fundamental frequency of 2ω. In this paper, the above class of linkage mechanisms are referred to as speed-doubling linkage mechanisms. Such mechanisms can be cascaded to provide further doubling of the fundamental frequency (rocking motion) of the output motion. They can also be cascaded with other appropriate linkage mechanisms to obtain crank-rocker or crank-crank type of mechanisms. The conditions for the existence of speed-doubling linkage mechanisms are provided and their mode of operation is described in detail. Such speed-doubling mechanisms have practical applications, particularly when higher output speeds are desired, since higher output motions can be achieved with lower input speeds. Such mechanisms also appear to generally have force transmission and dynamics advantages over regular mechanisms designed to achieve similar output speeds."
1466,"The paper begins with a graphical technique to locate the pole; i.e., the point in the plane of motion which is coincident with the instantaneous center of zero velocity of the coupler link. Since the single flier linkage is indeterminate, the Aronhold-Kennedy theorem cannot locate this instantaneous center of zero velocity. The technique that is presented here is believed to be an original contribution to the kinematics literature and will provide geometric insight into the velocity analysis of an indeterminate linkage. The paper then presents an analytical method, referred to as the method of kinematic coefficients, to determine the radius of curvature and the center of curvature of the path traced by an arbitrary coupler point of the single flier eight-bar linkage. This method has proved useful in curvature theory since it separates the geometric effects of the linkage from the operating speed of the linkage."
1467,"A modular robot system is a collection of actuators, links, and connections that can be arbitrarily assembled into a number of different robot configurations and sequences. High performance modular robots require more than just sophisticated controls. They also need top-quality mechanical components. Bearings in particular must operate well at low speed, have high rotational accuracy, be compact for low weight, and especially be stiff for high positional accuracy. To ensure the successful use of bearings in precision modular robots, knowledge of the bearing properties and requirements are investigated. Background information on various topics such as modular robots, precision modular actuators, and their error sources are described with respect to precision engineering. Extensive literature on thin section bearings is reviewed to examine their use in precision robotic applications. Theoretical studies are performed to calculate bearing stiffness adopting a methodology based on Hertzian theory. This approach is applied to analyze two proposed designs of equivalent-sized crossed roller and four-point bearings, principal bearings used for transmitting all the payload and mass of the robot structure. The maximum deflections and contact stresses for the proposed actuator assembly and loading conditions are estimated and compared including a range of general bearing properties such as friction, cost, and shock resistance."
1468,"This article explores the effect that end-effector velocities have on a nonredundant robotic manipulator’s ability to accelerate its end-effector as well as to apply forces/moments to the environment at the end-effector. The velocity effects considered here are the Coriolis and Centrifugal forces, and the reduction of actuator torque with rotor velocity, as described by the "
1469,"A new analytical method for determining, describing, and visualizing the solution space for the contact force distribution of multi-limbed robots with three feet in contact with the environment in three-dimensional space is presented. The foot contact forces are first resolved into strategically defined foot contact force components to decouple them for simplifying the solution process, and then the static equilibrium equations are applied to find certain contact force components and the relationship between the others. Using the friction cone equation at each foot contact point and the known contact force components, the problem is transformed into a geometrical one to find the ranges of contact forces and the relationship between them that satisfy the friction constraint. Using geometric properties of the friction cones and by simple manipulation of their conic sections, the whole solution space which satisfies the static equilibrium and friction constraints at each contact point can be found. Two representation schemes, the “force space graph” and the “solution volume representation,” are developed for describing and visualizing the solution space which gives an intuitive visual map of how well the solution space is formed for the given conditions of the system."
1470,"One of the inherent problems of multi-limbed mobile robotic systems is the problem of multi-contact force distribution; the contact forces and moments at the feet required to support it and those required by its tasks are indeterminate. A new strategy for choosing an optimal solution for the contact force distribution of multi-limbed robots with three feet in contact with the environment in three-dimensional space is presented. The optimal solution is found using a two-step approach: first finding the description of the entire solution space for the contact force distribution for a statically stable stance under friction constraints, and then choosing an optimal solution in this solution space which maximizes the objectives given by the chosen optimization criteria. An incremental strategy of opening up the friction cones is developed to produce the optimal solution which is defined as the one whose foot contact force vector is closest to the surface normal vector for robustness against slipping. The procedure is aided by using the “force space graph” which indicates where this solution is positioned in the solution space to give insight into the quality of the chosen solution and to provide robustness against disturbances. The “margin against slip with contact point priority” approach is also presented which finds an optimal solution with different priorities given to each foot contact point for the case when one foot is more critical than the other. Examples are presented to illustrate certain aspects of the method and ideas for other optimization criteria are discussed."
1471,"In this paper, an efficient dynamic simulation algorithm is developed for an Unmanned Underwater Vehicle (UUV) with a "
1472,"In this research we investigate the design of nesting forces for exactly constrained, robust mechanical assemblies. Exactly constrained assemblies have a number of important advantages including the ability to assemble over a wide range of conditions. Such designs often require nesting forces to keep the design properly seated. To date, little theory has been developed for the design of nesting forces. We show how the effects of tolerances on nesting forces, a key issue, can be analyzed and apply the analysis to a simple design problem. For the example problem, good agreement is achieved with results from Monte Carlo simulation."
1473,"In recent years a number of practicing engineers have discussed the virtues of exactly constrained (EC) mechanical assemblies. While found by engineers in industry to have many benefits, EC designs remain somewhat unrecognized by academia. One reason for this minimal exposure may be the lack of a mathematical foundation for such designs. EC designs can be analyzed quite simply by understanding that they are statically determinate. This paper describes the history and current background for EC designs. It also begins to develop the mathematical foundation for EC design based on equations of equilibrium. Finally, it examines a Monte Carlo simulation of the effects of variation on EC assemblies vs. over-constrained assemblies. The EC design assembles 100% of the time, while the over-constrained design assembles only 50% of the time with greater error."
1474,"This work is a part of a larger research project to understand the challenges in creating MultiStable Equilibrium (MSE) devices. MSE devices are those that have more than one stable position or configuration which can be maintained with no power input. The study of potential energy minima in magnetic systems can be used to create novel and efficient MSE mechanisms. This research focuses on using the magnetic energy in space as the main criteria for analysis of MSE devices. Permanent magnets are used to create a 2D magnetostatic field. The magnetic energy density in air is then plotted along a path through the field. For a piece of iron following a specified path, the stable equilibrium positions correspond to the locations of maximum air energy density. Furthermore, the stiffness at each of the equilibrium positions can be correlated to a pseudo-stiffness of the energy density. This information can help one design systems for multiple stable positions with minimal computational time and effort without simplifying the problem geometry."
1475,"This paper deals with the geometric issues that arise in designing a system for measuring the orientation of an object in three dimensional space using a new class of wireless angular position sensors. The wireless sensors are waveguides that receive and record the electromagnetic energy emitted by a polarized RF source. The angular position of the waveguide relative to the source is indicated by the energy level. A system equipped with multiple waveguides is used as a 3D orientation sensor. This paper explores the geometry for orientation measurement using the system and provides the guidelines for sensor design."
1476,"This paper addresses similarities between various nutating or wobbling mechanisms, especially kinematic similarities. A case is made for the generalization of several mechanisms into a mechanism “class” having common kinematic characteristics. This mechanism class is shown to be typified by bevel epicyclic gear trains. It is proposed that not only kinematic analysis, but static-force, power-flow, and efficiency analyses of mechanisms belonging to this “class” can be simplified by modeling them as bevel-gear trains. Simplified kinematic, force, and efficiency analyses are demonstrated for a novel wobbling speed reducer using this concept of “equivalent” geared mechanisms. The reduction in complexity of these analyses is the main motivation for this work."
1477,"In this work, we investigate the geometry and position kinematics of planar parallel manipulators composed of three GPR serial sub-chains, where G denotes a rolling contact, or geared joint, P denotes a prismatic joint, and R denotes a revolute joint. The rolling contact joints provide a passive one degree-of-freedom relative motion between the base and the prismatic links. It is shown, both theoretically and numerically, that when all the G-joints have equal circular contact profiles, there are at most 48 real forward kinematic solutions when the P joints are actuated. The solution procedure is general and can be used to predict and solve for the kinematics solutions of 3-GPR manipulators with any combination of rational contact ratios."
1478,"The robust design of a novel mobile robot, which comprises two driving wheels and an intermediate body carrying the payload, is the subject of this paper. We prove that, by virtue of the robot architecture, the kinetostatic model of the system is isotropic. Moreover, regarding the robot dynamic response, a robust design problem is formulated by minimizing the "
1479,"This paper deals with the kinematic analysis of a wheeled mobile robot (WMR) moving on uneven terrain. It is known in literature that a wheeled mobile robot, with a fixed length axle and wheels modeled as thin disk, will undergo slip when it negotiates an uneven terrain. To overcome slip, variable length axle (VLA) has been proposed in literature. In this paper, we model the wheels as a torus and propose the use of a passive joint allowing a lateral degree of freedom. Furthermore, we model the mobile robot, instantaneously, as a hybrid-parallel mechanism with the wheel-ground contact described by differential equations which take into account the geometry of the wheel, the ground and the non-holonomic constraints of no slip. Simulation results show that a three-wheeled WMR can negotiate uneven terrain without slipping. Our proposed approach presents an alternative to variable length axle approach."
1480,"Traction drive systems offer unique advantages over geared systems. They will typically run quieter and they can be designed to eliminate all backlash. Furthermore, rolling elements are easy to manufacture and the rolling motion will produce very efficient power transmission. In this paper the authors describe a two-stage, self-actuating, traction drive system that has been fabricated to produce a speed ratio of 50:1. Given specific values for coefficients of friction, the geometry of each stage of the device must be designed to ensure self-actuation. In addition, dimensions of the drive rollers and output rings for the two stages must be selected to ensure that one stage does not cause the other stage to overrun."
1481,"The goal of this research is to obtain the optimum design of a new interbody fusion implant for use in lumbar spine fixation. A new minimally invasive surgical technique for interbody fusion is currently in development. The procedure makes use of an interbody implant that is inserted between two vertebral bodies. The implant is packed with bone graft material that fuses the motion segment. The implant must be capable of retaining bone graft and supporting spinal loads while fusion occurs. Finite element-based optimization techniques are used to drive the design. The optimization process is performed in two stages: topology optimization and then shape optimization. The different load conditions analyzed include: flexion, extension, and lateral bending."
1482,"Most of the algorithmic engineering design optimisation approaches reported in the literature aims to find the best set of solutions within a quantitative (QT ) search space of the given problem while ignoring related qualitative (QL ) issues. These QL  issues can be very important and by ignoring them in the optimisation search, can have expensive consequences especially for real world problems. This paper presents a new integrated design optimisation approach for QT  and QL  search space. The proposed solution approach is based on design of experiment methods and fuzzy logic principles for building the required QL  models, and evolutionary multi-objective optimisation technique for solving the design problem. The proposed technique was applied to a two objectives rod rolling problem. The results obtained demonstrate that the proposed solution approach can be used to solve real world problems taking into account the related QL  evaluation of the design problem."
1483,"In most present CAD systems, the accurate geometry representation of a part is given but the tolerance is just a text attribute without semantics. It hampers the integration of CAD/CAM. In last two decades, much research has been conducted on computer aided tolerancing. And many approaches to generation of variational geometry for various kinds of features have been given. However how to generate the variational geometry of a pattern of holes (POH) is not well studied yet although a POH is one of the most widely-used features in mechanical design. Based on DOF, an approach is proposed for generating variational geometry of a POH with composite positional tolerance in this paper. First, the limitation conditions for variations of a POH are given. Second, the mathematical models of translational and rotational variations are systematically derived. Then the algorithm for generating variational geometry of a POH is given. At last the proposed approach has been implemented with Visual C++ based on the geometric modeling kernel ACIS5.0."
1484,"Design activities are a series of decision-making, and important decision-making takes place often early in product development. However, uncertainty and imprecision exist in such design stages and make decision-making extremely difficult. Furthermore, as collaborative engineering emerges, it is hard to coordinate every project participant because of the differences in locations, disciplines, cultures and languages. In the early design cycle, therefore, two factors might improve to reach optimal decisions—making communication in a project team smoother and allowing all the members to evaluate designs with uncertainty and aggregate pieces of these evaluations as a whole. To address these two, the Internet-based collaboration tools, RP, VP and fuzzy analysis should be introduced to product development. In this study, these tools are analyzed and structured in product development so that decision-making in difficult situations is eased and product cost and quality are improved while time-to-market is shortened."
1486,"Compliant sheet metal assemblies are often used as support structures in automobiles, airplanes and appliances. These structures not only provide a metrology frame for other modules to be assembled, but also give the product its aesthetic form. For this reason, the dimension quality of the assemblies is a very important factor to control, in order to make sure that the product will function as planned and continuously keep the product cost low. The assembly is influenced by variations in the component parts and the assembly processes. Tolerance analysis, as conducted in most industries today, is normally based on the assumption of rigid parts and is thus not always valid for sheet metal assemblies, due to their compliance. This paper will present a method, based on finite element analysis (FEA) and design of computer experiments, of identifying the influence of input variables on the final geometry variation of the assembly. The influence and the interactions among the input variables are analyzed with a response model that has been constructed, using the simulation results. This response model could be used to identify the important variables that need to be controlled in assembly. An example application is included, in order to demonstrate the simulation model and response model construction. Analysis of the results from the simulations can facilitate the design of the assembly process, in order to control the dimensional quality of the product."
1488,"Reconstruction of freeform surfaces is a state-of-the-art topic in reverse engineering. In this paper, a method of reconstructing freeform faces with parameterized freeform features is proposed. By their definitions in 3-Dimensional (3-D) space, freeform feature templates are analyzed and parameterized by high-level parameters first. The proposed parameterization provides both interactive user control and automatic feature extraction. With such freeform features, the designer can reason about and operate high-level entities in freeform surfaces than basic geometric constituents. An optimization function based on Hausdorff-like shape distance measuring method is proposed and applied as the similarity measuring method between the digitalized model surface and the parameterized feature template according to different kinds of features. Considering the fitting procedure between digitalized model surface data and the freeform feature templates, optimizing strategies were studied and the influence of parameters in each feature representation was qualified especially for shape elimination features. The proposed method was tested by numerical experiments based on ACIS®  and Open Inventor® , fitting errors were analyzed as well."
1489,"In this article, vibratory parts feeder widely used in industrial automation with the structure of a symmetric four-bar linkage mechanism driven by piezoelectric actuator is investigated. The dynamic modeling and simulation of the system as well as the driving controller are developed. Experiment is carried out to measure the practical acceleration and force of the vibratory platform. In comparison the experimental result with the analytical result, both outcomes are quite matched, which indicates their good accuracy. Driving control circuitry with feedback design is instrumental to provide steady output performance. Finally, optimum design to improve the transport speed and keep the parts in order is discussed."
1490,"The paper presents a new approach aiming at modeling and supporting the design activity as the substantial activity within the product development process. The Autogenetic Design Theory facilitates the integration of intuition, creativity and artificial intelligence into the conventional design process. To this end, a phase-like allocation of the design process is assumed as the essential structure and an evolutionary algorithm is integrated as the core facilitating purposeful searching and combining. Hence, the flow of the design process can be influenced as all requirements can be included and, on the other hand, intuition and creativity are ensured through the evolutionary algorithm. The example of the shift fork, which was described in the paper, shows how ADT can support variant design. Next steps will include the application of the ADT to the other types of design. The optimisation of the shift fork was the first step towards a computer-based implementation of the ADT."
1491,"In this paper a paradigm of human head aesthetic design based on hybrid genetic algorithm is proposed. Hybrid means the integration of interactive and natural selection of the population in genetic algorithm during the evolution process. Firstly, a sketching paradigm is proposed to sketch the head profile by mouse input, the profile is then parameterized and modified; quantities of more profiles are generated based on GA optimizer, where the user can select the best one. After 2D profile design, a 3D reconstitution algorithm is developed to construct the 3D head in NURBS model by analyzing four input sketching curves, i.e. one longitudinal curve, three side profile curves. The 3D head mesh model is capable of modification and easy to be manipulated. The prototype CAD system is developed based on ACIS 3D modeling kernel, and the design result is illustrated to show the effectiveness of this paradigm."
1492,"The bi-directional communication of CAD programs with subsequent applications such as process planning remains a key challenge in design-for-the-lifecycle. While it seems sensible that individual applications use their own collection of feature types and thereby allow users to have their specific perspective of the product, it is still difficult to automatically close the gap between the variety of applications. Universal Linking of Engineering Objects (ULEO) targets this concern. It is general enough to facilitate informational integration of the applications along the process chain. This paper examines a number of scenarios for exploiting ULEO’s benefits in the field of automotive development and reports on the associated prototypical software implementations. Principle alternatives and technical aspects relevant for applying ULEO are discussed in some detail beforehand."
1493,"This paper proposes a mini-max type formulation for strict robust design optimization under correlative variation based on design variation hyper sphere and quadratic polynomial approximation. While various types of formulations and techniques have been developed for computational robust design, they confront the compromise among modeling of parameter variation, feasibility assessment, definition of optimality such as sensitivity, and computational cost. The formulation of this paper aims that all points within the distribution region are thoroughly optimized. For this purpose, the design space with correlative variation is diagonalized and isoparameterized into a hyper sphere, and the functions of nominal constraints and the nominal objective are modeled as quadratic polynomials. These transformation and approximation enable the analytical discrimination of inner or boundary type on the worst design and its quantified values with less computation cost under a certain condition, and bring the procedural definition of the strictly robust optimality of a design as a maximization problem. The minimization of this formulation, that is, mini-max type optimization, can find the robust design under the above meaning. Its validity is ascertained through numerical examples."
1494,"Machine product designs routinely have so many mutually related characteristics that common design optimization methods often result in an unsatisfactory local optimum solution. In order to overcome this problem, this paper proposes a design optimization method based on the clarification of the conflicting and cooperative relationships among the characteristics. First of all, each performance characteristic is divided into simpler basic characteristics according to its structure. Next, the relationships among the basic characteristics are systematically identified and clarified. Then, based on this clarification, the optimization problem is expressed using hierarchical constructions of these basic characteristics and design variables related to the most basic characteristics. Finally, an optimization strategy and detailed hierarchical optimization procedures are constructed, after clarifying the influence levels of each basic characteristic upon the objective functions and setting a core characteristic for the product under consideration. Here, optimizations are sequentially repeated starting with the basic optimal unit group at the bottom hierarchical level and proceeding to higher levels by the hierarchical genetic algorithms. Then, the Pareto optimum solutions at the top hierarchical level are obtained. With the proposed optimization methods, optimization can be more easily applied after the optimization problems have been simplified by decomposition. In doing so, the volume of design spaces for each optimization is reduced, while useful and unique rules and laws may be uncovered. The optimization strategy expressed by the hierarchical structures can be used for the optimization of similar product designs, which realize these breakthroughs, yielding improved product performances. The proposed method is applied to a machine-tool structural model."
1495,"Development of products that thoroughly integrate lifecycle design factors requires the simultaneous clarification and evaluation of numerous design elements. When such product development problems are treated as design optimization problems, these elements are expressed by decision variables, constraint functions and objective functions. The existence of complex relationships among such design elements increases the difficulty of formulating the problem model and conducting straightforward searches for optimal solutions. This study presents a method for representing design alternatives that have subordinate relationships, using hierarchical genotypes. Constraint relationships of alternatives are clarified using constraint matrices which are also transformed to hierarchical genotypes. Furthermore, the efficiency of different genotype structures is compared using numerical experimentations. Based on the experimental results, a procedure for constructing optimal hierarchical genotypes is proposed."
1496,"The divergent tree method was adopted in this paper, and an illustrative example of tool storage design in the machining center was given to describe the divergent thinking in the conceptual design process of mechanical products. Firstly general divergent tree method was applied to get various schemes of storage, then the primary schemes were achieved by using the measure of known characteristics, finally the excellent degree appraisal approach was applied to find out the optimum one. In addition, an intelligent computer aided conceptual design system of tool storage based on the divergent tree method and excellent degree appraisal approach was demonstrated in this paper."
1497,"The BOSS Quattro system (developed by Samtech s.a.) is an open architecture allowing to run various optimisation engines including gradient based methods (SQP, GCMMA, Conlin, MDAQ, ...), DOE and Response Surfaces, and Genetic Algorithms. This system has been used and various optimisation strategies compared for the resolution of non linear optimisation problems including crash worthiness and airbags opening simulation. Thanks to BOSS Quattro open architecture, using neutral or specific drivers so to read information from models, software like MADIMO, Pam Crash, LS-Dyna, but also NASTRAN, SAMCEF and Abaqus can been used in order to model the non linear behaviour of the optimised vehicles. One or several models were used and computation distributed by BOSS parallel architecture on network of workstations. Direct parallel mode can be used, or the tasks distributed on the network through a Task Manager like for example LSF. As a first example, the optimisation of a stiffened box is performed using one (dynamic) then two models (static but elasto-plastic/dynamic). The external software used are NASTRAN and ABAQUS. A second example deals with the optimisation of a Golf crash model using MADIMO to represent a sequence of events including opening of air bags. Mixed optimisation is performed using as design variables : the time to open a given airbag (continuous) or the type of wheel configuration (pure discrete variable). Constraints are related to safety of the passenger (sternum invasion, various criteria related to accelerations)."
1498,"Due to environmental considerations, corrugated paperboard folded into appropriate 3-D structural shapes are increasingly being used as packaging cushions, as a substitute for those traditionally made of polymer foams. However, since paperboards are manufactured in the form of sheets, 3-D structures have to be created from these boards by folding. The design of the necessary flat layout pattern of a board that can be folded into a reasonably complex and intricate shape is a process requiring a lot of costly trial-and-error and creativity on the part of the designer. This paper describes a methodology developed to aid the designer by automatically and systematically generating many possible flat layouts that can be folded into a specified 3-D folded structure. The key to such a method is a computer representation of the topology/connectivity of the faces of the 3-D folded structure by a graph-theoretic model, and an algorithm to operate on this model to unfold and generate the geometry of the planar layout. The procedure is implemented on a computer and resulting flat layout designs have been generated for four example structures. Some of the issues concerning the types of folded structures that can and cannot be easily unfolded and the types of layouts that can and cannot be generated by the current methodology are discussed."
1499,"This paper presents a soft computing strategy to determine the optimal die gap parison programming of extrusion blow molding process. The design objective is to minimize part weight subject to stress constraints. The finite-element software, BlowSim, is used to simulate the parison extrusion and the blow molding processes. However, the simulations are time consuming, and minimizing the number of simulation becomes an important issue. The proposed strategy, Fuzzy Neural-Taguchi and Genetic Algorithm (FUNTGA), first establishes a back propagation network using Taguchi’s experimental array to predict the relationship between design variables and response. Genetic algorithm is then applied to search for the optimum design of parison programming. As the number of training samples is greatly reduced due to the use of orthogonal arrays, the prediction accuracy of the neural network model is closely related to the distance between sampling points and the evolved designs. The Reliability Distance is proposed and introduced to genetic algorithm using fuzzy rules to modify the fitness function and thus improve search efficiency. This study uses ANSYS to find the stress distribution of blown parts under loads. The comparison of results demonstrates the effectiveness of the proposed strategy."
1500,"Pareto solutions in multiobjective optimization are very problematic to measuring the characteristics of solutions for engineering design because of their considerable variety in function space and parameter space. To overcome these situations, a clustering-based interpretation process for Pareto solutions is considered. For better competitive clustering algorithm, we propose an evolutionary clustering algorithm — ECA. The ECA requires less computational effort, and overcomes local optimum of the K-means clustering algorithm and its related algorithms. Effectiveness of the method is examined in detail through the comparison with other algorithms."
1501,"This paper presents an automatic method for conceptual design of mechanical devices by embodying the functions to the graph carrier according to the design rules. Through “and”, “or”, “no” logical calculations, and “adjacency”, “overlap”, “topology” relation calculations among the basic fimctional forms supplied in this paper, the design rules can be defined. Since the design rule only reflects the logical relationships among the basic function forms, it is easily to be carried out by computers. By introducing imaginary function forms and defining special basic function forms, the system can deal with the conceptual design problem for complex mechanical devices. Comparing with enumerating manually method, by doing strict similarity analysis of links of graph carrier and exact logical calculation among basic functional forms, our method can efficiently avoid producing redundance schemes and missing suitable results, and will get all suitable results. The research has shown that the conceptual design results obtained by combination of basic simple mechanisms belong to a subset of the results group obtained by the method presented in this paper. The most prominent advantage of our method presented in this paper is that by refining the design rules, the conceptual design process is convergent, and optimal conceptual design solutions are available."
1502,"In design processes of machinery, much computer software, for examples CAD/CAE/CAM software, are used. The design processes are aided by this software. The seamless connections of the design processes are necessary for the improvement of the cost benefit of the products. However, sometimes, the conflicts among designs or design processes are occurred by the change of the design at a certain design process and these conflicts are should be noticed and solved immediately. In this research, we have developed the design system including CAD software and the developed system has some functions to find the conflicts of the change of designs and to solve the conflicts. We use Data Envelopment Analysis (DEA) to calculate the efficiency of the design and Satisficing Trade-off Method (STM) to find and solve the conflicts. We have confirmed through some fundamental numerical examples and discussed of the proposed system and method and showed the effectiveness of out study."
1503,"This paper presents a new method for adapting the existing manufacturing system for a requirement change, such as the change of a required motion task, by modifying its kinematic structure from the viewpoint of the degrees of freedom of the end effecter of the mechanism. In a previous report, the authors formulated the kinematic structure and motion task using Lie algebra. In this report, using these representations and their inclusion relations, the authors propose a method for evaluating the reusability of a kinematic structure for a newly specified motion task, and then propose a method for reconfiguring the mechanism in Lie algebra according to the result of the reusability evaluation. Finally, in order to evaluate the usefulness of the proposed method, it is applied to a kinematic design example of a mechanism."
1504,"This paper presents a new method to determine an optimum topology of plate structure using coordinate transformation by conformal mapping. We have already proposed a method to determine an optimum topology of planar structure using coordinate transformation by conformal mapping. In that study first we defined simple design domain in which analysis and optimization were performed easily. We calculated optimum topology in this simple design domain. Then we applied coordinate transformation by conformal mapping to optimum topology calculated in simple design domain, and obtained some optimum topologies in complex design domain. We also showed that the invariants of stresses which were the sum and difference of principal stress satisfied Laplace equation and relationshi p between fluid mechanics and electromagnetic could be valid in the theory of elasticity. In this study we clarify two invariants of bending moments satisfy Laplace equation under a certain condition. We note the similarity between Airy stress function of 2-D elastic body and deflection of plate, and will show that the two invariants of bending moments which are the sum and difference of principal bending moments satisfy Laplace equation using this similarity. As a result we will show that corresponding relationship between fluid mechanics, electromagnetic and elasticity may be valid in the theory of plate. Then by using this relationship, we proposed a new method to determine optimum topology using coordinate transformation by conformal mapping. Our proposed method will be useful to determine optimum topology easily in complex design domain. Through numerical examples, we can examine the effectiveness of the proposed method."
1505,"This method simulates a stepped beam by substructuring it into its "
1506,"A commercial spacecraft should survive on orbit for more than 10 years under the severe circumstances without any maintenance. To realize this subject, not only performance but also other design factors such as reliability, mass, robustness, cost, etc. should be taken into consideration. From point of the thermal design, it is very important to obtain the robust thermal control subsystem with matrix heat pipe layout while minimizing the mass (weight). A new thermal optimization method without compromising the thermal robustness and the mass of thermal subsystem is highly anticipated. This paper proposes a robust thermal design approach for optimizing the heat pipe shape to minimize the mass of the spacecraft panel. We apply a combination of Design of Experiments (DOE), Response Surface Methodology (RSM) and Monte Carlo Simulation to determine the robust design parameters that minimize the mass of the heat pipe structure. Dimensions of the heat pipe design parameters were determined with rationally in a short time and practical robust optimization method was established."
1507,"Split power continuously variable transmissions can offer clutchless transitions from reverse through park and into the forward driving mode. The advantages of these transmissions can include elimination of a launch device while maintaining the continuous variability and fuel economy benefits of conventional CVTs. This paper offers further improvements to the transmission by providing an efficiency optimization methodology. Models for the transmission are constructed. A genetic algorithm is developed for the optimization routine. The effects of the gear material strength, gear module, gear tooth width, and shaft offset on the efficiency are determined. The bearing diameters and gear parameters are calculated and reviewed. Power loss contributions are presented in conjunction with their effect on the overall transmission efficiency. Recommendation for further improvements to the methodology is made."
1508,"Manufacturing processes produce a considerable amount of data as dimensions are measured, tests are performed and assembly checks are undertaken. Predominantly these data are used to inform and help improve the associated manufacturing processes and procedures. A variety of Knowledge Discovery techniques [1] have been introduced in the engineering field, most typically in areas with large quantities of data [2]. This paper describes research into the use of such techniques in the manufacture and assembly of large complex engineering products, an area which is characterised by low volume of data and dispersed databases. The developed methodology seeks to incorporate various approaches, with emphasis on using extracted knowledge to inform the implementation of subsequent techniques. This investigation centres on discovering and quantifying relationships between the various balance and vibration tests performed throughout assembly of gas turbine rotors, and to highlight critical parameters. Current assembly practices do not use forward prediction of test performance, and the first stages of this project aim to produce a model to enable this. The scope of this model will then be extended to feed this knowledge back to be used in the design and manufacture of future components."
1509,"In this study, we propose multi-stage and hybrid real-coded genetic algorithm. In the proposed algorithm, there are two stages. In the first stage, Real-coded Genetic Algorithm with Active Constraints (RGAAC) is applied to find a solution that is close to the global optimum. In RGAAC, indviduals who are out of the feasible region are pulled back into feasible region. Therefore, the effective search can be carried out even in the constraints problems. In the second stage, Feasible Region Limiting Method (FRLM) is applied to obtain an optimum solution. FRLM uses the solution that is derived in the first stager as an initial point. In this study, RGAAC is applied to solve the truss structure problems. Through these problems, the effectiveness and the searching mechanism of RGAAC is discussed. The, the proposed algorithm is also applied to 2D problem. In this problem, there are about 1000 design variables. The proposed method can derive the reasonable solution. From these results, it is concluded that the proposed method is effective to solve optimzation problems of large scale structures."
1510,"The paper deals with the choice of the motion parameters for a window regulator system. This problem is affected by aesthetic, geometric and functional constraints. A geometric formulation of the topic is discussed in the first part of the paper: types of surfaces suitable to define proper motion parameter are systematically listed. Then, computational procedures for the best fit of given points to some of such surfaces are discussed in details. They are applied to two numerical examples, one of known mathematical properties, the second taken from a real design problem. In the second part of the paper further design constraints are introduced and the consequent computational procedures described. Finally three different design solutions for the real problem are presented and evaluated from an engineering point of view."
1511,"Constraint-based feature modelling is a promising approach for object modelling. During the feature creation and modification, the relationships between features are well described and maintained. This eliminates the tedious work for constructing a complex feature-based model and provides an efficient method for shape modification. However, most of the feature representation schemes cover only the regular object domain. The application in sculpture object domain has been put aside. In this paper, the constraint-based feature modelling approach for sculpture object such as human body will be introduced."
1512,"This study analyzes the loads of a needle by using singularity functions and determines the Von-Mises stresses to predict the failure modes of needles by using a personal computer. After principal stresses are calculated from the bending stress, compressive stress and shear stress, predicted failure modes of needles based on the Von-Mises stress coincide with practical existing failure modes reported by a manufacturer. These calculated stresses are also compared with the results obtained by using the software ABAQUS in the mainframe, and the deviation between the results calculated by these two methods is also investigated. Using this methodology can obtain loads, stresses and failure modes of a needle with acceptable accuracy while reducing the cost of using the commercial software in the mainframe."
1513,"An Artificial Immune System (AIS in short) based Isomorphism Identification method for Mechanism Kinematic Chains (IMKC) is proposed in this paper. The problem of IMKC has puzzled the scholars in the field of mechanical design for a long time, however, most researchers have omitted one important fact that this problem can be viewed as a Traveling Salesman Problem (TSP) and solved with relevant approaches. AIS is a newly developed bionic paradigm of information processing and problem solving, with powerful ability in recognition, memory and feature extraction. Temporally, AIS has been employed in solving many combinatorial optimization problems like TSP. In view of the existing problems of IMKC, we introduce the definition of Structural Code and convert the problem of computation on IMKC to a similar TSP. Then an AIS based algorithm is utilized to solve the problem and the result shows AIS is a powerful tool in solving IMKC problem and the numerical experiment shows that the AIS based method is better than the one used GA and both of the two methods are highly efficient in solving IMKC problems."
1515,"This paper provides a systematic approach for copying and pasting of freeform features among existing models of design. Freeform feature as complex high-level shape entity enables a fast creation and modification of a geometric model in the context of both mechanical and aesthetic design. Copying and pasting of freeform feature can enhance not only the rapid shaping of the geometric model itself, but also the inheriting of design knowledge built in existing designs. In this paper definitions of freeform feature are reviewed and consummated. An analysis of parametric and topological relevancy of freeform feature is given in terms of copying operation and an elaboration of the reconstruction of freeform feature in a new geometric model regarding to pasting operation is presented. The reuse of freeform feature is discussed, and related algorithms are presented in detail."
1516,"This paper proposes a general architecture for using evolutionary algorithms to achieve MEMS design synthesis. Functional MEMS devices are designed by combining parameterized basic MEMS building blocks together using Multi-objective Genetic Algorithms (MOGAs) to produce a pareto optimal set of feasible designs. The iterative design synthesis loop is implemented by combining MOGAs with the SUGAR MEMS simulation tool. Given a high-level description of the device’s desired behavior, both the topology and sizing are generated. The topology or physical configuration includes the number and types of basic building blocks and their connectivity. The sizing of the designs entails assigning numerical values to parameterized building blocks. A sample from the pareto optimal set of designs is presented for a meandering resonator example, along with convergence plots."
1517,"A critical aspect of the design of a space structure is the prediction of the amplification factor. This factor is often estimated from comparison with similar structures, which can lead to costly errors. Adding viscoelastic patches enables an accurate prediction of the damping level of the structure since the viscoelastic patches become the main cause of damping for the structure. In this project, a test panel similar to large satellite feed panels is damped using five small viscoelastic patches. The location of the patches is optimized using the strain energy method. The amplification factor is obtained through a complex eigenvalue finite element analysis. The complex eigenvalue method is shown to be as accurate as the direct frequency analysis, but it runs much faster. The predicted amplification factor is within 15% of the experimental value which is a very good estimation for such a complex structure."
1518,"Hydraulic hoses are used in industrial machines to transmit power. These hoses have a physical stiffness that depends upon the hose size and type. If a computer model is used to predict the path a hose will take through a machine, accurate physical stiffness properties are required for the hose. These stiffness properties are not available from hose manufacturers and hose test methods are scarce in the available literature. This paper describes methods used for testing the bending stiffness, torsion stiffness, and axial stiffness of hydraulic hoses. Plots of the stiffness data are given for a sample hose."
1519,"Traditional methods for applying boundary conditions in finite element analysis require the mesh to conform to the geometry boundaries. This in turn requires complex meshing algorithms for automated mesh generation from CAD geometry, particularly when using quadrilateral and hexahedral elements. The 3D extension of the penalty boundary method (PBM) is presented as a method that significantly reduces the time required generating finite element models because the mesh is not required to conform to the CAD geometry. The PBM employs penalty methods to apply boundary conditions on a simple, regular mesh. The PBM also eliminates discretization error because boundary conditions are applied using CAD geometry directly rather than an approximation of the geometry."
1520,"This paper discusses the issues of integrating the Computer-Aided Design (CAD) and Computer-Aided Manufacturing (CAM) programs in commercial software. Integration was achieved through implementation of a computer-aided process planning (CAPP) system within the commercial software. The part model was imported into, or designed in, the commercial CAD system. Manufacturing information was then extracted from the part model by the CAPP system using commercial Application Programming Interfacing (API) methods. The CAPP system then uses the extracted information to produce a process plan consistent with the requirements of the commercial CAM module to produce Numerical Control (NC) code. The internal integration was accomplished using commercial API methods that dynamically bind the CAD, CAPP, and CAM into a single continuous application. These APIs are implemented using the Orbix middleware following the CORBA standard. A case study demonstrating the integration is presented. Strengths and weaknesses of integrating the CAD and CAM domains using APIs and middleware are discussed."
1521,"This paper investigates the number of degrees of freedom for geometric design of developable Bézier surfaces. The conditions for developability are derived geometrically from the de Casteljau algorithm and expressed as a set of equations that must be fulfilled by the Bézier control points. This set of equations enables us to infer important properties of developable Bézier patches that characterize the patch design and simplify its solution process. With one boundary curve freely specified in 3D space, five more degrees of freedom are available for the second boundary curve of the same degree. Imposing parametric or geometric continuities across the boundary of two adjacent developable Bézier patches results in a composite developable Bézier surface that has fewer degrees of freedom. This work provides the foundation for a systematic implementation of a computer-aided design system for developable Bézier surfaces."
1522,"In automotive lamps, an ideal paraboloid is the reflector shape of choice when lens optics is utilized. However, geometric distortions occur among manufactured automotive lamps. This paper discusses the effects of geometric distortions on spread, packing, and gradient of reflected light from automotive lamps. Relevant legal requirements set by Federal Motor Vehicle Safety Standard on the performance of automotive lamps are also discussed. A new parametric mathematical model is developed to represent the geometry of an ideal lamp reflector. A non-linear parametric estimation problem is formulated using the Box-Kanemasu modification of the Gauss method. An application of methodology is also presented in this paper. The results show significant distortions of paraboloidal reflector with respect to the ideal design-intent reflector geometry. The numerically calculated deviations of focal point, focal length and paraboloidal axis from the ideally designed reflector necessitate improvements in the tooling and the manufacturing process for better dimensional control."
1523,"In curve design, controlling the characteristics of the overall shape is difficult using conventional microscopic shape-information such as dimension. Curvature entropy as calculated by the distribution of curvature was proposed as the macroscopic shape-information, and was confirmed to represent the complexity of a shape. In order to adjust the shape recognition of human beings, a Markov process was introduced into the definition of the macroscopic shape-information. Shape-generation method using the macroscopic shape-information was developed, and was applied to the design of an automobile side-view. Thus, the possibility of curve design-support using the macroscopic shape-information was indicated."
1524,"In distributed product realization problems, new paradigms and accompanying software systems are necessary to support the collaborative work of geographically dispersed engineering teams from different disciplines who have different knowledge, experience, tools and resources. In the context of prototyping product using SFF technologies, digital interfaces are constructed between engineering teams, especially between design and manufacturing teams, to separate their product realization activities. Across digital interfaces, each engineering team holds its own perspective towards the product realization problem, and each controls a subset of design variables and seeks to maximize its own payoff function subject to individual constraints. That is, engineering teams act like players in a team sport (i.e., a game) cooperating to achieve a set of overall goals. Hence, we postulate the use of principles from game theory to model the relationships between engineering teams. In this paper, a decision template is used as a digital interface enabling information about product realization activities to be transferred between engineering teams. Three game protocols are used to facilitate collaborative decision making without iteration across digital interfaces. A simple product realization scenario is introduced to demonstrate the efficacy of inserting digital interfaces between design and manufacturing teams."
1525,"Three-dimensional laser scanning equipment is being used more frequently to convert clay model automobile designs to large, detailed meshes for computer-aided design of outer-body panels. The panels are generally composed of large, constant curvature patches with small local features, called character lines, superposed to give the car a distinctive look. Although modern laser scanners are very accurate and precise, their tolerances nevertheless admit meshes with geometric flaws that destroy the constant curvature of patches and make character lines nearly invisible in a reflection simulation. Thus, we require an algorithm to fair the mesh by restoring the intended curvature while minimizing the vertex displacemtents. Existing approaches such as Laplacian and curvature flow operators are not suitable because they tend to shrink the mesh and introduce a bias toward planar geometries. Our approach aims to solve both of these problems by fitting a least squares surface to a set of vertices adjacent to the target vertex and moving the target vertex vertically onto the least squares surface in a local coordinate system. This algorithm has linear time complexity in the number of vertices and makes convergence likely while eliminating the planar bias of other operators. We show the effectiveness of our operator with both geometric and real-world mesh examples."
1526,"With the continuous improvement of powerful computers, vehicle structural designs have been addressed using computational methods, resulting in more efficient development of new vehicles. Most simulation-based optimization generates deterministic optimal designs without considering variability effects in modeling, simulation, and/or manufacturing. This paper presents an application of a new stochastic optimization method for vehicle side impact design. Nonlinear response surface models are employed for approximations of the side impact related test performance functions to conduct this study. The main goal is to maintain or enhance the vehicle side impact test performance while minimizing the vehicle weight under various uncertainties. The new algorithm alleviates the computational burden of excessive model evaluations by estimating the objective and constraint functions during the optimization process through a reweighting method. The efficiency and accuracy of this algorithm is presented through an actual vehicle safety design problem."
1527,"This study used finite element models to assess potential benefits of selected unconventional features implemented in this study for occupant protection in side impact. These features include door lockdown, gullwing door with a corrugated aluminum panel and cross-car beams. The intrusion and intrusion velocity of the B-pillar were used as the parameters for measuring side impact protection performance. No attempt was made here to assess manufacturablity, design feasibility, mass implications or market interest."
1528,"A method is presented to reduce the size of models used in analysis of elastic systems. The reduction is accomplished using a multiresolution analysis applied to elasticity operators to average fine scale properties and behavior while limiting loss of information. In discretized form, the method results in smaller matrices that can be used as building blocks to construct larger systems. The principal application envisioned is in design problems involving complex structural systems, such as crash-worthiness design, where very intensive computations demand computational efficiency."
1529,"Parametric design facilitates mass customization, concurrent engineering, optimization, and other product development integration processes used during preliminary and detailed design stages. Automated methods for regenerating and accessing parametric models can significantly reduce the time-to-market of new products. Applications that automate this process have inherent limitations based on the available features in commercial CAD systems. New technologies such as the "
1530,"In this paper, the problem of selecting from among a set of alternatives using multiple, potentially conflicting criteria is discussed. A number of approaches are commonly used to make these types of decisions in engineering design, including pair-wise comparisons, ranking methods, rating methods, weighted sum approaches, and strength of preferences methods. In this paper, we first demonstrate the theoretical and practical flaws with a number of these commonly employed methods. We then present a method based on the concept of hypothetical equivalents and expand the method to include hypothetical inequivalents. We demonstrate the strengths and weaknesses of the various decision making approaches using an aircraft selection problem. The design of a research laboratory is used to demonstrate the method of hypothetical equivalents further."
1531,"We present an automated method to aid a Decision Maker (DM) in selecting the ‘most preferred’ from a set of design alternatives. The method assumes that the DM’s preferences reflect an implicit value function that is quasi-concave. The method is iterative, using three approaches in sequence to eliminate lower-value alternatives at each trial design. The method is interactive, with the DM stating preferences in the form of attribute tradeoffs at each trial design. We present an approach for finding a new trial design at each iteration. We provide an example, the design selection for a cordless electric drill, to demonstrate the method."
1532,"The ability to select a design alternative, from a set of feasible alternatives, that is likely to meet customers’ and designer’s preferences and also account for uncertainties is vital to the success of a product design process. This paper presents a new metric, a Customer-based Expected Utility ("
1533,"Traditional dimensional analysis techniques for predicting the performance characteristics of a product can be greatly improved in both accuracy and domain of applicability by the infusion of empirical data, derived from material tests, into the equations that characterize the system parameters of interest. Advanced similarity methods are investigated which overcome the constraints associated with the traditional methods and provide increased analysis capability and improved insight into the phenomenon governing the problem. Such capability greatly increases the design toolbox available to product developers, across a large range of scale and application. It also significantly enhances a developer’s choices for prototype portioning during a development cycle. Solid mechanics and heat transfer applications are used to illustrate the basic utility of the methods."
1534,"This paper presents a method for systematically decomposes product geometry into a set of components considering the structural stiffness of the end product. A structure is represented a graph of its topology, and the optimal decomposition is obtained by combining FEM analyses with a Genetic Algorithm. As a case study, the side frame of a passenger car is decomposed for the minimum distortion of the front door panel geometry, where spot-welded joints are modeled as torsional springs. First, the rates of the torsional springs are treated as constant values obtained in the literature. Second, they are treated as design variables within realistic bounds. By allowing the change in the joint rates, it is demonstrated that the optimal decomposition can achieve the smaller distortion with less amount of joint stiffness (hence less welding spots), than the optimal decomposition with the typical joint rates available in the literature."
1535,"Functional design is a process in engineering design that dominates the key features of the result to be developed. Designing good functions that both satisfies the requirements and leads to better results is a challenge due to uncertainties on the consequences of the selected functions, and the lack of analysis methods for identifying the properties of function structures. Therefore, extensive experiences are usually required for functional design. This research argues that the physical relationships among the resulting components of a design are the consequences of functional dependencies developed during the functional design process. Therefore based on the understanding of functions and functional dependencies, a reasoning procedure can be developed to predict the performance properties of the design so that the effectiveness of the functional design can be evaluated at an early design stage. This paper proposes a dependency-based function modeling and analysis method that can be applied to represent and assess functions and function structures at the functional design stage. Designers can predict the properties of the functions they designed without having to have similar design experiences. An application software is also developed to implement the method and demonstrate its effectiveness."
1536,"A method for the rational choice of control, design and error variables in optimization problems is devised, based on the reduction of the maximum matching of the problem graph to the Dulmage-Mendelsohn canonical form. The method allows the designer to find with minimum effort appropriate sets of control, design and error variables that lead to an ultimate decomposition in design optimization problems of any dimension. In design automation, this procedure is useful as a rationale to plan manual interventions, where designers guide the process according to domain-specific knowledge. The proposed technique is rigorous and intuitive, thanks to the application of sound graph-theoretic concepts. A real-life example of mechanical engineering design shows the applicability of the method."
1537,"This paper presents a method of assembly synthesis focused on the in-process adjustability, where assembly synthesis is defined as the decomposition of the end product design prior to the detailed component design phase. Focusing on the effect of joint configurations on dimensional integrity of complex assemblies, the method recursively decomposes a product configuration and assigns joint configurations according to simple rules, in order to achieve a designed dimensional adjustability and non-forced fit. The rules employed during the decomposition process are drawn from the previous works of assembly design. An augmented AND/OR graph is utilized to represent a process of assembly synthesis with the corresponding assembly sequences, and the algorithm for generating the AND/OR graph is discussed. The method is applied to two dimensional skeletons of product designs at very early stage of the design process. The relation of the assembly synthesis to Datum Flow Chain (Mantripragada and Whitney, 1998) is discussed. It is also shown that each final design from the assembly synthesis defines its own Datum Flow Chain."
1538,"This paper involves the development, implementation and testing of a geometric representation scheme for building feasible sheet metal components. The approach taken here is based on prior shape grammar methods for engineering design. A series of seventeen grammar rules have been developed to represent a variety of cutting and bending operations that can be applied to sheet metal to construct feasible shapes. The implemented system has benefits as both a user interaction tool or as the basis for a computational design synthesis approach for designing sheet metal components. An example of a real component is shown as well as the method for invoking the sheet metal grammar to create this component."
1539,"The objective of this paper is to understand and study sustainable product development using a systems approach. This paper focuses on the development of the “Factor X” model to study diffusion of eco-innovation strategies. The concept of “Factor X” is qualitatively similar to the concepts of eco-efficiency and dematerialization with an added quantitative edge. It focuses on reducing the environmental impact of economic activities in quantitative terms such as reducing environmental impacts by a factor of 4 or factor of 10, etc. In the real world, artifacts interact with each other to form a web structure (known as artifact system (AS)), which has its own laws of evolution. We have implemented the “Factor X” framework through an adaptive agent based simulation (AABS) model to study the implications of different AS scenarios (achieved by setting system defining parameters) on the propagation and extent of diffusion of eco-innovation strategies."
1540,"Designing forged gears with constrained involute tooth profile is a time-consuming activity with high costs due to the complexity in the plastic deformation, machine limitations, and trial-error iterative methods used to design dies and develop process conditions. Recently, knowledge-based systems are proving to be a powerful tool and a great potential for developing intelligent design support systems to improve quality of products and reduce costs by eliminating or minimizing many of the trail-error iterations involved in process design. This paper describes research work in developing a knowledge-based gear design and manufacturing system that assists des igners to reduce the time needed to design precision forged solid gears and provide detailed process specification along with the required process model implemented in it. The system integrates knowledge about all aspects of gear design and manufacturing and provides powerful reasoning and decision-making capabilities for reducing the time between detailed design and final production. Once the user specifies the basic design requirements, the system automatically carries out geometric calculations and strength analysis according to American Gear Manufacturers Association (AGMA) standards, power rating and Finite Element Analysis (FEA) based techniques. With a successful power rating achieved, the system automatically feeds input parameters into the GEARFORGING program and carries out process planning for the gear forging process. Estimation of the number of preforming stages required, generation of detail die drawings, geometric parameters, and estimation of forging load and energy requirements of the process are calculated based on available material design databases, knowledge-based rules and feature-level calculations. The system generates the whole forging process from the billet to the final gear product. The results have been compared with those available in the current literature and good agreement has been demonstrated."
1541,"Parts representing the aerospace, automotive, and power generation industries were machined using Curvature Matched Machining, Sturz milling, and traditional 3-axis ball-end methods. The performance of these methods were compared using the benchmarks of tool path length, surface finish, and post-machining finishing time. Curvature Matched Machining proved to be superior method for the parts studied. Significant reductions in tool path length were observed (over 80% in some instances). Improvements were also realized in surface finish and post-machining operations."
1542,"Nonlinear constrained optimization algorithms are widely utilized in artifact design. Certain algorithms also lend themselves well to design of experiments (DOE). "
1543,"Approximation models (also known as metamodels) have been widely used in engineering design to facilitate analysis and optimization of complex systems that involve computationally expensive simulation programs. The accuracy of metamodels is directly related to the sampling strategies used. Our goal in this paper is to investigate the general applicability of sequential sampling for creating global metamodels. Various sequential sampling approaches are reviewed and new approaches are proposed. The performances of these approaches are investigated against that of the one-stage approach using a set of test problems with a variety of features. The potential usages of sequential sampling strategies are also discussed."
1544,
1545,"A large rigid body rotation of a finite element can be described by changing the definition of the axes of the element coordinate system or by keeping the axes unchanged and change the slopes or the position vector gradients. In the first method, the definition of the local element parameters (spatial coordinates) changes with respect to a body or a global coordinate system. The use of this method will always lead to a nonlinear mass matrix and non-zero centrifugal and Coriolis forces. The second method, in which the axes of the element coordinate system do not rotate with respect to the body or the global coordinate system, leads to a constant mass matrix and zero centrifugal and Coriolis forces when the absolute nodal coordinate formulation is used. This important property remains in effect even in the case of flexible bodies with slope discontinuities. The concept employed to accomplish this goal resembles the concept of the "
1546,"The objective in product platform design is to synthesize a set of components that will be shared by a number of product variants considering potential sacrifices in individual product performance that result from parts sharing. "
1547,"Product variety can be provided more efficiently and effectively by creating families of products based on product platforms. One of the major advantages of the development of product platforms is the facilitation of an overall product development strategy, and an important factor in product development is the evolution of a family of products, including addition and retirement of products as well as changing demand and associated production quantities. In this paper, we present a quantitative approach for designing "
1548,"Product portfolio valuation is a core business milestone in a firm’s product development process: Determine what will be the final value to the firm derived from allocating assets into an appropriate product mix. Optimal engineering design typically deals with determining the best product based on technological (and, occasionally, cost) requirements. Linking technological with business decisions allows the firm to follow a product valuation process that directly considers not only what assets to invest but also what are the appropriate physical properties of these assets. Thus, optimal designs are determined within a business context that maximizes the firm’s value. The article demonstrates how this integration can be accomplished analytically using a simple example in automotive product development."
1549,"This paper presents an on-going research effort on platform-based product family design using a knowledge intensive support paradigm. The background research related to product family design is first reviewed. Then, the fundamental issues underlying the product family design are discussed. A module-based integrated product family design scheme is proposed with knowledge support for customer requirements’ modeling, product architecture modeling, product platform establishment, product family generation, and product assessment. The systematic methodology and the relevant technologies are investigated and developed for knowledge modeling and support in the product family design process. An information and knowledge-modeling framework is developed for the module-based product family design scheme. The issues and requirements related to develop knowledge intensive support system for module-based product family design are also addressed. Finally, a case study on knowledge support for power supply family design and evaluation is provided for illustration."
1550,"Designing a family of product variants that share some components usually entails a performance loss relative to the individually optimized variants due to the commonality constraints. Choosing components for sharing may depend on what performance losses can be tolerated. This article presents a methodology for making commonality decisions while controlling individual performance losses. Previous work focused on evaluating individual performance losses due to pre-specified sharing. Trade-offs were identified for different platforms (i.e., the sets of components shared among products) by means of Pareto sets. In the present work an optimal design problem is formulated to choose product components to be shared without exceeding a user-specified performance loss tolerance. This enables the designer to control trade-offs and obtain optimal product family designs for different levels of performance losses in an attempt to maximize commonality. A family of automotive side frames is used to demonstrate the approach."
1551,"Parametric modeling has become a widely accepted mechanism for generating data set variants for product families. These data sets that include geometric models and feature-based process plans are created by specifying values for parameters within feasible ranges specified as constraints in the definition. The ranges denote the extent or envelope of the product family. Increasingly, with globalization the inverse problem is becoming important. This takes independently generated product data sets that on observation belong to the same product family and creates a parametric model for that family. This problem is also of relevance to large companies where independent design teams may work on product variants without much collaboration only to attempt consolidation later on to optimize the design of manufacturing processes and systems. In this paper we present a methodology for generating a feature-based part family parametric model through merging independently generated product data sets. We assume that these data sets are feature-based with relationships such as precedences captured using graphs. Since there are typically numerous ways in which these data sets can be merged, we formulate this as an optimization problem and solve using the A* algorithm. The parameter ranges generated by this approach will be used to design appropriate Reconfigurable Machine Tools (RMTs) and systems (RMS) for manufacturing the resulting part family."
1552,"Several CAD system independent feature recognition techniques have been developed to drive manufacturing applications. Commercial implementations of these techniques require translating CAD models using STEP or other neutral file formats. With large CAD models found in some application domains; e.g., powertrain machining, corresponding STEP files are also large. This leads to large processing times. Another approach is to use lightweight formats such as STL or VRML. Here, complete & accurate parameter extraction is difficult because these formats approximate surfaces as tessellations. This paper discusses a new methodology for feature recognition, in which a VRML file is used for feature identification. To some extent, parameters of faces with simple surface-types are recovered from the tessellated model. If identified features consist of faces whose parameters are not recovered from the tessellated model, a partial STEP file translation is used for extracting exact parameters. This CAD system independent algorithmic development and implementation reduces the amount of data exported to neutral files, thus leading to more efficient feature recognition."
1553,"CAE-Based simulation and Design of Experiments (DoE) are becoming mature and increasingly effective in development of complex industrial products such as automobiles. We present in this paper a CAE mesh-modeling paradigm that ultimately led to fast, automatic generation of a family of meshes based on a base design. This paradigm is hinged on the so-called mesh features to achieve productivity for modeling CAE meshes. Mesh features are self-contained mesh deformation operations that are context-free, stored separately from the base model, and can be applied to the model in a proper mix at any time. Libraries of mesh features can also be established to archive useful features for future use. Furthermore, by assigning mesh features for DoE factors, one can specify for the system the proper way to assemble features and apply them automatically to the base model to generate input meshes for a DoE study. Automatic generation of a family of DoE input meshes results in maximum time savings and minimum chances for errors, especially for applications involving large-scale CAE models."
1554,"For several years, adaptive range genetic algorithms have been developed and it turned out to be very efficient in treatment of both discrete variables and continuous variables. That means it is useful tools for mixed variable problem, which was thought most daunting problem in design engineering. We have shown its effectiveness through benchmark problems and outperformed the other method in its accuracy. In this paper, we will newly develop a new type of Adaptive Range Genetic Algorithms, which will consider inheritance of searching range. We have demonstrated its effectiveness through benchmark problem and showed its characters. Through numerical example, we showed the stability and effectiveness of the proposed method compared with the conventional method."
1555,"The solving strategy of GA-Based Multi-objective Fuzzy Matter-Element optimization is put forward in this paper to the kind of characters of product optimization such as multi-objective, fuzzy nature, indeterminacy, etc. Firstly, the model of multi-objective fuzzy matter-element optimization is created in this paper, and then it defines the matter-element weightily and changes solving multi-objective optimization into solving dependent function "
1556,"In this paper an optimization approach is used to solve the problem of finding the minimum distance between concave objects, without the need for partitioning the objects into convex sub-objects. Since the optimization problem is not unimodal ("
1557,"Increased commonality in a family of products can simplify manufacturing and reduce the associated costs and lead-times. There is a tradeoff, however, between commonality and individual product performance within a product family, and in this paper we introduce a genetic algorithm based method to help find an acceptable balance between commonality in the product family and desired performance of the individual products in the family. The method uses Design of Experiments to help screen unimportant factors and identify factors of interest to the product family and a multiobjective genetic algorithm, the non-dominated sorting genetic algorithm, to optimize the performance of the products in the resulting family. To demonstrate implementation of the proposed method, the design of a family of three General Aviation Aircraft is presented along with a product variety tradeoff study to determine the extent of the tradeoff between commonality and individual product performance within the aircraft family. The efficiency and effectiveness of the proposed method is illustrated by comparing the family of aircraft against individually optimized designs and designs obtained from an alternate gradient-based multiobjective optimization method."
1558,"The present paper introduces a new methodology for designing machine element shapes. The element is represented using non-uniform rational B-Spline (NURBS) in order to give it a form of shape flexibility. A special form of genetic algorithms known as real-coded genetic algorithms is used to conduct the search for the design objectives. Shape optimization of 3D C-frames are used as an application of the proposed methodology. The design parameters of these frames include the dimensions of their cross-sections, which should be chosen to withstand the applied loads and minimize the element’s overall weight. In a further development, the hybridization of different optimization methods has been used to find the optimum shape of the element. Real coded genetic algorithm is used as a random search method, while Nelder-Mead is used as a direct search method, where the result of the genetic algorithm search is used as the starting point of direct search. The results showed that the use of Nelder-Mead with Real coded Genetic Algorithms has been very significant in improving the optimum shape of a solid 3D C-frames subjected to a combined tension and bending stresses. The hybrid optimization method could be extended to more complex shape optimization problems. For the purpose of analysis, curved beam theory is applied on local cross-sections on the NURBS surface. A finite elements analysis was conducted on SDRC-IDEAS for verifying the results obtained using the curved beam theory."
1559,"In this paper we present a hybrid optimization approach to perform robust design. The motivation for this work is the fact that many realistic engineering systems are mutimodal in nature with multiple local optima, and moreover may have one or more uncertain design parameters. The approach that is presented utilizes both local and global optimization algorithms to find good design points more efficiently than either could alone. The mean and variance of the objective function at a design point is calculated using Monte Carlo simulation and is used to drive the optimization process. To demonstrate the usefulness of this approach a case study is considered involving the design of a beam with dimensional uncertainty."
1560,"This paper presents a nonlinear kinematic tolerance analysis algorithm for higher pairs based on configuration space construction. The input is a pair of planar parts whose profiles consist of line and circle segments. Each part translates along a planar axis or rotates around an orthogonal axis. The part shapes and motion axes are parameterized by a vector of tolerance parameters with range limits. The algorithm constructs a generalized configuration space, called a contact zone, that bounds the worst-case kinematic variation over the tolerance parameter range. The zone specifies the variation of the pair at every configuration and in every operating mode. It reveals qualitative changes in kinematic function, such as jamming, that can be caused by parameter variations. Case studies show that the algorithm is more accurate and detects more failure modes than a prior algorithm that computes a linear approximation of the contact zone."
1561,"This paper presents a study on the sensitivity of the dynamics of parallel kinematic mechanisms (PKMs) and their effects on the system identification of inertial parameters. Comparing the sensitivity of the individual terms in the equations of motion, a delineation of areas prone to convergence errors in the presence of measurement noise and design parameter variations can be characterized. The design parameter variations evaluated were the strut masses, platform mass, joint location errors and friction. Comparative observations are made using sample trajectory based characterizations. Detailed results are presented for the University of Florida Special 6-6 PKM."
1562,"Symbolic estimation of base inertia parameters is attractive. All published symbolic methods, however, are complicated and can only be applied to rotational joints and translational joints. This paper presents a new simple symbolic method to determine base inertia parameters for general mechanisms that include spherical joints and universal joints. The concepts of mass transfer and moment of inertia transfer are successfully extended to spatial joints and four propositions are developed based on the concepts. With the four propositions, a set of symbolic base parameters can be determined directly from the inspection of the joint configurations of general mechanisms. The determination of symbolic base inertia parameters of a spatial four bar mechanism is presented as an application example."
1563,"This paper presents an industrial case study in which a spatial higher pair is redesigned using our configuration space method of kinematic analysis. The task is to remove occasional blocking in an asynchronous reverse gear pair from a car transmission. A systematic kinematic analysis is required because the blocking configurations are unknown and because very few initial configurations cause blocking. We use our configuration space method of kinematic analysis to solve the problem. We determine why the gears block by constructing a series of two-dimensional configuration spaces that model the engagement kinematics. Blocking occurs when two consecutive pairs of teeth make contact during engagement. The gear angles at the contact determine whether or not the gears will block. Our analysis determines that blocking occurs in 4% of the angle space. Fine tuning the gear parameters reduces the range to 0.5%, but cannot eliminate the blocking. Removing every second gear tooth eliminates the blocking. The analysis results are consistent with the experimental data. The case study demonstrates that the configuration space method helps solve industrial problems that are outside the scope of prior work."
1564,"In this paper, error sensitivity analysis is discussed for the purpose of optimal calibration of parallel kinematic machines (PKMs). The idea is to find a less error sensitive area in the workspace for calibration. To do so, an error model is developed that takes into consideration all the geometric errors due to imprecision in manufacturing and assembly. Based on this error model, it is shown that the error mapping from the geometric errors to the pose error of the PKM depends on the Jacobian inverse. The Jacobian inverse would introduce spurious errors that would affect the calibration results, if used without proper care. Hence, it is suggested to select the areas in the workspace with smaller condition numbers for calibration. A case study is presented to illustrate the proposed method."
1565,"The Internet and World Wide Web (WWW) are now evolving as an important communication technology and a major information resource provider for industry. This paper discusses the development and implementation of a knowledge-based gear design and manufacturing system over the Internet to create new integrated design and manufacturing environments. By providing access to an interactive web-based support system, any designer with a WWW browser becomes a potential user of this on-line design system. Once connected, the designer follows the system instructions and submits the necessary input data on the appropriate web pages. The server receives a request from a client and invokes a CGI (Common Gateway Interface) program that processes the information provided through the user Interface. The CGI parses the data and has the ability to remotely run the knowledge-based gear design system that integrates knowledge about all aspects of gear design and manufacturing and provides powerful reasoning and decision-making capabilities for reducing the time between gear tooth creation, detailed design and manufacturing process specification via the Internet. When the execution is completed, full specifications definition, geometry, kinematic-loads and stresses are determined through Finite Element Analysis (FEA) within ANSYS, VRML models of the gear pair and gearbox assembly including gears, shafts, bearings and housing are exported according to the designer requests and sent back to designer on the web browser. To accomplish this, a combination of HTML, JavaScript, VRML, CGI Script and C++ is used. Finally, an example on spur gear design utilizing a parallel gearbox design model configuration is discussed."
1566,"Understanding the global feasibility of engineering decision-making problems is fundamental to the synthesis of rational engineering decisions. An Extensive Simplex Method is presented to solve the global feasibility for a linear decision model relating multiple decision variables to multiple performance measures, and constrained by corresponding limits. The developed algorithm effectively traverses all extreme points in the feasible space and establishes the graph structure reflecting the active constraints and their connectivity. The algorithm demarcates basic and nonbasic variables at each extreme point, which is exploited to traverse the active constraints and merge the degenerate extreme points. Finally, a random model generator is presented with the capability to control the matrix sparseness and the model degeneracy for an arbitrary number of decision variables and performance measures. The results indicate that all these model properties are significant factors affect the total number of extreme points, their connected graph, and the global feasibility."
1567,"Comparing to the traditional sequential design approach, the integrated structure and control design approach integrates the mechanical structure design and the control system design by formulating the design as an optimization problem. This paper investigates two important questions pertaining to the integrated structure and control design approach: "
1568,"This paper presents a computational method for calculating the shortest path along the surface of a product assembly between two components. The goal of this method is to check whether or not there is sufficient distance between two electrical components to prevent the occurrence of a spark between them. Our approach is an approximating method using a discrete weighted graph. To improve accuracy we have added two methods, geometric improvement and K-shortest path. Through experiments, we show that our method is effective for the quality assurance of electric components."
1569,"Current Computer Aided Design (CAD) technologies offer parametric design functions to allow users to easily change products’ configurations, shape and dimensions without reconstructing the entire product model. A programming tool that often comes with most CAD packages enables a designer to better control the parametric interface. However, these two functions focus on rapid production of computer models, which usually takes place after the product design is completed. The product design process, which has more significant influence on product life-cycle costs, is not fully supported. This paper proposes a systematic method in which a product design can be finalized and optimized through the interactions between 3-D solid modeling and customized cost / performance analysis. The entire optimal design process and generation of design deliverables is fully automated through interactive programming. In addition, an automatic design and optimization system for industrial silencers has been developed, which takes customers’ order from the Internet, sends the order to a CAD system, generates the optimal design, and sends back the design to the customer. The entire process takes only a few minutes. The proposition of integrating customized product life-cycle considerations with the model generation and optimization, as well as the developed silencer design system should be useful for other product designs."
1570,"This paper outlines a methodology for optimising the multi-domain architecture of a relatively integrated system through an appropriate level of modularisation to maximise societal value created. This method is developed through the application of real options theory and the dependency structure matrix (DSM), and illustrated using a reference example of an industrial gas turbine."
1571,"A design methodology is developed to achieve optimum design of tensioner in serpentine belt drive systems. System component responses to a harmonic excitation from the crankshaft are obtained analytically by using a complete multi-degree of freedom model and also in explicit expressions using an equivalent single-degree of freedom model. Sequential quadratic programming and Kuhn-Tucker methods are applied to obtain the optimum design of the system modeled as multi-degree of freedom and single-degree of freedom respectively, with the objective of minimizing the undesired vibration of system components. It is shown that system vibration behavior improves substantially by optimizing the design of tensioner device."
1572,"This paper outlines the need and describes the design of a modular software library and configurable and extensible framework built to support research into developing design automation techniques and methods for use during the early stages of design. Simple examples are provided to illustrate the extensibility and configurability of the complete system and a brief comparison of features is made with other existing systems. Lastly, a few preliminary results are provided to demonstrate the early potential of the system."
1573,"In this paper, an entropy-based metric is presented for quality assessment of non-dominated solution sets obtained from a multiobjective optimization technique. This metric quantifies the ‘goodness’ of a solution set in terms of its distribution quality over the Pareto-optimal frontier. Therefore, it can be useful in comparison studies of different multi-objective optimization techniques, such as Multi-Objective Genetic Algorithms (MOGAs), wherein the capabilities of such techniques to produce and maintain diversity among different solution points are desired to be compared on a quantitative basis. An engineering test example, the multiobjective design optimization of a speed-reducer, is presented in order to demonstrate an application of the proposed entropy metric."
1574,"This paper introduces the Sensitivity-based Pattern Search (SPS) algorithm for 3D component layout. Although based on the pattern search algorithm, SPS differs in that at any given step size the algorithm does not necessarily perturb the search space along all possible search dimensions. Instead all possible perturbations, or moves are ranked in decreasing order of their effect on the objective function and are applied in that order. The philosophy behind this algorithm is that moves that affect the objective function more must be applied before the moves that affect the objective function less. We call this effect on the objective function the "
1575,"A hybrid method combining a genetic algorithms based containment algorithm with a complex mating algorithm is presented. The approach uses mating between a pair of objects as means to accelerate the packaging process. In this study, mating between two objects has been defined as positioning one object relative to others by merging common features that are assigned through mating conditions between them. A constrained move set is derived from the mating condition that allows the transformation of a component in each mating pair to be fully or partially constrained with respect to the other. By using mating in the packaging, the number of components to be placed can be reduced significantly and overall speed of the packaging process can also be improved. The hybrid method uses a genetic algorithm to search mating pairs and global positions of selected objects. The mating pair is mated first by a simple mating condition which is derived from geometric features of mating objects. If a proper mating is not obtained, the complex mating algorithm finds an optimal mating condition using Quasi-Newton method."
1576,"The most significant design decisions are typically made during the conceptual phase of the engineering design process, when critical design features are proposed, evaluated and selected. In this paper, we explore the critical task of concept selection and propose a non-deterministic, optimization-based approach for selecting the most promising concept. The method presented in this paper builds upon the recently-proposed "
1577,"Probabilistic optimization design offers tools for making reliable decisions with the consideration of uncertainty associated with design variables/parameters and simulation models. In a probabilistic design, such as reliability-based design and robust design, the design feasibility is formulated probabilistically such that the probability of the constraint satisfaction (reliability) exceeds the desired limit. The reliability assessment for probabilistic constraints often involves an iterative procedure; therefore, two loops are involved in a probabilistic optimization. Due to the double-loop procedure, the computational demand is extremely high. To improve the efficiency of a probabilistic design, a novel method – sequential optimization and reliability assessment (SORA) is developed in this paper. The SORA method employs a single-loop strategy where a serial of cycles of optimization and reliability assessment is employed. In each cycle optimization and reliability assessment are decoupled from each other; no reliability assessment is required within optimization and the reliability assessment is only conducted after the optimization. The key concept of the proposed method is to shift the boundaries of violated deterministic constraints (with low reliability) to the feasible direction based on the reliability information obtained in the previous cycle. Hence the design is quickly improved from cycle to cycle and the computational efficiency is improved significantly. Two engineering applications, the reliability-based design for vehicle crashworthiness of side impact and the integrated reliability and robust design of a speed reducer, are presented to demonstrate the effectiveness of the SORA method."
1578,"Deterministic optimum designs that are obtained without consideration of uncertainty could lead to unreliable designs, which call for a reliability approach to design optimization, using a Reliability-Based Design Optimization (RBDO) method. A typical RBDO process iteratively carries out a design optimization in an original random space ("
1579,"A method of inspecting the design space of multivariable objective functions is proposed. By scanning 1 or 2 of the variables at a constant step while partially minimizing or maximizing the function with respect to the remaining variables, sets of points are generated that can be fiarther used in producing 2D or 3D diagrams. A number of examples are given for showing the usefialness of the method in studying the design space of objective functions and of the constraint activity. All graphs are produced with an in-house program that allows generation of logarithmically spaced level-curve diagrams and accurately truncating fimction surfaces over the z-axis at specified heights."
1580,"As our ability to generate more and more data for increasingly large engineering models improves, the need for methods for managing that data becomes greater. Information management from a decision-making perspective involves being able to capture and represent significant information to a designer so that they can make effective and efficient decisions. However, most visualization techniques used in engineering, such as graphs and charts, are limited to two-dimensional representations and at most three-dimensional representations. In this paper, we present a new visualization technique to capture and represent engineering information in a multidimensional context. The new technique, Cloud Visualization, is based upon representing sets of points as clouds in both the design and performance spaces. The technique is applicable to both single and multiobjective optimization problems and the relevant issues with each type of problem are discussed. A multiobjective case study is presented to demonstrate the application and usefulness of the Cloud Visualization techniques."
1581,"A structural-acoustic design optimization of a vehicle is presented using finite element and boundary element analyses. The steady-state dynamic behavior of the vehicle is calculated from the finite element frequency response analysis, while the sound pressure level within the acoustic cavity is calculated using the boundary element analysis. A reverse solution procedure is employed for the design sensitivity calculation using the adjoint variable method. An adjoint load is obtained from the acoustic boundary element re-analysis, while the adjoint solution is calculated from the structural dynamic re-analysis. The evaluation of pressure sensitivity only involves a numerical integration process for the structural part. Two design optimization problems are formulated and solved. It has been shown that the structural weight is saved when the noise level is maintained, and the weight needs to increase in order to reduce the noise level in the passenger compartment."
1583,"Engineering may be described as the main information and innovation source of a company. For an effective Engineering it is necessary to know well, to monitor, and to control all its processes and activities. Therefore a company needs a dynamic project and process navigation support for every user, which allows to model, to run, to react dynamically, and to evaluate activities in order to process these within given requirement, time, and cost frames. In this contribution, a system for dynamic project navigation in Engineering is presented with aspects of modelling, improvement and evaluation of processes."
1584,"In business theory there are no suitable benefit evaluation procedures for new technologies (e.g. CAD/CAM systems, EDM/PDM systems) in product development. Another problem is the missing process orientation as well as an inadmissible mix of quantifiable and qualitative benefits (if they are not be even neglected). Hence, the results are difficult to comprehend ([Schabacker, 2001], [Bauer, 1995])."
1585,"Research conducted over the past decade has resulted in a suite of methods for robust design which can be applied during different design stages. These methods focus on reducing the sensitivity of the design to variation without removing its causes. In this research we are investigating an additional and very powerful means for achieving robustness that complements the other methods developed to date. We have dubbed this area “smart assemblies.” A smart assembly has features, not otherwise required by the function of the design, which allow the design to absorb or cancel out the effects of variation. In this paper we report our results to date. We discuss the close connection between smart assembly design and exactly constrained design. We present the beginnings of a smart feature classification system, a preliminary methodology for smart assembly design, and a case study."
1587,"Over the last several years we have developed the Berkeley Solid Interchange Format (SIF) for layered manufacturing data exchange. By building both design software that outputs SIF as well as manufacturing software that processes the SIF input files, we gained insights into the concerns of both sides of data exchange — insights which often led to major changes in successive versions of the format. In this paper, we share some of the most important lessons we learned (many of which are applicable to all geometric data exchange, not merely for layered manufacturing) and explain how they shaped SIF."
1588,"This paper presents a new complete CAD-driven rapid prototyping system for polystyrene. ModelAngelo, as it is called, consists of six subsystems featuring hardware and software. The system utilizes a virtual 3D CAD model to produce a set of commands used to control a 5-axis CNC Machine to move a heated-wire tool into a block of polystyrene to form the desired surface. The ultimate goal of ModelAngelo is to reach the stage of “what you see is what you get”. An overview of ModelAngelo system is first presented followed by a detailed discussion of ModelAngeloSoftware . The various mechanical subsystems responsible for realizing the final product are then described. Advantages and disadvantages of several tooltip designs are discussed and the best solution is selected. The main features of the robotic arm, which carry the tooltip is also discussed in detail. Finally ModelAngelo controller that manages the interaction between software and hardware is briefly highlighted and possible applications of ModelAngelo are suggested."
1589,"With the integration of multi-axis layered manufacturing and material removal (machining) processes, a hybrid system has more capability and flexibility to build complicated geometry with a single setup. Process planning to integrate the two different processes is a key issue. In this paper, an algorithm of adaptive slicing for five-axis Laser Aided Manufacturing Process (LAMP) is summarized which can generate uniform- or non-uniform slices. In order to avoid interruption in the deposition process for one slice, a skeleton-based offset deposition tool-path method is used to generate continuous moving paths. A method to build a non uniform (thickness) layer which utilizes two processes is presented and an overall algorithm for integration is described. The newly developed algorithm implemented in the process planning helps the hybrid system build part more efficiently."
1590,"This paper discusses implementation of an implicit solid modeling approach to the representation of heterogeneous objects, i.e. solids whose material composition is not uniform. We present a brief review of related literature and then focus on implementation of heterogeneous implicit solid modeling (H-ISM) using a computer algebra system. Several heterogeneous implicit solid models of multi-material objects from standard part catalogues are presented to demonstrate the effectiveness of H-ISM implementation."
1591,"Model validation has become a primary means to evaluate accuracy and reliability of computational simulations in engineering design. Mathematical models enable engineers to establish what the most likely response of a system is. However, despite the enormous power of computational models, uncertainty is inevitable in all model-based engineering design problems, due to the variation in the physical system itself, or lack of knowledge, and the use of assumptions by model builders. Therefore, realistic mathematical models should contemplate uncertainties. Due to the uncertainties, the assessment of the validity of a modeling approach must be conducted based on stochastic measurements to provide designers with the confidence of using a model. In this paper, a generic model validation methodology via uncertainty propagation is presented. The approach reduces the number of physical testing at each design setting to one by shifting the evaluation effort to uncertainty propagation of the computational model. Response surface methodology is used to create metamodels as less costly approximations of simulation models for uncertainty propagation. The methodology is illustrated with the examination of the validity of a finite-element analysis model for predicting springback angles in a sample flanging process."
1592,"This paper deals with the development of simulation-based design models under uncertainty, and presents an approach for building surrogate models and validating them for their efficacy and relevance from a design decision perspective. Specifically, this work addresses the fundamental research issue of how to build such surrogate models that are computationally efficient and sufficiently accurate, and meaningful from the viewpoint of its subsequent use in design. Towards this goal, this work presents a Bayesian analysis based iterative model building and model validation process leading to reliable and accurate surrogate models, which can then be invoked in the final design optimization phase. The resulting surrogate models can be expected to act as abstractions or idealizations of the engineering analysis models and can mimic system performance in a computationally efficient manner to facilitate design decisions under uncertainty. This is accomplished by first building initial models, and then refining and validating them over many stages, in line with the iterative nature of the engineering design process. Salient features of this work include the introduction of a novel preference-based design screening strategy nested in an optimally-selected prior information set for validation purposes; and the use of a Bayesian evaluation based model-updating technique to capture new information and enhance model’s value and effectiveness. A case study of the design of a windshield wiper arm is used to demonstrate the overall methodology and the results are discussed."
1593,"This work proposes an uncertainty metric to capture and encode parametric uncertainty information that will enable engineering decision analyst to combine and compute probabilities of expected outcomes through mathematical constructs of joint probability functions. Its integration in a mechanical design system can be expected to facilitate simulation-based design under uncertainty. Specifically, the proposed technique helps to study the impact of the probabilistic nature of the input design or state variables and by applying the concept of failure probability aims to generate the corresponding probabilistic information of the output performance function. This work is based on evaluating a series of probabilities that the output cannot exceed a certain value for a given perturbed value of the design point. In this context, this paper reviews the First Order Second Moment (FOSM) reliability theory where the random parameters influencing the design appear only through their means and co-variances. Building on these works, an alternate approach is presented taking into account the fact that the system output is all the more influenced by functional constraints in the system, which if ignored can lead to inaccurate or irrelevant error estimation and could seriously affect subsequent posterior decision analysis. This work includes a reliable and efficient error estimation procedure to identify design points that violate boundary conditions through methodical constraint evaluations and subsequent adjusting of output estimation values. The proposed method is illustrated with the aid of a constrained optimization case study and an I-beam design problem."
1594,"This paper presents a probabilistic design system (ADAPRES_NET) in which structural reliability problems are addressed by applying structural reliability methods along with a response surface method in a distributed computing environment. In this study, first the value of using distributed network computing for structural reliability analyses is explained. Secondly, a basic structure of distributed networking is given in which the fundamental concepts of network programming for structural reliability analysis are explained. Finally, the use of ADAPRES_NET is demonstrated for a case study involving the concurrent application of structural reliability analyses."
1595,"This paper presents a method for carrying out a quantitative evaluation of design concepts with incomplete assessment information. When evaluating design solutions, in addition to consideration of product functionality, quality and cost, product life cycle performance such as maintainability should also be evaluated. Product maintainability issues are one of the more difficult design aspects to evaluate in early design stage. This paper describes specific maintainability metrics for evaluating product maintenance of conceptual design alternatives. Because of the uncertainty associated in early stage design evaluation, the varying degree of customer expectation must be incorporated into the evaluation system. Non-traditional fuzzy sets are used to represent expectations of the customer and compare them to design solution parameters. A case study is presented to illustrate the design method."
1596,"Tolerance analysis is the most important issue in computer-aided fixture design (CAFD) since the primary task of fixturing is to ensure the quality of machining, and it is an important part in computer-aided fixture design verification (CAFDV). This study presents a new approach for fixture tolerance analysis that is more generalized and can be used to assign locator tolerances based on machining surface tolerance requirements. The tolerance analysis is also generalized to handle any type of fixture designs, workpieces, datum features, and machining feature tolerances. Locator tolerance assignment distributes tolerances to locators based on a sensitivity analysis."
1597,"Traditionally tolerances for manufactured parts are specified using symbolic schemes as per ASME or ISO standards. To use these tolerance specifications in computerized tolerance synthesis and analysis, we need information models to represent the tolerances. Tolerance specifications could be modeled as a class with its attributes and methods [ROY01]. Tolerances impose restrictions on the possible deviation of features from its nominal size/shape. These variations of shape/size of a feature could be modeled as deviation of a set of generalized coordinates defined at some convenient point on the feature [BAL98]. In this paper, we present a method for converting tolerance specifications as per MMC (Maximum Material Condition) / LMC (Least Material Condition) / RFS (Regardless of Feature Size) material conditions for standard mating features (planar, cylindrical, and spherical) into a set of inequalities in a deviation space for representation of deviation of a feature from it’s nominal shape. We have used the virtual condition boundaries (VCB) as well as tolerance zones (as the case may be) for these mappings. For the planar feature, these relations are linear and the bounded space is diamond shaped. For the other cases, the mapping is a set of nonlinear inequalities. The mapping transforms the tolerance specifications into a generalized coordinate frame as a set of inequalities. These are useful in tolerance synthesis, and analysis as well as in assemblability analysis in the generalized coordinate system (deviation space). In this paper, we also illustrate the mapping procedures with an example."
1598,"This paper describes an intuitive way of defining geometry design variables for solving structural topology optimization problems using an evolutionary algorithm (EA). The geometry representation scheme works by defining a skeleton which represents the underlying topology/connectivity of the continuum structure. As the effectiveness of any EA is highly dependent on the chromosome encoding of the design variables, the encoding used here is a graph which reflects this underlying topology so that the genetic crossover and mutation operators of the EA can recombine and preserve any desirable geometric characteristics through succeeding generations of the evolutionary process. The overall optimization procedure is applied to design a straight-line compliant mechanism : a large displacement flexural structure that generates a vertical straight line path at some point when given a horizontal straight line input displacement at another point."
1599,"Extensive literature exists on the topology design of single-component structures while multi-component structural systems have received much less attention. In this paper, we present a technique for optimizing the topology of a structure that should be connected to one or more pre-designed components to maximize the stiffness of the overall assembly. We call it an embedding problem because pre-designed components are to be optimally positioned and oriented within a design region while the connecting structure’s topology is optimized simultaneously. Continuous design variables are used to vary the locations of the embedded objects smoothly along with the topology of the connecting structure in order to apply gradient-based optimization algorithms. A new material interpolation function on the basis of normal distribution function is used for this purpose. Optimality criteria method combined with the steepest descent method is used to minimize mean compliance to obtain the stiffest structure for a given volume of material. As a special case of this method, topology optimization of multi-component structural systems is also considered. Illustrative examples are presented."
1600,"Auxetic structures are ones, which exhibit an in-plane negative Poisson ratio behavior. Such structures can be obtained by specially designed honeycombs or by specially designed composites. The design of such honeycombs and composites has been tackled using a combination of optimization and finite elements analysis. Since, there is a tradeoff between the Poisson ratio of such structures and their elastic modulus, it might not be possible to attain a desired value for both properties simultaneously. The presented work approaches the problem using evolutionary multiobjective optimization to produce several designs rather than one. The algorithm provides the designs that lie on the tradeoff frontier between both properties."
1601,"In this paper, compliant mechanism design with non-linear materials using topology optimization is presented. A general displacement functional with non-linear material model is used in the topology optimization formulation. Sensitivity analysis of this displacement functional is derived from the adjoint method. Optimal compliant mechanism examples for maximizing the mechanical advantage are presented and the effect of nonlinear material on the optimal design are considered."
1602,"Virtual Reality (VR) applications will become increasingly important as the need to link several locations in the product development process arises. This motivates research in advanced techniques for the visualization of remotely located participants within a computer-generated environment. This research ports an existing application, which allows the assembly of predefined objects, into a virtual environment (VE). A further extension allows the user’s hands to be filmed and superimposed onto the computer-generated VE. To create this effect, live video was combined with a projection display and the “blue-box” technique. This combination enabled the texture of the hands to be untied from the background without the effort of putting up blue walls. The image is sent over the network to a high-end graphics computer generating the VE. The knowledge gained in this research will be the basis for future work on distributed multi-user access to a shared VE."
1603,"Hydraulic hoses are key components used to transfer power in heavy industrial machinery. The routing of these hoses is currently performed late in the product design process because no accurate physical models of the hoses exist that allow designers to predict the path the hoses will follow when installed in the machine. Designers must either guess the path the hose will take based on prior experience or wait until the first product prototype is built in order to experiment with the hose routes. This paper describes the use of ADAMS, a commercially available dynamic modeling package, to predict hose paths. The hose path model was verified by comparing the predicted paths to the paths of real hoses."
1604,"This study used finite element models to assess potential benefits of selected unconventional features, implemented in an experimental car, for vehicle crashworthiness in frontal impact. These safety features include: structural energy-absorbing bumper, hood lockdown with optimized hood and extendable bumper. The A-pillar intrusion and the effective acceleration of the vehicle were used as the parameters for measuring frontal impact crashworthiness performance."
